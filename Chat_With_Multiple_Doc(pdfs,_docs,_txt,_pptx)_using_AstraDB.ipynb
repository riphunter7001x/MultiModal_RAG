{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riphunter7001x/MultiModal_RAG/blob/main/Chat_With_Multiple_Doc(pdfs%2C_docs%2C_txt%2C_pptx)_using_AstraDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install unstructured\n",
        "!pip install openai\n",
        "!pip install Cython\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "9RDOffvrZ3F4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dafe765-e3b6-40d3-a000-4439a7d2b6b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.4)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.72)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
            "Requirement already satisfied: unstructured in /usr/local/lib/python3.10/dist-packages (0.14.4)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.12.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.6)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.25.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.9.3)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.0)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.23.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2024.2.2)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (7.0.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (23.2)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (4.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured) (4.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.31.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.3)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (3.0.10)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i929xxKLnRgr",
        "outputId": "3569e071-fb30-466a-a92f-d950ac3054c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-astradb in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: astrapy<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain-astradb) (1.2.0)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1.31 in /usr/local/lib/python3.10/dist-packages (from langchain-astradb) (0.2.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-astradb) (1.25.2)\n",
            "Requirement already satisfied: bson<0.6.0,>=0.5.10 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain-astradb) (0.5.10)\n",
            "Requirement already satisfied: cassio<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain-astradb) (0.1.7)\n",
            "Requirement already satisfied: deprecation<2.2.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain-astradb) (2.1.0)\n",
            "Requirement already satisfied: httpx[http2]<1,>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain-astradb) (0.27.0)\n",
            "Requirement already satisfied: toml<0.11.0,>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain-astradb) (0.10.2)\n",
            "Requirement already satisfied: uuid6<2024.2.0,>=2024.1.12 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain-astradb) (2024.1.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.66 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (0.1.72)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (2.7.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (8.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from bson<0.6.0,>=0.5.10->astrapy<2.0,>=1.2->langchain-astradb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bson<0.6.0,>=0.5.10->astrapy<2.0,>=1.2->langchain-astradb) (1.16.0)\n",
            "Requirement already satisfied: cassandra-driver<4.0.0,>=3.28.0 in /usr/local/lib/python3.10/dist-packages (from cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb) (3.29.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb) (2.32.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (1.3.1)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (4.1.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.31->langchain-astradb) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.1.31->langchain-astradb) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.31->langchain-astradb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.31->langchain-astradb) (2.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.31->langchain-astradb) (4.12.0)\n",
            "Requirement already satisfied: geomet<0.3,>=0.1 in /usr/local/lib/python3.10/dist-packages (from cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb) (0.2.1.post1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (4.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb) (2.0.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (1.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb) (8.1.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain-astradb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai datasets pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWdY3uvRnZKn",
        "outputId": "30d40733-cdb0-49de-b81e-1da3a5749e2e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.1.8)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.4)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.72)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.31.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain-openai) (1.2.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdf2image"
      ],
      "metadata": {
        "id": "B6oJrqqRauvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26cbd4be-f2fd-420e-ae5d-12a3a0377a25"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (10.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "id": "Ox_1QUszavjV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02f0337-d649-41b1-8915-5ee487b465ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20231228)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured[pdf]"
      ],
      "metadata": {
        "id": "fvp_dAEWayjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4121b241-3718-4bb3-d401-b1864fd57f1d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured[pdf] in /usr/local/lib/python3.10/dist-packages (0.14.4)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2.12.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.6.6)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.25.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (3.9.3)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.12.0)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.23.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.14.1)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.16.1)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.17.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (20231228)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (9.0.0)\n",
            "Requirement already satisfied: pillow-heif in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.16.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.2.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.3.10)\n",
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (3.7.2)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.4.1)\n",
            "Requirement already satisfied: unstructured-inference==0.7.33 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.7.33)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.3.12)\n",
            "Requirement already satisfied: layoutparser in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (0.0.9)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (0.23.2)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (4.8.0.76)\n",
            "Requirement already satisfied: onnxruntime>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (1.18.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (3.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (2.3.0+cu121)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (1.0.3)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (4.41.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]) (10.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[pdf]) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[pdf]) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[pdf]) (0.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[pdf]) (0.18.0+cu121)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[pdf]) (2.0.7)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[pdf]) (2.3.0)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]) (2.11.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured[pdf]) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]) (42.0.7)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from pikepdf->unstructured[pdf]) (1.2.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (2024.2.2)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (7.0.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (1.16.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured[pdf]) (4.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.63.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.64.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[pdf]) (0.14.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->unstructured[pdf]) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->unstructured[pdf]) (6.0.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.33->unstructured[pdf]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.33->unstructured[pdf]) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.33->unstructured[pdf]) (1.12.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.33->unstructured[pdf]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.33->unstructured[pdf]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.33->unstructured[pdf]) (4.52.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.33->unstructured[pdf]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.33->unstructured[pdf]) (3.1.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->unstructured-inference==0.7.33->unstructured[pdf]) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (3.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->unstructured-inference==0.7.33->unstructured[pdf]) (12.5.40)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.33->unstructured[pdf]) (0.19.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (1.11.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (2.0.3)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (0.11.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.6.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[pdf]) (1.2.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.33->unstructured[pdf]) (10.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->unstructured-inference==0.7.33->unstructured[pdf]) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (2024.1)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber->layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (4.30.0)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.7.33->unstructured[pdf]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  sentence_transformers langchain_community"
      ],
      "metadata": {
        "id": "BWGolr1FNpmd",
        "outputId": "df1105c0-a06a-49c5-c5ca-ab99b5d3326b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m224.7/224.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "from datasets import (\n",
        "    load_dataset,\n",
        ")\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator"
      ],
      "metadata": {
        "id": "gPuH-fXlnaiX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "embedding = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "Bost4y11ngS2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gXD1e0iknq9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Unstructured for loading Multiple Pdfs"
      ],
      "metadata": {
        "id": "BhXC2nsaaao4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir=\"/content/\""
      ],
      "metadata": {
        "id": "obMEfgOUaYoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_folder_path = f'{root_dir}/docs/'"
      ],
      "metadata": {
        "id": "fHwmBphmaMrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(pdf_folder_path)"
      ],
      "metadata": {
        "id": "EXg7WYjmaMx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# location of the pdf file/files.\n",
        "loaders = [UnstructuredPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]"
      ],
      "metadata": {
        "id": "gdyyz5uDbF65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaders"
      ],
      "metadata": {
        "id": "cIOOjInebHHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorstoreIndexCreator().from_loaders(loaders)"
      ],
      "metadata": {
        "id": "C6sNGjHsaM05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.query('What is the tokenization in RAG?')"
      ],
      "metadata": {
        "id": "TyONx7bRaM6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.query_with_sources('What is the tokenization in RAG?')"
      ],
      "metadata": {
        "id": "1kCaJmvhaM9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pypdf loader with Multiple Pdfs."
      ],
      "metadata": {
        "id": "2X3IRcpxbSKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_astradb import AstraDBVectorStore"
      ],
      "metadata": {
        "id": "btAgdVVknvyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_astradb import AstraDBVectorStore\n",
        "ASTRA_DB_API_ENDPOINT=\"https://d2357619-8f04-4cfd-bc3a-16e410893ba3-us-east-2.apps.astra.datastax.com\"\n",
        "ASTRA_DB_APPLICATION_TOKEN=\"AstraCS:hTmlZSqmAOUHSWZaeNqzEDOR:1128826e960e49c2508b3014ae7fa40e6b5d0490d8565702a30b4ea338083a4a\"\n",
        "ASTRA_DB_KEYSPACE=\"default_keyspace\""
      ],
      "metadata": {
        "id": "DgLFd0Kd2nIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir=\"/content/\"\n",
        "pdf_folder_path = f'{root_dir}/data/'\n",
        "pdfs=os.listdir(pdf_folder_path)"
      ],
      "metadata": {
        "id": "fLh8RfMwaNLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Quw8romYBEpV",
        "outputId": "d3a645f6-8cce-4d4a-b0c5-3d35f2ae51ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MachineTranslationwithAttention.pdf',\n",
              " 'Retrieval-Augmented-Generation-for-NLP.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=PyPDFLoader(\"/content/data/MachineTranslationwithAttention.pdf\")"
      ],
      "metadata": {
        "id": "iGWaBKx7BSiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4Y6bmItBhbB",
        "outputId": "0ba46137-2b3e-42b7-90ae-b9afefcad5b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x7f575ae96890>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)"
      ],
      "metadata": {
        "id": "u280YuAtCCzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.load_and_split(text_splitter=splitter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRH8dsh5B-n9",
        "outputId": "c092b97a-84e9-4605-bf8b-010ee09482c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis, Ethan Perez,\\nAleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler,\\nMike Lewis, Wen-tau Yih, Tim Rocktschel, Sebastian Riedel, Douwe Kiela\\nFacebook AI Research;University College London;New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 0}),\n",
              " Document(page_content='in their parameters, and achieve state-of-the-art results when ne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-specic architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 0}),\n",
              " Document(page_content='memory have so far been only investigated for extractive downstream tasks. We\\nexplore a general-purpose ne-tuning recipe for retrieval-augmented generation\\n(RAG)  models which combine pre-trained parametric and non-parametric mem-\\nory for language generation. We introduce RAG models where the parametric\\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 0}),\n",
              " Document(page_content='pare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, and another which can use different passages\\nper token. We ne-tune and evaluate our models on a wide range of knowledge-\\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-specic retrieve-and-extract\\narchitectures. For language generation tasks, we nd that RAG models generate', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 0}),\n",
              " Document(page_content='more specic, diverse and factual language than a state-of-the-art parametric-only\\nseq2seq baseline.\\n1 Introduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\\nedge from data [ 47]. They can do so without any access to an external memory, as a parameterized\\nimplicit knowledge base [ 51,52]. While this development is exciting, such models do have down-\\nsides: They cannot easily expand or revise their memory, cant straightforwardly provide insight into', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 0}),\n",
              " Document(page_content='their predictions, and may produce hallucinations [ 38]. Hybrid models that combine parametric\\nmemory with non-parametric (i.e., retrieval-based) memories [ 20,26,48] can address some of these\\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\\ninspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 0}),\n",
              " Document(page_content='combine masked language models [ 8] with a differentiable retriever, have shown promising results,arXiv:2005.11401v4  [cs.CL]  12 Apr 2021', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 0}),\n",
              " Document(page_content='The\\tDivine\\nComedy\\t(x) qQuery\\nEncoder\\nq(x)\\nMIPS p Generator \\xa0p\\n(Parametric)\\nMargin-\\nalize\\nThis\\t14th\\tcentury\\twork\\nis\\tdivided\\tinto\\t3\\nsections:\\t\"Inferno\",\\n\"Purgatorio\"\\t&\\n\"Paradiso\"\\t\\t\\t\\t\\t\\t\\t\\t\\t (y)End-to-End Backprop through q and\\xa0 p \\nBarack\\tObama\\twas\\nborn\\tin\\tHawaii. (x)\\nFact V erication: Fact Querysupports \\t(y)\\nQuestion GenerationFact V erication:\\nLabel GenerationDocument\\nIndexDefine\\t\"middle\\tear\" (x)\\nQuestion Answering:\\nQuestion QueryThe\\tmiddle\\tear\\tincludes\\nthe\\ttympanic\\tcavity\\tand\\nthe\\tthree\\tossicles.\\t\\t (y)', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 1}),\n",
              " Document(page_content='the\\ttympanic\\tcavity\\tand\\nthe\\tthree\\tossicles.\\t\\t (y)\\nQuestion Answering:\\nAnswer GenerationRetriever p\\n(Non-Parametric)\\nz 4\\nz3\\nz2\\nz 1d(z)\\nJeopardy Question\\nGeneration:\\nAnswer QueryFigure 1: Overview of our approach. We combine a pre-trained retriever ( Query Encoder +Document\\nIndex ) with a pre-trained seq2seq model ( Generator ) and ne-tune end-to-end. For query x, we use\\nMaximum Inner Product Search (MIPS) to nd the top-K documents zi. For nal prediction y, we', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 1}),\n",
              " Document(page_content='treatzas a latent variable and marginalize over seq2seq predictions given different documents.\\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\\nand non-parametric memory to the workhorse of NLP, i.e. sequence-to-sequence (seq2seq) models.\\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\\na general-purpose ne-tuning approach which we refer to as retrieval-augmented generation (RAG).', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 1}),\n",
              " Document(page_content='We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\\nretriever (Dense Passage Retriever [ 26], henceforth DPR) provides latent documents conditioned on\\nthe input, and the seq2seq model (BART [ 32]) then conditions on these latent documents together with', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 1}),\n",
              " Document(page_content='the input to generate the output. We marginalize the latent documents with a top-K approximation,\\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\\nbasis (where different documents are responsible for different tokens). Like T5 [ 51] or BART, RAG\\ncan be ne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 1}),\n",
              " Document(page_content='memory which are trained from scratch for specic tasks, e.g. memory networks [ 64,55], stack-\\naugmented networks [ 25] and memory layers [ 30]. In contrast, we explore a setting where both\\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\\npresent without additional training.', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 1}),\n",
              " Document(page_content='present without additional training.\\nOur results highlight the benets of combining parametric and non-parametric memory with genera-\\ntion for knowledge-intensive tasks tasks that humans could not reasonably be expected to perform\\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\\non open Natural Questions [ 29], WebQuestions [ 3] and CuratedTrec [ 2] and strongly outperform', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 1}),\n",
              " Document(page_content='recent approaches that use specialised pre-training objectives on TriviaQA [ 24]. Despite these being\\nextractive tasks, we nd that unconstrained generation outperforms previous extractive approaches.\\nFor knowledge-intensive generation, we experiment with MS-MARCO [ 1] and Jeopardy question\\ngeneration, and we nd that our models generate responses that are more factual, specic, and\\ndiverse than a BART baseline. For FEVER [ 56] fact verication, we achieve results within 4.3% of', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 1}),\n",
              " Document(page_content='state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\\nthe non-parametric memory can be replaced to update the models knowledge as the world changes.1\\n2 Methods\\nWe explore RAG models, which use the input sequence xto retrieve text documents zand use them\\nas additional context when generating the target sequence y. As shown in Figure 1, our models\\nleverage two components: (i) a retriever p(z|x)with parameters that returns (top-K truncated)', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 1}),\n",
              " Document(page_content='distributions over text passages given a query xand (ii) a generator p(yi|x,z,y 1:i1)parametrized\\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\\ners Library [ 66] and can be found at https://github.com/huggingface/transformers/blob/master/\\nexamples/rag/ . An interactive demo of RAG models can be found at https://huggingface.co/rag/\\n2', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 1}),\n",
              " Document(page_content='bythat generates a current token based on a context of the previous i1tokensy1:i1, the original\\ninputxand a retrieved passage z.\\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\\nWe propose two models that marginalize over the latent documents in different ways to produce a\\ndistribution over generated text. In one approach, RAG-Sequence , the model uses the same document', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 2}),\n",
              " Document(page_content='to predict each target token. The second approach, RAG-Token , can predict each target token based\\non a different document. In the following, we formally introduce both models and then describe the\\npandpcomponents, as well as the training and decoding procedure.\\n2.1 Models\\nRAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate\\nthe complete sequence . Technically, it treats the retrieved document as a single latent variable that', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 2}),\n",
              " Document(page_content='is marginalized to get the seq2seq probability p(y|x)via a top-K approximation. Concretely, the\\ntop K documents are retrieved using the retriever, and the generator produces the output sequence\\nprobability for each document, which are then marginalized,\\npRAG-Sequence (y|x)\\nztop-k(p(|x))p(z|x)p(y|x,z) =\\nztop-k(p(|x))p(z|x)N\\nip(yi|x,z,y 1:i1)\\nRAG-Token Model In the RAG-Token model we can draw a different latent document for each', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 2}),\n",
              " Document(page_content='target token and marginalize accordingly. This allows the generator to choose content from several\\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\\nretriever, and then the generator produces a distribution for the next output token for each document,\\nbefore marginalizing, and repeating the process with the following output token, Formally, we dene:\\npRAG-Token (y|x)N\\ni\\nztop-k(p(|x))p(z|x)p(yi|x,z,y 1:i1)', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 2}),\n",
              " Document(page_content='pRAG-Token (y|x)N\\ni\\nztop-k(p(|x))p(z|x)p(yi|x,z,y 1:i1)\\nFinally, we note that RAG can be used for sequence classication tasks by considering the target class\\nas a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n2.2 Retriever: DPR\\nThe retrieval component p(z|x)is based on DPR [26]. DPR follows a bi-encoder architecture:\\np(z|x)exp(\\nd(z)q(x))\\nd(z) =BERTd(z),q(x) =BERTq(x)', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 2}),\n",
              " Document(page_content='p(z|x)exp(\\nd(z)q(x))\\nd(z) =BERTd(z),q(x) =BERTq(x)\\nwhere d(z)is a dense representation of a document produced by a BERT BASE document encoder [8],\\nandq(x)a query representation produced by a query encoder , also based on BERT BASE. Calculating\\ntop-k (p(|x)), the list ofkdocumentszwith highest prior probability p(z|x), is a Maximum Inner\\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23]. We use', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 2}),\n",
              " Document(page_content='a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\\nretriever was trained to retrieve documents which contain answers to TriviaQA [ 24] questions and\\nNatural Questions [29]. We refer to the document index as the non-parametric memory .\\n2.3 Generator: BART\\nThe generator component p(yi|x,z,y 1:i1)could be modelled using any encoder-decoder. We use\\nBART-large [ 32], a pre-trained seq2seq transformer [ 58] with 400M parameters. To combine the input', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 2}),\n",
              " Document(page_content='xwith the retrieved content zwhen generating from BART, we simply concatenate them. BART was\\npre-trained using a denoising objective and a variety of different noising functions. It has obtained\\nstate-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\\nmodels [32]. We refer to the BART generator parameters as the parametric memory henceforth.\\n2.4 Training\\nWe jointly train the retriever and generator components without any direct supervision on what', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 2}),\n",
              " Document(page_content='document should be retrieved. Given a ne-tuning training corpus of input/output pairs (xj,yj), we\\n3', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 2}),\n",
              " Document(page_content='minimize the negative marginal log-likelihood of each target,\\njlogp(yj|xj)using stochastic\\ngradient descent with Adam [ 28]. Updating the document encoder BERTdduring training is costly as\\nit requires the document index to be periodically updated as REALM does during pre-training [ 20].\\nWe do not nd this step necessary for strong performance, and keep the document encoder (and\\nindex) xed, only ne-tuning the query encoder BERT qand the BART generator.\\n2.5 Decoding', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 3}),\n",
              " Document(page_content='2.5 Decoding\\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxyp(y|x).\\nRAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\ntor with transition probability: p\\n(yi|x,y 1:i1) =\\nztop-k(p(|x))p(zi|x)p(yi|x,zi,y1:i1)To\\ndecode, we can plug p\\n(yi|x,y 1:i1)into a standard beam decoder.\\nRAG-Sequence For RAG-Sequence, the likelihood p(y|x)does not break into a conventional per-', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 3}),\n",
              " Document(page_content='token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\\neach document z, scoring each hypothesis using p(yi|x,z,y 1:i1). This yields a set of hypotheses\\nY, some of which may not have appeared in the beams of all documents. To estimate the probability\\nof an hypothesis ywe run an additional forward pass for each document zfor whichydoes not\\nappear in the beam, multiply generator probability with p(z|x)and then sum the probabilities across', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 3}),\n",
              " Document(page_content='beams for the marginals. We refer to this decoding procedure as Thorough Decoding. For longer\\noutput sequences,|Y|can become large, requiring many forward passes. For more efcient decoding,\\nwe can make a further approximation that p(y|x,zi)0whereywas not generated during beam\\nsearch from x,zi. This avoids the need to run additional forward passes once the candidate set Yhas\\nbeen generated. We refer to this decoding procedure as Fast Decoding.\\n3 Experiments', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 3}),\n",
              " Document(page_content='3 Experiments\\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint\\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute an\\nembedding for each document, and build a single MIPS index using FAISS [ 23] with a Hierarchical', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 3}),\n",
              " Document(page_content='Navigable Small World approximation for fast retrieval [ 37]. During training, we retrieve the top\\nkdocuments for each query. We consider k{5,10}for training and set kfor test time using dev\\ndata. We now discuss experimental details for each task.\\n3.1 Open-domain Question Answering\\nOpen-domain question answering (QA) is an important real-world application and common testbed\\nfor knowledge-intensive tasks [ 20]. We treat questions and answers as input-output text pairs (x,y)', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 3}),\n",
              " Document(page_content='and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\\nthe popular extractive QA paradigm [ 5,7,31,26], where answers are extracted spans from retrieved\\ndocuments, relying primarily on non-parametric knowledge. We also compare to Closed-Book\\nQA approaches [ 52], which, like RAG, generate answers, but which do not exploit retrieval, instead\\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 3}),\n",
              " Document(page_content='Questions (NQ) [ 29], TriviaQA (TQA) [ 24]. WebQuestions (WQ) [ 3] and CuratedTrec (CT) [ 2]. As\\nCT and WQ are small, we follow DPR [ 26] by initializing CT and WQ models with our NQ RAG\\nmodel. We use the same train/dev/test splits as prior work [ 31,26] and report Exact Match (EM)\\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\\n3.2 Abstractive Question Answering\\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 3}),\n",
              " Document(page_content='text generation. To test RAGs natural language generation (NLG) in a knowledge-intensive setting,\\nwe use the MSMARCO NLG task v2.1 [ 43]. The task consists of questions, ten gold passages\\nretrieved from a search engine for each question, and a full sentence answer annotated from the\\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\\n4', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 3}),\n",
              " Document(page_content='MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\\nanswered in a way that matches the reference answer without access to the gold passages, such as\\nWhat is the weather in V olcano, CA? so performance will be lower without using gold passages.\\nWe also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\\nRAG can rely on parametric knowledge to generate reasonable responses.\\n3.3 Jeopardy Question Generation', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 4}),\n",
              " Document(page_content='3.3 Jeopardy Question Generation\\nTo evaluate RAGs generation abilities in a non-QA setting, we study open-domain question gen-\\neration. Rather than use questions from standard open-domain QA tasks, which typically consist\\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions.\\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 4}),\n",
              " Document(page_content='For example, The World Cup is the answer to the question In 1986 Mexico scored as the rst\\ncountry to host this international sports competition twice. As Jeopardy questions are precise,\\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\nchallenging knowledge-intensive generation task.\\nWe use the splits from SearchQA [ 10], with 100K train, 14K dev, and 27K test examples. As', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 4}),\n",
              " Document(page_content='this is a new task, we train a BART model for comparison. Following [ 67], we evaluate using the\\nSQuAD-tuned Q-BLEU-1 metric [ 42]. Q-BLEU is a variant of BLEU with a higher weight for\\nmatching entities and has higher correlation with human judgment for question generation than\\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\\none for specicity. We dene factuality as whether a statement can be corroborated by trusted external', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 4}),\n",
              " Document(page_content='sources, and specicity as high mutual dependence between the input and output [ 33]. We follow\\nbest practice and use pairwise comparative evaluation [ 34]. Evaluators are shown an answer and two\\ngenerated questions, one from BART and one from RAG. They are then asked to pick one of four\\noptionsquuestion A is better, question B is better, both are good, or neither is good.\\n3.4 Fact Verication\\nFEVER [ 56] requires classifying whether a natural language claim is supported or refuted by', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 4}),\n",
              " Document(page_content='Wikipedia, or whether there is not enough information to decide. The task requires retrieving\\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\\nwhether the claim is true, false, or unveriable from Wikipedia alone. FEVER is a retrieval problem\\ncoupled with an challenging entailment reasoning task. It also provides an appropriate testbed for\\nexploring the RAG models ability to handle classication rather than generation. We map FEVER', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 4}),\n",
              " Document(page_content='class labels (supports, refutes, or not enough info) to single output tokens and directly train with\\nclaim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on\\nretrieved evidence. In many real-world applications, retrieval supervision signals arent available, and\\nmodels that do not require such supervision will be applicable to a wider range of tasks. We explore\\ntwo variants: the standard 3-way classication task (supports/refutes/not enough info) and the 2-way', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 4}),\n",
              " Document(page_content='(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\\n4 Results\\n4.1 Open-domain Question Answering\\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\\nthe generation exibility of the closed-book (parametric only) approaches and the performance of', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 4}),\n",
              " Document(page_content='\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\\nwithout expensive, specialized salient span masking pre-training [ 20]. It is worth noting that RAGs\\nretriever is initialized using DPRs retriever, which uses retrieval supervision on Natural Questions\\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based cross-\\nencoder to re-rank documents, along with an extractive reader. RAG demonstrates that neither a', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 4}),\n",
              " Document(page_content='re-ranker nor extractive reader is necessary for state-of-the-art performance.\\nThere are several advantages to generating answers even when it is possible to extract them. Docu-\\nments with clues about the answer but do not contain the answer verbatim can still contribute towards\\na correct answer being generated, which is not possible with standard extractive approaches, leading\\n5', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 4}),\n",
              " Document(page_content='Table 1: Open-Domain QA Test Scores. For TQA,\\nleft column uses the standard test set for Open-\\nDomain QA, right column uses the TQA-Wiki\\ntest set. See Appendix D for further details.\\nModel NQ TQA WQ CT\\nClosed\\nBookT5-11B [52] 34.5 - /50.1 37.4 -\\nT5-11B+SSM[52] 36.6 - /60.5 44.7 -\\nOpen\\nBookREALM [20] 40.4 - / - 40.7 46.8\\nDPR [26] 41.5 57.9/ - 41.1 50.6\\nRAG-Token 44.1 55.2/66.1 45.5 50.0\\nRAG-Seq. 44.5 56.8/ 68.0 45.2 52.2Table 2: Generation and classication Test Scores.', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 5}),\n",
              " Document(page_content='MS-MARCO SotA is [ 4], FEVER-3 is [ 68] and\\nFEVER-2 is [ 57] *Uses gold context/evidence.\\nBest model without gold access underlined.\\nModel Jeopardy MSMARCO FVR3 FVR2\\nB-1 QB-1 R-L B-1 Label Acc.\\nSotA - - 49.8*49.9*76.8 92.2 *\\nBART 15.1 19.7 38.2 41.6 64.0 81.1\\nRAG-Tok. 17.3 22.2 40.1 41.572.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2\\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 5}),\n",
              " Document(page_content='even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such\\ncases for NQ, where an extractive model would score 0%.\\n4.2 Abstractive Question Answering\\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\\nimpressive given that (i) those models access gold passages with specic information required to', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 5}),\n",
              " Document(page_content='generate the reference answer , (ii) many questions are unanswerable without the gold passages, and\\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\\nfrom our models. Qualitatively, we nd that RAG models hallucinate less and generate factually\\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\\nBART generations (see 4.5).\\n4.3 Jeopardy Question Generation', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 5}),\n",
              " Document(page_content='BART generations (see 4.5).\\n4.3 Jeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\\nwith both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\\npairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\\nthan RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 5}),\n",
              " Document(page_content='BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\\nthe task over a state-of-the-art generation model. Evaluators also nd RAG generations to be more\\nspecic by a large margin. Table 3 shows typical generations from each model.\\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform\\nbest because it can generate responses that combine content from several documents. Figure 2 shows', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 5}),\n",
              " Document(page_content='an example. When generating Sun, the posterior is high for document 2 which mentions The\\nSun Also Rises. Similarly, document 1 dominates the posterior when A Farewell to Arms is\\ngenerated. Intriguingly, after the rst token of each book is generated, the document posterior attens.\\nThis observation suggests that the generator can complete the titles without depending on specic\\ndocuments. In other words, the models parametric knowledge is sufcient to complete the titles. We', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 5}),\n",
              " Document(page_content='nd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The\\nSun. BART completes the generation \"The SunAlso Rises\" isanovel bythis author of\"The Sun\\nAlso Rises\" indicating the title \"The Sun Also Rises\" is stored in BARTs parameters. Similarly,\\nBART will complete the partial decoding \"The SunAlso Rises\" isanovel bythis author of\"A\\nwith \"The SunAlso Rises\" isanovel bythis author of\"AFarewell toArms\" . This example shows', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 5}),\n",
              " Document(page_content='how parametric and non-parametric memories work together the non-parametric component helps\\nto guide the generation, drawing out specic knowledge stored in the parametric memory.\\n4.4 Fact Verication\\nTable 2 shows our results on FEVER. For 3-way classication, RAG scores are within 4.3% of\\nstate-of-the-art models, which are complex pipeline systems with domain-specic architectures and\\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\\n6', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 5}),\n",
              " Document(page_content='Document 1 : his works are considered classics of American\\nliterature ... His wartime experiences formed the basis for his novel\\nA Farewell to Arms (1929) ...\\nDocument 2 : ... artists of the 1920s Lost Generation expatriate\\ncommunity. His debut novel, The Sun Also Rises , was published\\nin 1926.\\nBOS\\nTheSunAlsoRisesisa\\nnovelbythis\\nauthorofA\\nFarewellto\\nArmsDoc 1\\nDoc 2\\nDoc 3\\nDoc 4\\nDoc 5Figure 2: RAG-Token document posterior p(zi|x,yi,yi)for each generated token for input Hem-', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 6}),\n",
              " Document(page_content='ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\\nwhen generating A Farewell to Arms\" and for document 2 when generating The Sun Also Rises\".\\nTable 3: Examples from generation tasks. RAG models generate more specic and factually accurate\\nresponses. ? indicates factually incorrect responses, * indicates partially correct responses.\\nTask Input Model Generation\\nMS-\\nMARCOdene middle', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 6}),\n",
              " Document(page_content='Task Input Model Generation\\nMS-\\nMARCOdene middle\\nearBART?The middle ear is the part of the ear between the middle ear and the nose.\\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\\nwhat currency\\nneeded in\\nscotlandBART The currency needed in Scotland is Pound sterling.\\nRAG-T Pound is the currency needed in Scotland.\\nRAG-S The currency needed in Scotland is the pound sterling.\\nJeopardy\\nQuestion\\nGener', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 6}),\n",
              " Document(page_content='Jeopardy\\nQuestion\\nGener\\n-ationWashingtonBART?This state has the largest number of counties in the U.S.\\nRAG-T Its the only U.S. state named for a U.S. president\\nRAG-S Its the state where youll nd Mount Rainier National Park\\nThe Divine\\nComedyBART*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\\nRAG-T Dantes \"Inferno\" is the rst part of this epic poem\\nRAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 6}),\n",
              " Document(page_content='For 2-way classication, we compare against Thorne and Vlachos [57], who train RoBERTa [ 35]\\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\\nevidence in FEVER. We calculate the overlap in article titles between the top kdocuments retrieved', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 6}),\n",
              " Document(page_content='by RAG and gold evidence annotations. We nd that the top retrieved document is from a gold article\\nin 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n4.5 Additional Results\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and specic than\\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding\\n[33,59,39], we also investigate generation diversity by calculating the ratio of distinct ngrams to', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 6}),\n",
              " Document(page_content='total ngrams generated by different models. Table 5 shows that RAG-Sequences generations are\\nmore diverse than RAG-Tokens, and both are signicantly more diverse than BART without needing\\nany diversity-promoting decoding.\\nRetrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.\\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 6}),\n",
              " Document(page_content='during training. As shown in Table 6, learned retrieval improves results for all tasks.\\nWe compare RAGs dense retriever to a word overlap-based BM25 retriever [ 53]. Here, we replace\\nRAGs retriever with a xed BM25 system, and use BM25 retrieval scores as logits when calculating\\np(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\\nheavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 6}),\n",
              " Document(page_content='improves results on all other tasks, especially for Open-Domain QA, where it is crucial.\\nIndex hot-swapping An advantage of non-parametric memory models like RAG is that knowledge\\ncan be easily updated at test time. Parametric-only models like T5 or BART need further training to\\nupdate their behavior as the world changes. To demonstrate, we build an index using the DrQA [ 5]\\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 6}),\n",
              " Document(page_content='index from our main results (December 2018). We prepare a list of 82 world leaders who had changed\\n7', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 6}),\n",
              " Document(page_content='Table 4: Human assessments for the Jeopardy\\nQuestion Generation Task.\\nFactuality Specicity\\nBART better 7.1% 16.8%\\nRAG better 42.7% 37.4%\\nBoth good 11.7% 11.8%\\nBoth poor 17.7% 6.9%\\nNo majority 20.8% 20.1%Table 5: Ratio of distinct to total tri-grams for\\ngeneration tasks.\\nMSMARCO Jeopardy QGen\\nGold 89.6% 90.0%\\nBART 70.7% 32.4%\\nRAG-Token 77.8% 46.8%\\nRAG-Seq. 83.5% 53.8%\\nTable 6: Ablations on the dev set. As FEVER is a classication task, both RAG models are equivalent.', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 7}),\n",
              " Document(page_content='Model NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2\\nExact Match B-1 QB-1 R-L B-1 Label Accuracy\\nRAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.475.1 91.6RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9\\nRAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.472.9 89.4RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3\\nRAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.474.5 90.6RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 7}),\n",
              " Document(page_content='between these dates and use a template Who is {position}? (e.g. Who is the President of Peru?)\\nto query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\\n2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\\nindices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).\\nThis shows we can update RAGs world knowledge by simply replacing its non-parametric memory.', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 7}),\n",
              " Document(page_content='Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent\\ndocuments, and we do not observe signicant differences in performance between them. We have the\\nexibility to adjust the number of retrieved documents at test time, which can affect performance and\\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\\nOpen-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 7}),\n",
              " Document(page_content='documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\\nRAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\\n10 20 30 40 50\\nKR e t r i e v e dD o c s394041424344NQ Exact MatchRAG-Tok\\nRAG-Seq\\n10 20 30 40 50\\nKR e t r i e v e dD o c s4050607080NQ Answer Recall @ KRAG-Tok\\nRAG-Seq\\nFixed DPR\\nBM25\\n10 20 30 40 50\\nKR e t r i e v e dD o c s4850525456Bleu-1 / Rouge-L scoreRAG-Tok R-L\\nRAG-Tok B-1\\nRAG-Seq R-L\\nRAG-Seq B-1', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 7}),\n",
              " Document(page_content='RAG-Tok B-1\\nRAG-Seq R-L\\nRAG-Seq B-1\\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-\\nmance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n5 Related Work\\nSingle-Task Retrieval Prior work has shown that retrieval improves performance across a variety of\\nNLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5,29],', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 7}),\n",
              " Document(page_content='fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article\\ngeneration [ 36], dialogue [ 41,65,9,13], translation [ 17], and language modeling [ 19,27]. Our\\nwork unies previous successes in incorporating retrieval into individual tasks, showing that a single\\nretrieval-based architecture is capable of achieving strong performance across several tasks.\\n8', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 7}),\n",
              " Document(page_content='General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP\\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\\nhas been shown to achieve strong performance on various classication tasks in the GLUE bench-\\nmarks [ 60,61] after ne-tuning [ 49,8]. GPT-2 [ 50] later showed that a single, left-to-right, pre-trained\\nlanguage model could achieve strong performance across both discriminative and generative tasks.', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 8}),\n",
              " Document(page_content='For further improvement, BART [ 32] and T5 [ 51,52] propose a single, pre-trained encoder-decoder\\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\\nand generative tasks. Our work aims to expand the space of possible tasks with a single, unied\\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\\nLearned Retrieval There is signicant work on learning to retrieve documents in information', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 8}),\n",
              " Document(page_content='retrieval, more recently with pre-trained, neural language models [ 44,26] similar to ours. Some\\nwork optimizes the retrieval module to aid in a specic, downstream task such as question answering,\\nusing search [ 46], reinforcement learning [ 6,63,62], or a latent variable approach [ 31,20] as in our\\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\\nachieve strong performance on a single task, while we show that a single retrieval-based architecture', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 8}),\n",
              " Document(page_content='can be ne-tuned for strong performance on a variety of tasks.\\nMemory-based Architectures Our document index can be seen as a large external memory for\\nneural networks to attend to, analogous to memory networks [ 64,55]. Concurrent work [ 14] learns\\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\\nwork. Other work improves the ability of dialog models to generate factual text by attending over', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 8}),\n",
              " Document(page_content='fact embeddings [ 15,13]. A key feature of our memory is that it is comprised of raw text rather\\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the models\\nmemory by editing the document index. This approach has also been used in knowledge-intensive\\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 8}),\n",
              " Document(page_content='rather than end-to-end learnt retrieval [9].\\nRetrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style\\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\\nto provide a nal output. These approaches have proved successful in a number of domains including\\nMachine Translation [ 18,22] and Semantic Parsing [ 21]. Our approach does have several differences,', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 8}),\n",
              " Document(page_content='including less of emphasis on lightly editing a retrieved item, but on aggregating content from several\\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\\ncould represent promising future work.\\n6 Discussion\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 8}),\n",
              " Document(page_content='memory. We showed that our RAG models obtain state of the art results on open-domain QA. We\\nfound that people prefer RAGs generation over purely parametric BART, nding RAG more factual\\nand specic. We conducted an thorough investigation of the learned retrieval component, validating\\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 8}),\n",
              " Document(page_content='can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\\nanother objective. Our work opens up new research directions on how parametric and non-parametric\\nmemories interact and how to most effectively combine them, showing promise in being applied to a\\nwide variety of NLP tasks.\\n9', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 8}),\n",
              " Document(page_content='Broader Impact\\nThis work offers several positive societal benets over previous work: the fact that it is more\\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it hallucinate less\\nwith generations that are more factual, and offers more control and interpretability. RAG could be\\nemployed in a wide variety of scenarios with direct benet to society, for example by endowing it\\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 9}),\n",
              " Document(page_content='effective at their jobs.\\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\\nemployed as a language model, similar concerns as for GPT-2 [ 50] are valid here, although arguably\\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 9}),\n",
              " Document(page_content='the news or on social media; to impersonate others; or to automate the production of spam/phishing\\ncontent [ 54]. Advanced language models may also lead to the automation of various jobs in the\\ncoming decades [ 16]. In order to mitigate these risks, AI systems could be employed to ght against\\nmisleading content and automated spam/phishing.\\nAcknowledgments\\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 9}),\n",
              " Document(page_content='paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\\nprogram.\\nReferences\\n[1]Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 9}),\n",
              " Document(page_content='Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\\nReading COmprehension Dataset. arXiv:1611.09268 [cs] , November 2016. URL http:\\n//arxiv.org/abs/1611.09268 . arXiv: 1611.09268.\\n[2]Petr Baudi and Jan ediv `y. Modeling of the question answering task in the yodaqa system. In\\nInternational Conference of the Cross-Language Evaluation Forum for European Languages ,\\npages 222228. Springer, 2015. URL https://link.springer.com/chapter/10.1007%\\n2F978-3-319-24027-5_20 .', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 9}),\n",
              " Document(page_content='2F978-3-319-24027-5_20 .\\n[3]Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\\nfrom Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing , pages 15331544, Seattle, Washington, USA, October 2013.\\nAssociation for Computational Linguistics. URL http://www.aclweb.org/anthology/\\nD13-1160 .\\n[4]Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 9}),\n",
              " Document(page_content='ing&autoregressive language model for context-conditioned generation. ArXiv , abs/2004.07159,\\n2020. URL https://arxiv.org/abs/2004.07159 .\\n[5]Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\\nOpen-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers) , pages 18701879, Vancouver, Canada,\\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 9}),\n",
              " Document(page_content='https://www.aclweb.org/anthology/P17-1171 .\\n[6]Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\\nJonathan Berant. Coarse-to-ne question answering for long documents. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\\npages 209220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\\n10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020 .\\n10', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 9}),\n",
              " Document(page_content='[7]Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-\\nhension. arXiv:1710.10723 [cs] , October 2017. URL http://arxiv.org/abs/1710.10723 .\\narXiv: 1710.10723.\\n[8]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\\nDeep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Con-\\nference of the North American Chapter of the Association for Computational Linguistics: Human', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 10}),\n",
              " Document(page_content='Language Technologies, Volume 1 (Long and Short Papers) , pages 41714186, Minneapolis,\\nMinnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\\nURL https://www.aclweb.org/anthology/N19-1423 .\\n[9]Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-\\nard of wikipedia: Knowledge-powered conversational agents. In International Conference on\\nLearning Representations , 2019. URL https://openreview.net/forum?id=r1l73iRqKm .', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 10}),\n",
              " Document(page_content='[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun\\nCho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.\\narXiv:1704.05179 [cs] , April 2017. URL http://arxiv.org/abs/1704.05179 . arXiv:\\n1704.05179.\\n[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 10}),\n",
              " Document(page_content='Long Papers) , pages 889898, Melbourne, Australia, July 2018. Association for Computational\\nLinguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/\\nP18-1082 .\\n[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\\nLong form question answering. In Proceedings of the 57th Annual Meeting of the Association\\nfor Computational Linguistics , pages 35583567, Florence, Italy, July 2019. Association for', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 10}),\n",
              " Document(page_content='Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/\\nanthology/P19-1346 .\\n[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers\\nwith KNN-based composite memory, 2020. URL https://openreview.net/forum?id=\\nH1gx1CNKPH .\\n[14] Thibault Fvry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.\\nEntities as experts: Sparse memory access with entity supervision. ArXiv , abs/2004.07202,', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 10}),\n",
              " Document(page_content='2020. URL https://arxiv.org/abs/2004.07202 .\\n[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen\\ntau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI\\nConference on Articial Intelligence , 2018. URL https://www.aaai.org/ocs/index.php/\\nAAAI/AAAI18/paper/view/16710 .\\n[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI\\nexceed human performance? evidence from AI experts. CoRR , abs/1705.08807, 2017. URL', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 10}),\n",
              " Document(page_content='http://arxiv.org/abs/1705.08807 .\\n[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In AAAI Conference on Articial Intelligence , 2018. URL https:\\n//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282 .\\n[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural\\nmachine translation. In 32nd AAAI Conference on Articial Intelligence, AAAI 2018 , 32nd', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 10}),\n",
              " Document(page_content='AAAI Conference on Articial Intelligence, AAAI 2018, pages 51335140. AAAI press, 2018.\\n32nd AAAI Conference on Articial Intelligence, AAAI 2018 ; Conference date: 02-02-2018\\nThrough 07-02-2018.\\n[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by\\nediting prototypes. Transactions of the Association for Computational Linguistics , 6:437450,\\n2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031 .\\n11', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 10}),\n",
              " Document(page_content='[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\\nRetrieval-augmented language model pre-training. ArXiv , abs/2002.08909, 2020. URL https:\\n//arxiv.org/abs/2002.08909 .\\n[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A\\nretrieve-and-edit framework for predicting structured outputs. In S. Bengio,\\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\\nitors, Advances in Neural Information Processing Systems 31 , pages 10052', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 11}),\n",
              " Document(page_content='10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/\\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\\npdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-\\nedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics , pages 25322538, Online, July 2020. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 11}),\n",
              " Document(page_content='anthology/2020.acl-main.228 .\\n[23] Jeff Johnson, Matthijs Douze, and Herv Jgou. Billion-scale similarity search with gpus. arXiv\\npreprint arXiv:1702.08734 , 2017. URL https://arxiv.org/abs/1702.08734 .\\n[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale\\nDistantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the\\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 11}),\n",
              " Document(page_content='pages 16011611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\\ndoi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147 .\\n[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-\\naugmented recurrent nets. In Proceedings of the 28th International Conference on\\nNeural Information Processing Systems - Volume 1 , NIPS15, page 190198, Cam-\\nbridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 11}),\n",
              " Document(page_content='5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets .\\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and\\nWen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint\\narXiv:2004.04906 , 2020. URL https://arxiv.org/abs/2004.04906 .\\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-\\ntion through memorization: Nearest neighbor language models. In International Conference on', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 11}),\n",
              " Document(page_content='Learning Representations , 2020. URL https://openreview.net/forum?id=HklBjCEKvH .\\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\\nBengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,\\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL\\nhttp://arxiv.org/abs/1412.6980 .\\n[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redeld, Michael Collins, Ankur Parikh,', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 11}),\n",
              " Document(page_content='Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-\\nton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-\\ntion Answering Research. Transactions of the Association of Computational Lin-\\nguistics , 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/\\nnatural-questions/main-1455-kwiatkowski.pdf .', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 11}),\n",
              " Document(page_content='natural-questions/main-1455-kwiatkowski.pdf .\\n[30] Guillaume Lample, Alexandre Sablayrolles, Marc Aurelio Ranzato, Ludovic Denoyer, and\\nHerve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,\\nA. Beygelzimer, F. d Alch-Buc, E. Fox, and R. Garnett, editors, Advances in Neural In-\\nformation Processing Systems 32 , pages 85488559. Curran Associates, Inc., 2019. URL http:\\n//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf .', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 11}),\n",
              " Document(page_content='[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\\nopen domain question answering. In Proceedings of the 57th Annual Meeting of the Association\\n12', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 11}),\n",
              " Document(page_content='for Computational Linguistics , pages 60866096, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/\\nanthology/P19-1612 .\\n[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\\npre-training for natural language generation, translation, and comprehension. arXiv preprint', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 12}),\n",
              " Document(page_content='arXiv:1910.13461 , 2019. URL https://arxiv.org/abs/1910.13461 .\\n[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\\nobjective function for neural conversation models. In Proceedings of the 2016 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies , pages 110119, San Diego, California, June 2016. Association for Computational', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 12}),\n",
              " Document(page_content='Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/\\nN16-1014 .\\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation\\nwith optimized questions and multi-turn comparisons. ArXiv , abs/1909.03087, 2019. URL\\nhttps://arxiv.org/abs/1909.03087 .\\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine\\ntranslation with joint textual and phonetic embedding. In Proceedings of the 57th Annual', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 12}),\n",
              " Document(page_content='Meeting of the Association for Computational Linguistics , pages 30443049, Florence, Italy,\\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\\nhttps://www.aclweb.org/anthology/P19-1291 .\\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\\nand Noam Shazeer. Generating wikipedia by summarizing long sequences. In International\\nConference on Learning Representations , 2018. URL https://openreview.net/forum?\\nid=Hyg0vbWC- .', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 12}),\n",
              " Document(page_content='id=Hyg0vbWC- .\\n[37] Yury A. Malkov and D. A. Yashunin. Efcient and robust approximate nearest neighbor search\\nusing hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence , 42:824836, 2016. URL https://arxiv.org/abs/1603.09320 .\\n[38] Gary Marcus. The next decade in ai: four steps towards robust articial intelligence. arXiv\\npreprint arXiv:2002.06177 , 2020. URL https://arxiv.org/abs/2002.06177 .', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 12}),\n",
              " Document(page_content='[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktschel, Vassilis\\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\\nveriability of generated text. arXiv preprint arXiv:1911.03587 , 2019. URL https:\\n//arxiv.org/abs/1911.03587 .\\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 12}),\n",
              " Document(page_content='precision training. In ICLR , 2018. URL https://openreview.net/forum?id=r1gs9JgRZ .\\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-\\ning background knowledge for building conversation systems. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing , pages 23222332, Brus-\\nsels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255 .', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 12}),\n",
              " Document(page_content='[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\\nsystems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\\nProcessing , pages 39503959, Brussels, Belgium, October-November 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/\\nanthology/D18-1429 .\\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 12}),\n",
              " Document(page_content='and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\\nTarek Richard Besold, Antoine Bordes, Artur S. dAvila Garcez, and Greg Wayne, editors,\\nProceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic\\n13', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 12}),\n",
              " Document(page_content='approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing\\nSystems (NIPS 2016), Barcelona, Spain, December 9, 2016 , volume 1773 of CEUR Workshop\\nProceedings . CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_\\n2016_paper9.pdf .\\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint\\narXiv:1901.04085 , 2019. URL https://arxiv.org/abs/1901.04085 .', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 13}),\n",
              " Document(page_content='arXiv:1901.04085 , 2019. URL https://arxiv.org/abs/1901.04085 .\\n[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings\\nof the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics (Demonstrations) , pages 4853, Minneapolis, Minnesota, June 2019. Association', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 13}),\n",
              " Document(page_content='for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb.\\norg/anthology/N19-4009 .\\n[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun\\nCho. Finding generalizable evidence by learning to convince q&a models. In Proceedings\\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 13}),\n",
              " Document(page_content='24022411, Hong Kong, China, November 2019. Association for Computational Linguistics.\\ndoi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244 .\\n[47] Fabio Petroni, Tim Rocktschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing and the 9th International', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 13}),\n",
              " Document(page_content='Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 24632473, Hong\\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/\\nD19-1250. URL https://www.aclweb.org/anthology/D19-1250 .\\n[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktschel, Yuxiang Wu, Alexander H.\\nMiller, and Sebastian Riedel. How context affects language models factual predictions. In\\nAutomated Knowledge Base Construction , 2020. URL https://openreview.net/forum?', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 13}),\n",
              " Document(page_content='id=025X0zPfn .\\n[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-\\nproving Language Understanding by Generative Pre-Training, 2018. URL\\nhttps://s3-us-west-2.amazonaws.com/openai-assets/research-covers/\\nlanguage-unsupervised/language_understanding_paper.pdf .\\n[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\\nSutskever. Language models are unsupervised multitask learners, 2019. URL\\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/language_', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 13}),\n",
              " Document(page_content='models_are_unsupervised_multitask_learners.pdf .\\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unied\\ntext-to-text transformer. arXiv e-prints , 2019. URL https://arxiv.org/abs/1910.10683 .\\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\\nthe parameters of a language model? arXiv e-prints , 2020. URL https://arxiv.org/abs/\\n2002.08910 .', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 13}),\n",
              " Document(page_content='2002.08910 .\\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\\nbeyond. Found. Trends Inf. Retr. , 3(4):333389, April 2009. ISSN 1554-0669. doi: 10.1561/\\n1500000019. URL https://doi.org/10.1561/1500000019 .\\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec\\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\\nArXiv , abs/1908.09203, 2019.', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 13}),\n",
              " Document(page_content='ArXiv , abs/1908.09203, 2019.\\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-\\nworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances\\nin Neural Information Processing Systems 28 , pages 24402448. Curran Associates, Inc., 2015.\\nURL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf .\\n14', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 13}),\n",
              " Document(page_content='[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\\nlarge-scale dataset for fact extraction and VERication. In Proceedings of the 2018 Conference\\nof the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long Papers) , pages 809819, New Orleans, Louisiana,\\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL\\nhttps://www.aclweb.org/anthology/N18-1074 .', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 14}),\n",
              " Document(page_content='https://www.aclweb.org/anthology/N18-1074 .\\n[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model\\nbiases in sentence-pair classication with elastic weight consolidation. ArXiv , abs/2004.14366,\\n2020. URL https://arxiv.org/abs/2004.14366 .\\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\n ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 14}),\n",
              " Document(page_content='S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\\nInformation Processing Systems 30 , pages 59986008. Curran Associates, Inc., 2017. URL\\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\\nCrandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 14}),\n",
              " Document(page_content='AAAI Conference on Articial Intelligence , 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/17329 .\\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\\nInProceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting\\nNeural Networks for NLP , pages 353355, Brussels, Belgium, November 2018. Association for', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 14}),\n",
              " Document(page_content='Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/\\nanthology/W18-5446 .\\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-\\nPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\\nF. d\\\\textquotesingle Alch-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 14}),\n",
              " Document(page_content='Processing Systems 32 , pages 32613275. Curran Associates, Inc., 2019. URL https://\\narxiv.org/abs/1905.00537 .\\n[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,\\nGerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain\\nquestion answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of\\nthe Thirty-Second AAAI Conference on Articial Intelligence, (AAAI-18), the 30th innovative', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 14}),\n",
              " Document(page_content='Applications of Articial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational\\nAdvances in Articial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,\\n2018 , pages 59815988. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.\\nphp/AAAI/AAAI18/paper/view/16712 .\\n[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,\\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 14}),\n",
              " Document(page_content='ranking in open-domain question answering. In ICLR , 2018. URL https://openreview.\\nnet/forum?id=rJl3yM-Ab .\\n[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\\nand Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR\\n2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL\\nhttp://arxiv.org/abs/1410.3916 .\\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and rene: Improved sequence', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 14}),\n",
              " Document(page_content='generation models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd\\nInternational Workshop on Search-Oriented Conversational AI , pages 8792, Brussels, Belgium,\\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL\\nhttps://www.aclweb.org/anthology/W18-5713 .\\n15', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 14}),\n",
              " Document(page_content='[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\\nMoi, Pierric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingfaces transformers:\\nState-of-the-art natural language processing. ArXiv , abs/1910.03771, 2019.', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 15}),\n",
              " Document(page_content='[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-\\nsupervised question answering. In Proceedings of the 2019 Conference on Empirical Meth-\\nods in Natural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing (EMNLP-IJCNLP) , pages 24952509, Hong Kong, China, Novem-\\nber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\\nhttps://www.aclweb.org/anthology/D19-1253 .', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 15}),\n",
              " Document(page_content='https://www.aclweb.org/anthology/D19-1253 .\\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and\\nJian Yin. Reasoning over semantic-level graph for fact checking. ArXiv , abs/1909.03745, 2019.\\nURL https://arxiv.org/abs/1909.03745 .\\n16', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 15}),\n",
              " Document(page_content='Appendices for Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nA Implementation Details\\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.\\nFor RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\\nThorough Decoding approach since answers are generally short. We use greedy decoding for QA as\\nwe did not nd beam search improved results. For Open-MSMarco and Jeopardy question generation,', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 16}),\n",
              " Document(page_content='we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\\nB Human Evaluation\\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions\\nand a worked example appear when clicking \"view tool guide\".', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 16}),\n",
              " Document(page_content='and a worked example appear when clicking \"view tool guide\".\\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position,\\nwhich model corresponded to sentence A and sentence B was randomly selected for each example.\\nAnnotators were encouraged to research the topic using the internet, and were given detailed instruc-\\ntions and worked examples in a full instructions tab. We included some gold sentences in order to', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 16}),\n",
              " Document(page_content='assess the accuracy of the annotators. Two annotators did not perform well on these examples and\\ntheir annotations were removed from the results.\\nC Training setup Details\\nWe train all RAG models and BART baselines using Fairseq [ 45].2We train with mixed precision\\noating point arithmetic [ 40], distributing training across 8, 32GB NVIDIA V100 GPUs, though\\ntraining and inference can be run on one GPU. We nd that doing Maximum Inner Product Search', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 16}),\n",
              " Document(page_content='with FAISS is sufciently fast on CPU, so we store document index vectors on CPU, requiring 100\\nGB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace\\nTransformers [ 66]3, which achieves equivalent performance to the previous version but is a cleaner\\nand easier to use implementation. This version is also open-sourced. We also compress the document\\nindex using FAISSs compression tools, reducing the CPU memory requirement to 36GB. Scripts to', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 16}),\n",
              " Document(page_content='run experiments with RAG can be found at https://github.com/huggingface/transformers/\\nblob/master/examples/rag/README.md and an interactive demo of a RAG model can be found\\nathttps://huggingface.co/rag/\\n2https://github.com/pytorch/fairseq\\n3https://github.com/huggingface/transformers\\n17', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 16}),\n",
              " Document(page_content='D Further Details on Open-Domain QA\\nFor open-domain QA, multiple answer annotations are often available for a given question. These\\nanswer annotations are exploited by extractive models during training as typically all the answer\\nannotations are used to nd matches within documents when preparing training data. For RAG, we\\nalso make use of multiple annotation examples for Natural Questions and WebQuestions by training', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 17}),\n",
              " Document(page_content='the model with each (q,a)pair separately, leading to a small increase in accuracy. For TriviaQA,\\nthere are often many valid answers to a given question, some of which are not suitable training targets,\\nsuch as emoji or spelling variants. For TriviaQA, we lter out answer candidates if they do not occur\\nin top 1000 documents for the query.\\nCuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres-', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 17}),\n",
              " Document(page_content='sions, which has been suggested as a reason why it is unsuitable for answer-generation models [20].\\nTo overcome this, we use a pre-processing step where we rst retrieve the top 1000 documents for\\neach query, and use the answer that most frequently matches the regex pattern as the supervision\\ntarget. If no matches are found, we resort to a simple heuristic: generate all possible permutations for\\neach regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 17}),\n",
              " Document(page_content='TriviaQA Evaluation setups The open-domain QA community customarily uses public develop-\\nment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading\\ncompehension purposes. We report our results using the datasets splits used in DPR [ 26], which are\\nconsistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public\\nTriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofcial Wikipedia test set', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 17}),\n",
              " Document(page_content='instead. Fvry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See\\nappendix of [ 14]). We report results on both test sets to enable fair comparison to both approaches.\\nWe nd that our performance is much higher using the ofcial Wiki test set, rather than the more\\nconventional open-domain test set, which we attribute to the ofcial Wiki test set questions being\\nsimpler to answer from Wikipedia.\\nE Further Details on FEVER', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 17}),\n",
              " Document(page_content='simpler to answer from Wikipedia.\\nE Further Details on FEVER\\nFor FEVER classication, we follow the practice from [ 32], and rst re-generate the claim, and\\nthen classify using the representation of the nal hidden state, before nally marginalizing across\\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The\\nrst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 17}),\n",
              " Document(page_content='we explore in the main paper. FEVERs other sub-task involves extracting sentences from Wikipedia\\nas evidence supporting the classication prediction. As FEVER uses a different Wikipedia dump to\\nus, directly tackling this task is not straightforward. We hope to address this in future work.\\nF Null Document Probabilities\\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [ 20] in order', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 17}),\n",
              " Document(page_content='to model cases where no useful information could be retrieved for a given input. Here, if kdocuments\\nwere retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null\\ndocument, before marginalizing over k+ 1predictions. We explored modelling this null document\\nlogit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or\\n(iii) a neural network to predict the logit. We did not nd that these improved performance, so in', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 17}),\n",
              " Document(page_content='the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents\\ncannot always be retrieved, we observe that the model learns to always retrieve a particular set of\\ndocuments for questions that are less likely to benet from retrieval, suggesting that null document\\nmechanisms may not be necessary for RAG.\\nG Parameters\\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 17}),\n",
              " Document(page_content='DPR, with 110M parameters each (although we do not train the document encoder ourselves) and\\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\\n18', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 17}),\n",
              " Document(page_content='Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\\nTask Train Development Test\\nNatural Questions 79169 8758 3611\\nTriviaQA 78786 8838 11314\\nWebQuestions 3418 362 2033\\nCuratedTrec 635 134 635\\nJeopardy Question Generation 97392 13714 26849\\nMS-MARCO 153726 12468 101093*\\nFEVER-3-way 145450 10000 10000\\nFEVER-2-way 96966 6666 6666\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 18}),\n",
              " Document(page_content='with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [ 52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 18}),\n",
              " Document(page_content='728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit oating\\npoint precision to manage memory and disk footprints.\\nH Retrieval Collapse\\nIn preliminary experiments, we observed that for some tasks such as story generation [ 11], the\\nretrieval component would collapse and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 18}),\n",
              " Document(page_content='and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\nI Number of instances per dataset', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 18}),\n",
              " Document(page_content='I Number of instances per dataset\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\\n19', metadata={'source': '/content/data/Retrieval-Augmented-Generation-for-NLP.pdf', 'page': 18})]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs=[]\n",
        "for pdf in pdfs:\n",
        "  data=PyPDFLoader(f\"/content/data/{pdf}\")\n",
        "  docs.append(data)"
      ],
      "metadata": {
        "id": "hK6CgClrbbS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "docs_from_pdf = docs.load_and_split(text_splitter=splitter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "agk6IZLabd3p",
        "outputId": "ffdbaa2b-58a4-406f-c781-a9a9fa2b20c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'load_and_split'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-d14b615aab55>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msplitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_overlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdocs_from_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_splitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'load_and_split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Documents from PDF: {len(docs_from_pdf)}.\")\n",
        "inserted_ids_from_pdf = vstore.add_documents(docs_from_pdf)\n",
        "print(f\"Inserted {len(inserted_ids_from_pdf)} documents.\")"
      ],
      "metadata": {
        "id": "oNllVkvIbgKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vstore = AstraDBVectorStore(\n",
        "    embedding=embedding,\n",
        "    collection_name=\"astra_vector_demo\",\n",
        "    api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
        "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
        "    namespace=ASTRA_DB_KEYSPACE,\n",
        ")"
      ],
      "metadata": {
        "id": "n4G743wn3i9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vstore.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "cfzD7a8naIEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "You are a philosopher that draws inspiration from great thinkers of the past\n",
        "to craft well-thought answers to user questions. Use the provided context as the basis\n",
        "for your answers and do not make up new reasoning paths - just mix-and-match what you are given.\n",
        "Your answers must be concise and to the point, and refrain from answering about other topics than philosophy.\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "YOUR ANSWER:\"\"\""
      ],
      "metadata": {
        "id": "9Y2EFU9_aINQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(prompt_template)"
      ],
      "metadata": {
        "id": "Nx0rM706aIPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI()\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | philo_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "tRg2VFehaISq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"How does Russel elaborate on Peirce's idea of the security blanket?\")"
      ],
      "metadata": {
        "id": "D9pg2syhbyHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Directory loders(Chat With Multiple Doc)"
      ],
      "metadata": {
        "id": "v2b452jhb6mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"/content/docs/.ipynb_checkpoints\""
      ],
      "metadata": {
        "id": "tZS1rEQB7YOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yqDtZ1M3z8U",
        "outputId": "5206fcc5-f7c1-4aec-f8cd-9b462eac2853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.0 (from langchain_community)\n",
            "  Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.3.0,>=0.2.0 (from langchain_community)\n",
            "  Downloading langchain_core-0.2.3-py3-none-any.whl (310 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n",
            "  Downloading langsmith-0.1.69-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.0->langchain_community)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain_community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain_community)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain_community)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.6 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.1 langchain-core-0.2.3 langchain-text-splitters-0.2.0 langchain_community-0.2.1 langsmith-0.1.69 marshmallow-3.21.2 mypy-extensions-1.0.0 orjson-3.10.3 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UuNkzrU5Q5q",
        "outputId": "7f9f12ca-2679-434b-a07e-cb728daa66a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured in /usr/local/lib/python3.10/dist-packages (0.14.4)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.12.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.6)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.25.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.9.3)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.0)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.23.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2024.2.2)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (7.0.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (23.2)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (4.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured) (4.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unstructured[pdf]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksK7gi4p5d1l",
        "outputId": "145c130b-0a5a-47aa-f749-0fdf9abab71d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured[pdf] in /usr/local/lib/python3.10/dist-packages (0.14.4)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2.12.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.6.6)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.25.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (3.9.3)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.12.0)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.23.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.14.1)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.16.1)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (1.17.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (20231228)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (9.0.0)\n",
            "Requirement already satisfied: pillow-heif in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.16.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (4.2.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.3.10)\n",
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (3.7.2)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.4.1)\n",
            "Requirement already satisfied: unstructured-inference==0.7.33 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.7.33)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from unstructured[pdf]) (0.3.12)\n",
            "Requirement already satisfied: layoutparser in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (0.0.9)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (0.23.2)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (4.8.0.76)\n",
            "Requirement already satisfied: onnxruntime>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (1.18.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (3.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (2.3.0+cu121)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (1.0.3)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.33->unstructured[pdf]) (4.41.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]) (10.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[pdf]) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[pdf]) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[pdf]) (0.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[pdf]) (0.18.0+cu121)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[pdf]) (2.0.7)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[pdf]) (2.3.0)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]) (2.11.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured[pdf]) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pdf]) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]) (42.0.7)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from pikepdf->unstructured[pdf]) (1.2.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pdf]) (2024.2.2)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (7.0.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pdf]) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (1.16.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured[pdf]) (4.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.63.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.64.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (4.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[pdf]) (0.14.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->unstructured[pdf]) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.0->effdet->unstructured[pdf]) (6.0.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.33->unstructured[pdf]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.33->unstructured[pdf]) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.33->unstructured[pdf]) (1.12.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.33->unstructured[pdf]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.33->unstructured[pdf]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.33->unstructured[pdf]) (4.52.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.33->unstructured[pdf]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.33->unstructured[pdf]) (3.1.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->unstructured-inference==0.7.33->unstructured[pdf]) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (3.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->unstructured-inference==0.7.33->unstructured[pdf]) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->unstructured-inference==0.7.33->unstructured[pdf]) (12.5.40)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.33->unstructured[pdf]) (0.19.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (1.11.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (2.0.3)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (from layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (0.11.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.6.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[pdf]) (1.2.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.33->unstructured[pdf]) (10.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath->layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->unstructured-inference==0.7.33->unstructured[pdf]) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (2024.1)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber->layoutparser->unstructured-inference==0.7.33->unstructured[pdf]) (4.30.0)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.7.33->unstructured[pdf]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uk7ezbu7OQp",
        "outputId": "78ef510d-0397-4598-859d-9b2531693ebc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Waiting for headers] [1 InRelease 12.7 kB/129 kB 10%] [Connected to cloud.r\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [1 InRelease 129 kB/129 kB 100%] [Connected to cloud.r\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                               \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r                                                                               \rGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.5 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,386 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,130 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,858 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,085 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [891 kB]\n",
            "Fetched 7,687 kB in 3s (3,029 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkesIO_m7P9P",
        "outputId": "a42b0626-3588-42f2-cb13-e97839d9ad65"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 48 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.4 [186 kB]\n",
            "Fetched 186 kB in 0s (1,143 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.4) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9OycnSq7Tt9",
        "outputId": "39dc673c-4e0f-48d6-aea2-270ff3b294fc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libarchive13 libimagequant0 libraqm0 mailcap mime-support\n",
            "  python3-olefile tesseract-ocr-osd\n",
            "Suggested packages:\n",
            "  lrzip python-pil-doc\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libimagequant0 libleptonica-dev libraqm0 libtesseract-dev\n",
            "  mailcap mime-support python3-olefile python3-pil tesseract-ocr\n",
            "  tesseract-ocr-eng tesseract-ocr-osd tesseract-ocr-script-latn\n",
            "The following packages will be upgraded:\n",
            "  libarchive13\n",
            "1 upgraded, 13 newly installed, 0 to remove and 47 not upgraded.\n",
            "Need to get 40.3 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive13 amd64 3.6.0-1ubuntu1.1 [369 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1.1 [582 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libimagequant0 amd64 2.17.0-1 [34.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libraqm0 amd64 0.7.0-4ubuntu1 [11.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-olefile all 0.46-3 [33.8 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-pil amd64 9.0.1-1ubuntu0.3 [419 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-script-latn all 1:4.00~git30-7274cfa-1.1 [30.9 MB]\n",
            "Fetched 40.3 MB in 1s (54.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 14.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 121948 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libarchive13_3.6.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libarchive13:amd64 (3.6.0-1ubuntu1.1) over (3.6.0-1ubuntu1) ...\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "Preparing to unpack .../01-libarchive-dev_3.6.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libimagequant0:amd64.\n",
            "Preparing to unpack .../02-libimagequant0_2.17.0-1_amd64.deb ...\n",
            "Unpacking libimagequant0:amd64 (2.17.0-1) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../03-libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libraqm0:amd64.\n",
            "Preparing to unpack .../04-libraqm0_0.7.0-4ubuntu1_amd64.deb ...\n",
            "Unpacking libraqm0:amd64 (0.7.0-4ubuntu1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../05-libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Selecting previously unselected package mailcap.\n",
            "Preparing to unpack .../06-mailcap_3.70+nmu1ubuntu1_all.deb ...\n",
            "Unpacking mailcap (3.70+nmu1ubuntu1) ...\n",
            "Selecting previously unselected package mime-support.\n",
            "Preparing to unpack .../07-mime-support_3.66_all.deb ...\n",
            "Unpacking mime-support (3.66) ...\n",
            "Selecting previously unselected package python3-olefile.\n",
            "Preparing to unpack .../08-python3-olefile_0.46-3_all.deb ...\n",
            "Unpacking python3-olefile (0.46-3) ...\n",
            "Selecting previously unselected package python3-pil:amd64.\n",
            "Preparing to unpack .../09-python3-pil_9.0.1-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking python3-pil:amd64 (9.0.1-1ubuntu0.3) ...\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "Preparing to unpack .../10-tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../11-tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../12-tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Selecting previously unselected package tesseract-ocr-script-latn.\n",
            "Preparing to unpack .../13-tesseract-ocr-script-latn_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-script-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up libarchive13:amd64 (3.6.0-1ubuntu1.1) ...\n",
            "Setting up tesseract-ocr-script-latn (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up python3-olefile (0.46-3) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libraqm0:amd64 (0.7.0-4ubuntu1) ...\n",
            "Setting up libimagequant0:amd64 (2.17.0-1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up mailcap (3.70+nmu1ubuntu1) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up mime-support (3.66) ...\n",
            "Setting up python3-pil:amd64 (9.0.1-1ubuntu0.3) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured-pytesseract\n",
        "!pip install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMP99Q_y7XWl",
        "outputId": "e8507ef5-7ff2-4d92-fbde-0db5a611c9ff"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured-pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from unstructured-pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-pytesseract) (10.3.0)\n",
            "Collecting tesseract-ocr\n",
            "  Downloading tesseract-ocr-0.0.1.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from tesseract-ocr) (3.0.10)\n",
            "Building wheels for collected packages: tesseract-ocr\n",
            "  Building wheel for tesseract-ocr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tesseract-ocr: filename=tesseract_ocr-0.0.1-cp310-cp310-linux_x86_64.whl size=169755 sha256=b4460aac3aa42cb0472449d4d53ef93c1b24582f20b1ac4404aa88528b92c708\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/fd/f3/5c231ecbbb80a1fe33204ff3021d99b54ef6daf6f8099311b8\n",
            "Successfully built tesseract-ocr\n",
            "Installing collected packages: tesseract-ocr\n",
            "Successfully installed tesseract-ocr-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unstructured[pptx]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RruMFEmtMhVw",
        "outputId": "13962ca7-e989-480b-d41e-0ff455775b35"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured[pptx] in /usr/local/lib/python3.10/dist-packages (0.14.4)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (2.12.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (0.6.6)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (1.25.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (3.9.3)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (4.12.0)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (0.23.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[pptx]) (1.14.1)\n",
            "Collecting python-pptx<=0.6.23 (from unstructured[pptx])\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx<=0.6.23->unstructured[pptx]) (10.3.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<=0.6.23->unstructured[pptx])\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[pptx]) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[pptx]) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[pptx]) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured[pptx]) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pptx]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pptx]) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pptx]) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[pptx]) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pptx]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pptx]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pptx]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[pptx]) (2024.2.2)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (7.0.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (1.6.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (23.2)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (4.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[pptx]) (1.0.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured[pptx]) (4.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pptx]) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pptx]) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pptx]) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[pptx]) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[pptx]) (1.2.1)\n",
            "Installing collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.2.0 python-pptx-0.6.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_astradb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR8YmEFX_bXo",
        "outputId": "8326f82a-78b9-4e32-a64d-e695bd04b0a6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_astradb in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: astrapy<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain_astradb) (1.2.0)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1.31 in /usr/local/lib/python3.10/dist-packages (from langchain_astradb) (0.2.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_astradb) (1.25.2)\n",
            "Requirement already satisfied: bson<0.6.0,>=0.5.10 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain_astradb) (0.5.10)\n",
            "Requirement already satisfied: cassio<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain_astradb) (0.1.7)\n",
            "Requirement already satisfied: deprecation<2.2.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain_astradb) (2.1.0)\n",
            "Requirement already satisfied: httpx[http2]<1,>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain_astradb) (0.27.0)\n",
            "Requirement already satisfied: toml<0.11.0,>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain_astradb) (0.10.2)\n",
            "Requirement already satisfied: uuid6<2024.2.0,>=2024.1.12 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain_astradb) (2024.1.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain_astradb) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain_astradb) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.66 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain_astradb) (0.1.72)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain_astradb) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain_astradb) (2.7.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain_astradb) (8.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from bson<0.6.0,>=0.5.10->astrapy<2.0,>=1.2->langchain_astradb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bson<0.6.0,>=0.5.10->astrapy<2.0,>=1.2->langchain_astradb) (1.16.0)\n",
            "Requirement already satisfied: cassandra-driver<4.0.0,>=3.28.0 in /usr/local/lib/python3.10/dist-packages (from cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain_astradb) (3.29.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain_astradb) (2.32.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain_astradb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain_astradb) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain_astradb) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain_astradb) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain_astradb) (1.3.1)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain_astradb) (4.1.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain_astradb) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.31->langchain_astradb) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.1.31->langchain_astradb) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.31->langchain_astradb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.31->langchain_astradb) (2.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.31->langchain_astradb) (4.12.0)\n",
            "Requirement already satisfied: geomet<0.3,>=0.1 in /usr/local/lib/python3.10/dist-packages (from cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain_astradb) (0.2.1.post1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain_astradb) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain_astradb) (4.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain_astradb) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain_astradb) (2.0.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain_astradb) (1.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain_astradb) (8.1.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai datasets pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rN3g_sxPLjN",
        "outputId": "c7bc150e-fb7d-48a5-dda9-69d95f5cc4c1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.1.8)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.4)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.72)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.31.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain-openai) (1.2.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unstructured[docx]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gowU5IyDLv7r",
        "outputId": "c83529bc-2850-4ab4-aa37-5bb9ef502bec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured[docx] in /usr/local/lib/python3.10/dist-packages (0.14.4)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (2.12.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (0.6.6)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (2024.4.27)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (1.25.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (3.9.3)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (4.12.0)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (0.23.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[docx]) (1.14.1)\n",
            "Collecting python-docx>=1.1.2 (from unstructured[docx])\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[docx]) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[docx]) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured[docx]) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured[docx]) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[docx]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[docx]) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[docx]) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[docx]) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[docx]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[docx]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[docx]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured[docx]) (2024.2.2)\n",
            "Requirement already satisfied: deepdiff>=6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[docx]) (7.0.1)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[docx]) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[docx]) (1.0.6)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[docx]) (1.0.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[docx]) (1.6.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[docx]) (23.2)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[docx]) (4.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[docx]) (2.8.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured[docx]) (1.0.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from deepdiff>=6.0->unstructured-client->unstructured[docx]) (4.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[docx]) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[docx]) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[docx]) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[docx]) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[docx]) (1.2.1)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "AjAFSJYlDpkA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n"
      ],
      "metadata": {
        "id": "nBVPhAdPDNE3"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DirectoryLoader('/content/docs')"
      ],
      "metadata": {
        "id": "GYA9S1oaU1g3"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)"
      ],
      "metadata": {
        "id": "icOls_EgDQy_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load_and_split(text_splitter=splitter)"
      ],
      "metadata": {
        "id": "gXfYNkYx5Lx7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaBLlukoN1in",
        "outputId": "ddb51d75-add9-46d8-ccd0-9afee6388ddc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='BTECH FINAL YEAR PROJECT SYNOPSIS A Synopsis Submitted in Partial Fulfillment of the Requirements for the Degree of BACHELOR OF TECHNOLOGY in Department of Artificial Intelligence\\n\\nGuided by : Prof. Shubhangi Ingale\\n\\nProject Topic / Title : Movie Review Sentiment Analysis on IMDB Data by Vishal Kshirsagar AIB14 Aditya Varpe AIA07 Shivam Kharat AIA62 Vivek Vaidya  BCOB121', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='G H Raisoni College of Engineering and Management Wagholi, Pune Department of Artificial Intelligence (An Autonomous Institute Affiliated to SPPU, Pune)\\n\\nMarch, 2024\\n\\nSynopsis\\n\\n1. Introduction', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='In an age dominated by digital discourse and the democratization of opinions, the film industry stands at the forefront of this cultural dialogue. The democratization of movie reviews on platforms like IMDB has transformed the audience from mere spectators to active contributors, shaping the narrative around cinematic creations. This project, \"Movie Review Sentiment Analysis on IMDB Data,\" embarks on a compelling exploration into the wealth of opinions embedded in these online repositories.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='The prevalence of movie reviews on platforms like IMDB not only reflects the sheer volume of content available but also serves as a goldmine of unfiltered public sentiment. As cinema continues to be a powerful medium influencing society, understanding the nuances of audience reactions becomes increasingly crucial. Leveraging the capabilities of Hadoop, a robust framework for distributed storage and processing, we intend to not only gather but also preprocess and analyze this voluminous data, uncovering', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='also preprocess and analyze this voluminous data, uncovering patterns and sentiments that lie beneath the surface of individual reviews.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='The choice of Hadoop as the technological backbone of this project is deliberate. Its scalability and parallel processing capabilities make it well-suited for handling the vast dataset inherent in sentiment analysis of movie reviews. This ensures not only the efficiency of our analysis but also positions the project to adapt and grow in the face of the ever-expanding world of cinematic content and its associated reviews.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='The primary aim of this project is twofold: firstly, to provide a granular understanding of sentiment on a per-movie basis, identifying the emotional spectrum that audiences traverse when engaging with cinematic works. Secondly, the analysis extends beyond individual movies, aiming to uncover broader trends by dissecting sentiments across genres or attributing them to specific directors. This dual-pronged approach not only offers filmmakers and studios valuable insights into the reception of their work but', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='studios valuable insights into the reception of their work but also provides a comprehensive overview of the dynamic relationship between cinematic genres, directors, and audience sentiment.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='As we stand at the intersection of big data analytics and the entertainment industry, this project symbolizes a synergy between technology and art, seeking to unravel the intricate tapestry of emotions woven into the fabric of film critique. Through this venture, we aspire to contribute not only to the scholarly discourse on sentiment analysis but also to offer practical, actionable insights for those immersed in the creation and appreciation of cinematic masterpieces.\\n\\n2. Motivation', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='The motivation behind this project stems from a desire to bridge the gap between subjective cinematic experiences and objective data analytics. By unraveling the sentiments embedded in vast pools of movie reviews, we aim to empower filmmakers with actionable insights into audience reactions. In an era where data-driven decision-making reigns supreme, this project serves as a testament to the transformative potential of combining technology with the art of filmmaking. Ultimately, our motivation lies in', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='with the art of filmmaking. Ultimately, our motivation lies in contributing to a deeper understanding of the intricate relationship between creators and consumers, fostering an environment where the language of data enriches the narrative of cinema.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='3. Literature Review Understanding sentiment analysis within the context of movie reviews has been a topic of increasing interest and scholarly exploration. Researchers and practitioners alike have delved into the nuances of extracting sentiments from textual data to gain insights into audience reactions and preferences. The literature review presents an overview of key findings and methodologies employed in related studies, offering a foundation for our project.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='Numerous studies have demonstrated the efficacy of sentiment analysis in deciphering movie reviews. Pang and Lee (2008) laid the groundwork by introducing a dataset for sentiment analysis, showcasing the potential of machine learning techniques. Their work inspired subsequent research, leading to the development of more sophisticated models. Turney (2002) introduced a lexicon-based approach, emphasizing the importance of word associations in determining sentiment polarity. While lexicon-based methods are', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='determining sentiment polarity. While lexicon-based methods are intuitive, machine learning techniques have gained prominence due to their adaptability to diverse datasets.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='In recent years, deep learning models have emerged as powerful tools for sentiment analysis. Kim (2014) applied convolutional neural networks (CNNs) to capture hierarchical features in movie reviews, achieving competitive performance. The depth and complexity of neural networks allow for a more nuanced understanding of contextual information, particularly beneficial in the inherently subjective domain of movie critiques.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='The application of sentiment analysis to movie reviews extends beyond binary classification. Tang et al. (2015) introduced a novel approach of aspect-based sentiment analysis, dissecting reviews into specific aspects and evaluating sentiment on each aspect individually. This approach provides a more granular understanding, allowing for targeted improvements in different facets of filmmaking.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='While sentiment analysis on individual movie reviews has been explored extensively, the literature also indicates a growing interest in aggregating sentiments to discern broader trends. Ghose and Ipeirotis (2011) studied the correlation between box office success and sentiment in movie reviews, highlighting the potential economic implications of sentiment analysis in the film industry. Our project builds upon this notion by not only analyzing sentiments on a per-movie basis but also exploring trends across', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='on a per-movie basis but also exploring trends across genres and directors.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content=\"The integration of big data technologies, specifically Hadoop, in sentiment analysis is a relatively unexplored domain within the context of movie reviews. Existing literature primarily focuses on sentiment analysis in general or adopts traditional data processing approaches. Our project seeks to fill this gap by leveraging Hadoop's capabilities for efficient preprocessing and analysis of large-scale movie review datasets.\", metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='In summary, the literature review underscores the evolution of sentiment analysis in the realm of movie reviews. From foundational lexicon-based methods to advanced deep learning models, the field has witnessed continuous innovation. Our project aligns with this trajectory, contributing by integrating big data technologies and extending sentiment analysis to discern trends across genres and directors, offering a holistic perspective on audience reactions within the cinematic landscape. 4. Related Work', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='The landscape of sentiment analysis on movie reviews has witnessed diverse methodologies and applications, providing a rich tapestry of related work that informs and contextualizes our project. Several studies have paved the way, each contributing unique insights and approaches to the understanding of sentiment in cinematic critiques. Sentiment Analysis Techniques: Numerous studies have explored various techniques for sentiment analysis. Turney (2002) introduced a lexicon-based approach, where sentiment is', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='(2002) introduced a lexicon-based approach, where sentiment is determined based on the presence of positive and negative words in a given text. Pang and Lee (2008) extended this work by employing machine learning algorithms, classifying reviews into positive and negative sentiments. Building upon these foundations, recent works like Kim (2014) have explored the capabilities of deep learning models, specifically convolutional neural networks (CNNs), showcasing improved performance in capturing complex', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='(CNNs), showcasing improved performance in capturing complex sentiment structures within movie reviews.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='Aspect-Based Sentiment Analysis: Tang et al. (2015) introduced aspect-based sentiment analysis, a paradigm that dissects reviews into specific aspects to evaluate sentiment on each aspect individually. This approach goes beyond overall sentiment polarity, providing a more nuanced understanding of audience reactions to different facets of a movie. Our project draws inspiration from this methodology, aiming to apply a similar granular analysis to discern sentiments across genres and directors, adding a layer', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='discern sentiments across genres and directors, adding a layer of specificity to the overall sentiment trends.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='Sentiment Analysis and Box Office Success: Ghose and Ipeirotis (2011) delved into the economic implications of sentiment analysis in the film industry by studying the correlation between movie reviews and box office success. Their work highlighted the potential influence of sentiment on the financial performance of movies. While our project is not explicitly focused on box office success, it extends this line of inquiry by exploring sentiment trends in relation to genres and directors, offering insights', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='trends in relation to genres and directors, offering insights that could impact various dimensions of filmmaking and audience engagement.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='Big Data Technologies in Sentiment Analysis: The integration of big data technologies, particularly Hadoop, in sentiment analysis is a relatively unexplored avenue within the context of movie reviews. Existing literature primarily focuses on sentiment analysis in general or adopts traditional data processing approaches. Our project pioneers the application of Hadoop in preprocessing and analyzing large-scale movie review datasets. This innovation not only addresses the scalability challenge posed by', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='not only addresses the scalability challenge posed by extensive review repositories but also aligns with the growing importance of big data analytics in extracting meaningful insights from vast datasets.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='In synthesizing these works, our project emerges at the intersection of sentiment analysis, deep learning, and big data technologies. By incorporating aspects of these diverse methodologies, it aspires to contribute to the evolving landscape of understanding audience sentiments towards movies, offering a unique perspective on sentiment trends across genres and directors within the cinematic realm. 5. Proposed Method', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='The proposed method integrates state-of-the-art sentiment analysis techniques with the scalability and parallel processing capabilities of Hadoop, aiming to provide a comprehensive understanding of audience sentiments in movie reviews. The methodology encompasses three key phases: data collection, preprocessing, and sentiment analysis.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='Data Collection: The initial step involves gathering a substantial dataset of movie reviews from IMDB or other prominent movie review platforms. IMDB stands out due to its extensive collection and diverse user-contributed reviews, providing a robust foundation for sentiment analysis. The dataset will encompass a broad spectrum of movies, spanning various genres and featuring multiple directors to ensure a representative sample.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content=\"Preprocessing with Hadoop: The collected dataset, often voluminous and unstructured, necessitates efficient preprocessing to extract meaningful insights. Hadoop, renowned for its ability to handle large-scale data processing, will play a pivotal role in this phase. The dataset will be ingested into the Hadoop Distributed File System (HDFS), allowing for distributed storage and retrieval. Hadoop's MapReduce paradigm will then be employed for parallelized data processing, enhancing the speed and efficiency\", metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='data processing, enhancing the speed and efficiency of tasks such as tokenization, stemming, and removal of stop words.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='To ensure the quality of sentiment analysis, a lexicon-based approach will be incorporated for sentiment polarity determination. The Hadoop ecosystem will facilitate the distribution of lexicons across nodes, optimizing the sentiment analysis process. Additionally, the distributed nature of Hadoop enables the parallelization of sentiment analysis tasks, accelerating the overall processing time.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='Sentiment Analysis and Trends Exploration: Following preprocessing, the sentiment analysis phase will involve the application of machine learning models, specifically deep learning techniques, to discern sentiments from the preprocessed data. Convolutional Neural Networks (CNNs) or Long Short-Term Memory networks (LSTMs) will be explored for their ability to capture intricate patterns and dependencies within textual data.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='The analysis will extend beyond individual movie sentiments to explore broader trends across genres and directors. Through a systematic categorization of reviews based on genres and directors, the project aims to uncover patterns in audience sentiments. This exploration is crucial for filmmakers, studios, and industry enthusiasts, offering insights into the factors influencing the reception of movies within specific genres or under particular directors.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='By leveraging Hadoop for efficient preprocessing and incorporating advanced sentiment analysis techniques, the proposed method aspires to provide a nuanced understanding of audience sentiments towards movies. The integration of big data technologies ensures scalability, enabling the analysis of extensive datasets, while the deep learning models enhance the sophistication of sentiment analysis, allowing for a more nuanced interpretation of textual data. This holistic approach positions the project at the', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='data. This holistic approach positions the project at the forefront of sentiment analysis within the cinematic landscape, offering a robust and scalable methodology for extracting meaningful insights from the vast sea of movie reviews.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='6. Methodology\\n\\nThe methodology encompasses a systematic approach to unraveling sentiment patterns in movie reviews, blending traditional sentiment analysis techniques with cutting-edge technologies. Initial data collection involves compiling a diverse dataset of movie reviews from IMDB. Leveraging Hadoop for preprocessing, the dataset undergoes tokenization, stemming, and sentiment lexicon-based analysis in a distributed environment.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='The sentiment analysis phase employs deep learning models, possibly CNNs or LSTMs, to decipher intricate sentiment structures within reviews. The focus extends beyond individual movie sentiments to exploring trends across genres and directors.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content=\"The project follows a stepwise process: (1) Data Collection, (2) Hadoop-based Preprocessing, and (3) Sentiment Analysis. This methodology not only ensures the efficiency of processing large-scale datasets but also incorporates advanced sentiment analysis techniques for a nuanced understanding of audience reactions. The combination of traditional sentiment analysis with Hadoop's scalability and deep learning models elevates the project's capacity to unravel comprehensive insights into the dynamic interplay\", metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='to unravel comprehensive insights into the dynamic interplay between movies, genres, directors, and audience sentiments.', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='References 1. Pang, B., & Lee, L. (2008). Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2), 1-135. 2. Turney, P. D. (2002). Thumbs up or thumbs down? : Semantic orientation applied to unsupervised classification of reviews. Proceedings of the Association for Computational Linguistics (ACL), 417-424. 3. Kim, Y. (2014). Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882. 4. Tang, D., Qin, B., & Liu, T. (2015). Learning', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='4. Tang, D., Qin, B., & Liu, T. (2015). Learning semantic representations of users and products for document-level sentiment classification. Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, 101-110. 5. Ghose, A., & Ipeirotis, P. G. (2011). Estimating the helpfulness and economic impact of product reviews: Mining text and reviewer characteristics. IEEE Transactions on Knowledge and Data Engineering, 23(10), 1498-1512 6. Manning, C. D., Raghavan, P., & Schtze, H. (2008).', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='6. Manning, C. D., Raghavan, P., & Schtze, H. (2008). Introduction to information retrieval. Cambridge University Press. 7. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26. 8. Pennington, J., Socher, R., & Manning, C. (2014). GloVe: Global vectors for word representation. Proceedings of the 2014 conference on empirical methods in natural language', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='of the 2014 conference on empirical methods in natural language processing (EMNLP), 1532-1543. 9. Goldberg, Y., & Levy, O. (2014).', metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content=\"Word2Vec Explained: Deriving Mikolov et al. 's Negative-Sampling Word-Embedding Method. arXiv preprint arXiv:1402.3722. 10. Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research, 3, 993-1022.\", metadata={'source': '/content/docs/syno.txt'}),\n",
              " Document(page_content='What is a Hadoop Cluster?\\n\\nA cluster is basically a collection. A computer cluster is a collection of computers interconnected to each other over a network. Similarly, a Hadoop Cluster is a collection of extraordinary computational systems designed and deployed to store, optimise, and analyse petabytes of Big Data with astonishing agility.\\n\\nHere this Big Data Course will explain to you more about Hadoop Cluster with real-time project experience, which was well designed by Top Industry working Experts.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Factors deciding the Hadoop Cluster Capacity\\n\\nNow that we know what exactly a Hadoop Cluster is, let us now learn why exactly we need to plan a Hadoop Cluster and what are various factors we need to look into, in order to plan an efficient Hadoop Cluster with optimum performance\\n\\nVolume of Data', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Volume of Data\\n\\nIf you ever wonder how Hadoop even came into existence, it is because of the huge volume of data that the traditional data processing systems could not handle. Since the introduction of Hadoop, the volume of data also increased exponentially.\\n\\nSo, it is important for a Hadoop Admin to know about the volume of Data he needs to deal with and accordingly plan, organize, and set up the Hadoop Cluster with the appropriate number of nodes for an Efficient Data Management\\n\\nData Retention', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Data Retention\\n\\nData Retention is all about storing only the important and valid data. There are many situations where the data arrived will be incomplete or invalid that may affect the process of Data Analysis. So, there is no point in storing such data.\\n\\nData Retention is a process where the user gets to remove outdated, invalid, and unnecessary data from the Hadoop Storage to save space and improve cluster computation speeds.\\n\\nData Storage', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Data Storage\\n\\nData Storage is one of the crucial factors that come into picture when you are into planning a Hadoop Cluster. Data is never stored directly as it is obtained. It undergoes through a process called Data Compression.\\n\\nHere, the obtained data is encrypted and compressed using various Data Encryption and Data Compression algorithms so that the data security is achieved and the space consumed to save the data is as minimal as possible.\\n\\nType of Work Load', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Type of Work Load\\n\\nThis factor is purely performance-oriented. All this factor deals with is the performance of the cluster. the Work Load on the processor can be classified into three types. Intensive, normal, and low.\\n\\nSome jobs like Data Storage cause low workload on the processor. Jobs like Data Querying will have intense workloads on both the processor and the storage units of the Hadoop Cluster.\\n\\nHardware Requirements for Hadoop Cluster', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Hardware Requirements for Hadoop Cluster\\n\\nWe have discussed Hadoop Cluster and the factors involved in planning an effective Hadoop Cluster. Now, we will discuss the standard hardware requirements needed by the Hadoop Components. Hadoops Architecture basically has the following components.\\n\\nNameNode Job Tracker \\uf0b7 \\uf0b7 DataNode \\uf0b7 Task Tracker\\n\\nNameNode/Secondary NameNode/Job Tracker.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='NameNode/Secondary NameNode/Job Tracker.\\n\\nNameNode and Secondary NameNode are the crucial parts of any Hadoop Cluster. They are expected to be highly available. The NameNode and Secondary NameNode servers are dedicated to storing the namespace storage and edit-log journaling.\\n\\nComponent Operating System FS-Image Other Softwares(Zookeeper) Processor RAM Intenet\\n\\nRequirement 1 Terabyte Harddisk Space 2 Terabyte Harddisk Space 1 Terabyte Harddisk Space Octa-Core Processor 2.5 GHz 128 GB 10 GBPS', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='DataNode/Task Tracker\\n\\nFollowed by the NameNode and Job Tracker, the next crucial components in a Hadoop Cluster where the actual data is stored and the Hadoop jobs get executed are data nodes and Task Tacker respectively. Let us now discuss the Hardware requirements for DataNode and Task Tracker.\\n\\nComponent Number of Nodes Processor RAM Internet\\n\\nRequirement 24 nodes(4 Terabytes each) Octa-Core Processor 2.5 GHz 128 GB 10 GBPS\\n\\nOperating System Requirement', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Operating System Requirement\\n\\nWhen it comes to software, then the Operating System becomes most important. You can set up your Hadoop cluster using the operating system of your choice. Few of the most recommended operating Systems to set up a Hadoop Cluster are,\\n\\nSolaris \\uf0b7 Ubuntu \\uf0b7 Fedora \\uf0b7 RedHat \\uf0b7 CentOS\\n\\nNow, let us understand a sample use case\\n\\nSample Hadoop Cluster Plan', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Sample Hadoop Cluster Plan\\n\\nNow that we have understood The Hardware and the Software requirements for Hadoop Cluster Capacity Planning, we will now plan a sample Hadoop Cluster for a better understanding. The following problem is based on the same.\\n\\nLet us assume that we have to deal with the minimum data of 10 TB and assume that there is a gradual growth of data, say 25% per every 3 months. In future, assuming that the data grows per every year and data in year 1 is 10,000 TB.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='By then end of 5 years, let us assume that it may grow to 25,000 TB. If we assume 25% of year-by-year growth and 10,000 TB data per year, then after 5 years, the resultant data is nearly 100,000 TB.\\n\\nSo, how exactly can we even estimate the number of data nodes that we might require to tackle all this data? The answer is simple. Using the formula as mentioned below.\\n\\nHadoop Storage (HS) = CRS / (1-i)\\n\\nWhere', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Hadoop Storage (HS) = CRS / (1-i)\\n\\nWhere\\n\\nC= Compression Ratio \\uf0b7 R= Replication Factor \\uf0b7 S= Size of the data to be moved into Hadoop \\uf0b7 i= Intermediate Factor\\n\\nCalculating the number of nodes required.\\n\\nAssuming that we will not be using any sort of Data Compression, hence, C is 1.\\n\\nThe standard replication factor for Hadoop is 3.\\n\\nThe Intermediate factor is 0.25, then the calculation for Hadoop, in this case, will result as follows\\n\\nHS = (1*3*S) / (1-(1/4)\\n\\nHS = 4S', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='HS = (1*3*S) / (1-(1/4)\\n\\nHS = 4S\\n\\nThe expected Hadoop Storage instance, in this case, is 4 times the initial storage. The following formula can be used to estimate the number of data nodes.\\n\\nN = HS/D = (CRS/(1-i)) / D\\n\\nWhere D is Diskspace available per Node.\\n\\nLet us assume that 25 TB is the available Diskspace per single node. Each Node Comprising of 27 Disks of 1 TB each. (2 TB is dedicated to Operating System). Also assuming the initial Data Size to be 5000 TB.\\n\\nN = 5000/25 = 200', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='N = 5000/25 = 200\\n\\nHence, We need 200 Nodes in this scenario.\\n\\nQuestion 2', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Projecting Required Big Data Capacity We start with 1 TB of daily data from Year 1 and assume 15% data growth per quarter. Further, assuming a 15% year on-year growth in data volumes and 1,080 TB of data in Year 1, by the end of Year 5 the capacity may grow to 8,295 TB of data. If we were to assume a 30% year-on-year growth in data volumes and 1080 TB of data in Year 1, then by the end of Year 5, the capacity might grow to 50,598 TB of data. The following formula can be used to estimate Hadoop storage and', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='following formula can be used to estimate Hadoop storage and arrive at the required number of data nodes:', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Hadoop Storage (H) = C*R*S/(1-i)\\n\\nLegend\\n\\nC: Average compression ratio\\n\\nR: Replication factor\\n\\nS: Size of data to be moved to Hadoop\\n\\ni: Intermediate factor\\n\\nEstimating Required Hadoop Storage and Number of Data\\n\\nNodes\\n\\nWith no compression, C equals 1. The replication factor is assumed to be 3 and the intermediate factor 0.25 or . The calculation for H in this case becomes:\\n\\nH= 1*3*S/(1-(1/4)) = 3*S/(3/4) = 4*S', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='H= 1*3*S/(1-(1/4)) = 3*S/(3/4) = 4*S\\n\\nThe required Hadoop storage in this instance is estimated to be four times the initial data size.\\n\\nThe following formula can be used to estimate the number of data nodes:\\n\\n(n) = H/D = C*R*S/(1-i)/D\\n\\nD: Disk space available per node\\n\\nLet us assume that 8 TB is the available disk space per node, each node comprising 10 disks of 1 TB capacity each, minus 2 disks for operating system. Also, assuming the initial data size to be 600 TB:\\n\\nN = 600/8 = 75', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='N = 600/8 = 75\\n\\nThus, 75 data nodes are needed in this case.\\n\\nIf complex processing is anticipated, then it is recommended to have at least 10% additional vacant space to accommodate such processing. This 10% is an addition to the 20% set aside for OS installation and operation.\\n\\nHadoop Admin Responsibilities\\n\\nResponsible for implementation and administration of Hadoop Administration.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Testing MapReduce, Hive, Pig and Acess for Hadoop Applications. \\uf0b7 Cluster maintenance tasks like backup, Recovery, Upgrading, Patching. \\uf0b7 Performance Tuning and Capacity planning for clusters. \\uf0b7 Monitor Hadoop Cluster and deploy Security.\\n\\nThe 6 Top Hadoop Distributions that You Can Employ for Your Big Data Needs In Big Data and HadoopTags big data analytics, top Hadoop distributionsFebruary 7, 20183169 Views\\n\\nlearntek\\n\\nApache Hadoop is an excellent software framework that allows the processing of big', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='data elements. It can use the power of commodity hardware by employing a modular\\n\\nsystem and process large sets of data. Hadoop is available in different distributions\\n\\nas companies often deliver it as a packaged deal. It uses the Hadoop Distributed File\\n\\nSystem (HDFS) which allows the use of different platforms and the ability to perform\\n\\nparallel data processing.\\n\\nHere, we discuss the six top Hadoop distributions that you can employ for your big', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='data needs. These are distributions that provide various advantages and you can\\n\\nlearn more about them to understand which one works out the best for your cloud\\n\\ndata processing needs:\\n\\nBig Data Hadoop Certification Training\\n\\n1. Cloudera\\n\\nMost market sources clearly declare Cloudera as the leader even among the top\\n\\nHadoop distributions that are available in the market. The company emerged in 2008\\n\\nand quickly became the top solutions provider for data handling and processing', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='needs. An evaluation by Forrester declared Cloudera as the leader of Hadoop\\n\\ndistributions.\\n\\nCloudera works by first taken the open source Hadoop software elements and then\\n\\nimplementing their proprietary improvements. These changes provide better data\\n\\ngovernance, data availability, better security and overall excellent administration of\\n\\nthe complete software package. The industry experts argue that these are the\\n\\nimportant benefits that big data software must provide to companies that are looking', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='for smart cloud solutions.\\n\\nCloudera Hadoop distribution truly allows your data to shine and provide you a\\n\\nunique insight. Cloudera connects their distribution with other solutions that provide\\n\\nexcellent options for companies\\n\\nthat are\\n\\nlooking\\n\\nfor cloud and Internet of\\n\\nThings (IoT) solutions. Another advantage is that you get improved security which is\\n\\nimportant for running a compliant business in several industries. Hadoop distribution', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='from the top company can help your business gain a competitive edge, with the\\n\\nability to extract the most useful information from the available data sets.\\n\\n2. Amazon Web Services (AWS) Elastic MapReduce\\n\\nThe Amazon Web Services (AWS) also provide a Hadoop distribution as part of their\\n\\noverall cloud-based services. You can be the Elastic MapReduce (EMR) which has\\n\\nbeen present since the earliest Hadoop distributions. It is one of the top Hadoop', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='distributions and is known to provide an excellent structure for organizing your data.\\n\\nIt provides powerful analytics and truly the ability to reduce the workload in your\\n\\norganization by using efficient data handling schemes.\\n\\nThe Amazon EMR is one of the top vendors with a large market share. Amazon has\\n\\nalso handled other efforts in terms of contributing to the Apache community and is\\n\\nwell-known to offer the best customer service. Since the company is already', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='providing all kinds of cloud computing solutions, a Hadoop distribution is likely to\\n\\noffer additional benefits.\\n\\nThe Amazon EMR not only offers Hadoop, it also allows you to use other big data\\n\\nsolutions, where you can employ any platform or a set of services that you find are a\\n\\nperfect match for your data handling needs. There are several data handling\\n\\nfunctions that this Hadoop distribution offers. It allows you to perform complex', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='financial analyses as well as use machine learning to improve processing methods.\\n\\nData transformations are also possible, resulting in a solution with the capacity to\\n\\noffer remedies for all big data handling needs.\\n\\nLearn AWS\\n\\n3. HortonWorks\\n\\nHortonWorks is one of the top Hadoop distributions in the world. They provide the\\n\\nideal big data solutions, as they offer an open source distribution. It continuously\\n\\ncontributes to the Apache community as well. Since it is a member of the Open Data', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Platform started by IBM, it has the capacity to offer the best technological solutions\\n\\nfor all your big data needs.\\n\\nSimilarly, HortonWorks is part of other networks as well, which allows it to offer the\\n\\nbest supporting tools for your data processing tools. The possibility of receiving the\\n\\nbest Hadoop tools is a reality with this Hadoop distribution. The tools are already in\\n\\nuse by large client organizations, and this allows any business switching to Hadoop', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='from HortonWorks to enjoy the proven benefits and facilities.\\n\\nThis distribution is supported by some of the top names in the IT industry. It runs\\n\\njoints with companies like Microsoft, RedHat and Teradata. This distribution offers\\n\\nyou the benefits of flexibility, innovation and quick access to the built-in facilities\\n\\npresent in the distribution package. This package can handle both your static and\\n\\ndynamic data requirements and therefore, works as one of the top Hadoop\\n\\ndistributions.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='distributions.\\n\\n4. IBM InfoSphere Insights\\n\\nIBM cannot be far behind when it comes to providing the top IT solutions for all types\\n\\nof businesses. The InfoSphere Insights is an excellent assimilation of important data\\n\\nmanagement tools. It includes powerful analytics that allow your business to benefit\\n\\nfrom the processing of big data sets. With the IBM Insights, your business can run a\\n\\nfast-paced business model, where your company can quickly accommodate the\\n\\ndynamic work environment.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='dynamic work environment.\\n\\nIBM InfoSphere is one of the top Hadoop distributions, because it offers excellent\\n\\nadvantages in a single package. The company strongly supports its distributions as it\\n\\nis now running a dedicated Apache System ML project. It provides an open source\\n\\nsoftware development with efficient machine learning ability. With each processing of\\n\\ndata, your software tools gain power and produce better results in the future.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='If you have business problems that you must solve, then IBM InfoSphere Insights is\\n\\ncertainly an excellent solution. It is a Hadoop distribution that recognizes identities\\n\\nand automatically generates relevant relationships that help in organizing and\\n\\nprocessing data. It determines new data entries and updates the information pool\\n\\nthroughout the database. Each data transaction is recorded and produces real-time\\n\\nvalue for the clients.\\n\\n5. MapR Distribution', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='value for the clients.\\n\\n5. MapR Distribution\\n\\nMapR Technologies is a solid name and it produces one of the top Hadoop\\n\\ndistributions that offer excellent potential. They can turn to the use of their proprietary\\n\\nfilesystem which provides excellent functionality. It can save trillions of separate data\\n\\nfiles and keep a detailed record of them. This makes the MapR distribution an\\n\\nexcellent choice when you are looking for a robust solution.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='excellent choice when you are looking for a robust solution.\\n\\nMapR understands that Apache Hadoop offers excellent usability when it is\\n\\ncombined with other data processing tools. The presence of a distributed file system\\n\\ncan be enhanced when it can be employed to generate information from the stored\\n\\nbig data elements. Modern technologies like NoSQL databases are possible when\\n\\nyou use MapR where you can perform live event streaming and update data as soon', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='as it becomes part of the Hadoop distribution system.\\n\\nMapR offers 99.999% uptime and is backed by a solid customer support department.\\n\\nThere is no data loss and you gain access to disaster recovery methods as well.\\n\\nWith a powerful security system, it allows businesses to work at a lower total cost of\\n\\nownership when buying an integrated Hadoop-based big data solution.\\n\\n\\uf0b7\\n\\n6. Microsoft Distribution\\n\\nMicrosoft is one of the top names in the software industry; meaning that the top', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Hadoop distributions also include the Microsoft Hadoop Distribution. It provides a\\n\\ndistribution within its Microsoft Azure cloud solution. It provides an excellent\\n\\nfunctionality to an already powerful big data solution that the company offers.\\n\\nThe Microsoft Hadoop Distribution provides more power to Azure and allows the use\\n\\nof SQL servers to hunt for the required data by using a simple set of relevant\\n\\nqueries. It is certainly among the most reliable options, since you receive the', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Microsoft Support ensuring that your software tools are always updated and offer\\n\\nyou the best solution.\\n\\nThese are some top Hadoop distributions that you can employ in your business.\\n\\nThese distributions are all capable of offering you the advantages of running a\\n\\ndistributed file system. Learn more about them to find the Hadoop distribution that\\n\\nbest serves your big data needs!\\n\\nHadoop  Cluster, Properties and its Types', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Before we start learning about the Hadoop cluster first thing we need to know is what actually cluster means. Cluster is a collection of something, a simple computer cluster is a group of various computers that are connected with each other through LAN(Local Area Network), the nodes in a cluster share the data, work on the same task and this nodes are good enough to work as a single unit means all of them to work together. Similarly, a Hadoop cluster is also a collection of various commodity', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='a Hadoop cluster is also a collection of various commodity hardware(devices that are inexpensive and amply available). This Hardware components work together as a single unit. In the Hadoop cluster, there are lots of nodes (can be computer and servers) contains Master and Slaves, the Name node and Resource Manager works as Master and data node, and Node Manager works as a Slave. The purpose of Master nodes is to guide the slave nodes in a single Hadoop cluster. We design Hadoop clusters for storing,', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='a single Hadoop cluster. We design Hadoop clusters for storing, analyzing, understanding, and for finding the facts that are hidden behind the data or datasets which contain some crucial information. The Hadoop cluster stores different types of data and processes them. \\uf0b7 Structured-Data: The data which is well structured like Mysql. \\uf0b7 Semi-Structured Data: The data which has the structure but not the data type like', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='XML, Json (Javascript object notation).\\n\\nUnstructured Data: The data that doesnt have any structure like audio, video. Hadoop Cluster Schema:\\n\\nHadoop Clusters Properties', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='1. Scalability: Hadoop clusters are very much capable of scaling-up and scaling-down the number of nodes i.e. servers or commodity hardware. Lets see with an example of what actually this scalable property means. Suppose an organization wants to analyze or maintain around 5PB of data for the upcoming 2 months so he used 10 nodes(servers) in his Hadoop cluster to maintain all of this data. But now what happens is, in between this month the organization has received extra data of 2PB, in that case, the', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='organization has received extra data of 2PB, in that case, the organization has to set up or upgrade the number of servers in his Hadoop cluster system from 10 to 12(lets consider) in order to maintain it. The process of scaling up or scaling down the number of servers in the Hadoop cluster is called scalability. 2. Flexibility: This is one of the important properties that a Hadoop cluster possesses. According to this property, the Hadoop cluster is very much Flexible means they can handle any type of', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='cluster is very much Flexible means they can handle any type of data irrespective of its type and structure. With the help of this property, Hadoop can process any type of data from online web platforms. 3. Speed: Hadoop clusters are very much efficient to work with a very fast speed because the data is distributed among the cluster and also because of its data mapping capabilitys i.e. the MapReduce architecture which works on the Master-Slave phenomena. 4. No Data-loss: There is no chance of loss of data', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='phenomena. 4. No Data-loss: There is no chance of loss of data from any node in a Hadoop cluster because Hadoop clusters have the ability to replicate the data in some other node. So in case of failure of any node no data is lost as it keeps track of backup for that data. 5. Economical: The Hadoop clusters are very much cost-efficient as they possess the distributed storage technique in their clusters i.e. the data is distributed in a cluster among all the nodes. So in the case to increase the storage we', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='among all the nodes. So in the case to increase the storage we only need to add one more another hardware storage which is not that much costliest.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Types of Hadoop clusters\\n\\n1. Single Node Hadoop Cluster 2. Multiple Node Hadoop Cluster', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='1. Single Node Hadoop Cluster: In Single Node Hadoop Cluster as the name suggests the cluster is of an only single node which means all our Hadoop Daemons i.e. Name Node, Data Node, Secondary Name Node, Resource Manager, Node Manager will run on the same system or on the same machine. It also means that all of our processes will be handled by only single JVM(Java Virtual Machine) Process Instance. 2. Multiple Node Hadoop Cluster: In multiple node Hadoop clusters as the name suggests it contains multiple', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='node Hadoop clusters as the name suggests it contains multiple nodes. In this kind of cluster set up all of our Hadoop Daemons, will store in different-different nodes in the same cluster setup. In general, in multiple node Hadoop cluster setup we try to utilize our higher processing nodes for Master i.e. Name node and Resource Manager and we utilize the cheaper system for the slave Daemons i.e.Node Manager and Data Node.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"Here's a complete roadmap for yo\\n\\nTop Hadoop Distributions You Can Use for\\n\\nHadoop Works\\n\\nby HDFS Tutorial Team\\n\\n8 min read\\n\\nWith the growing demand for big data technologies for analytics and business\\n\\ndecision, the demand for Hadoop distributions has also increased. Now\\n\\ncompanies prefer Hadoop distributions more over the custom clusters.\\n\\nWhen I say custom cluster that means the Hadoop cluster developed by the\\n\\nHadoop Admin from scratch. This includes all the life cycle like building\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='servers, installing Java and then installing Hadoop followed by Hadoop\\n\\necosystems.\\n\\nBut that\\n\\nbecomes a bit tough to handle due to the mess included. And so, companies\\n\\nprefer to go ahead with the top Hadoop distributions over the custom cluster.\\n\\nIf youre working on Hadoop in any company, you can relate to the situation.\\n\\nMost of the company have come across are either using Cloudera Hadoop\\n\\ndistribution or HortonWorks Hadoop distributions followed by mapR.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='If you will see the top Hadoop cloud service providers, these companies are\\n\\nmaking the top positions due to their influence in the big data segment.\\n\\nAlthough there is no absolute winner when we say best Hadoop distributions\\n\\nas all have their pros and cons but still Cloudera, HortonWorks, and mapR are\\n\\nleading the segment.\\n\\nThis list of Hadoop distributions is followed by some tier two products of IBM\\n\\nand Pivotal which are making an impact now. Most of these top Hadoop', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='distributions companies are focusing on key enterprise features such as\\n\\nsecurity, scale, integration, governance and performance.\\n\\nIn this top Hadoop vendors post, we will be sharing 5 best Hadoop vendors\\n\\nwhich are enabling users to use Hadoop from their Hadoop distributions.\\n\\nIf you will see the global Hadoop market forecast for 2020, the industry is\\n\\ncrossing $50 Billion and many other Hadoop vendors will emerge to capture\\n\\nthis market.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='this market.\\n\\nAs per Allied market research, nowadays Hadoop is being used in almost all\\n\\nthe industry for better predictions and market research. Here are some of the\\n\\ntop sectors where it is being used on large scale.\\n\\nI was stunned when I heard a state government is having 3.25 petabyte of data\\n\\nwhich they are using to forecast for crimes and climate. This is the power of\\n\\nbig data and a proper utilization of which can bring the positive impact for\\n\\nsure.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='sure.\\n\\nWe have also written some of the case studies, which you may want to refer to\\n\\nunderstand these industries in detail.\\n\\nHadoop\\n\\nuse\\n\\ncases\\n\\nin\\n\\neducation\\n\\nHadoop use cases in Healthcare\\n\\nsector\\n\\nBefore moving ahead with the post of top Hadoop vendors, lets see why\\n\\nHadoop distributions needed? Why not companies are creating their own\\n\\ncluster and using it.\\n\\nWhy are Hadoop Distributions preferred over\\n\\nCustom Cluster?\\n\\nWell, there are a number of reasons behind this and here I will share few top', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='reasons for the same.\\n\\nIf you have ever tried to design Hadoop cluster there are several things you\\n\\nhave to take care including scalability, availability, and security among the top.\\n\\nThis is not that easy as it sounds to and so need of dedicated big data Hadoop\\n\\ndistributions needed.\\n\\nHere are some of the reasons why companies prefer top Hadoop distributions\\n\\nover the custom cluster.\\n\\nSupport\\n\\nHadoop cluster installation and configuration is not an easy job. Many', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='newbies who start with Hadoop get stuck in the Hadoop installation part only.\\n\\nAnd if you stuck at any point of time, you have very limited sources available\\n\\nto get your query resolved.\\n\\nBut this is not the case with the Hadoop distributions. Their support teams are\\n\\nquite active and ready to help you, in any case, anytime. So, the company\\n\\nusually prefer Hadoop vendors for the cluster.\\n\\nComplete Setup\\n\\nIf you have ever created a Hadoop cluster you must have experienced this', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='issue. If you are designing your own cluster you will have to setup each\\n\\necosystem separately along with Java and servers. This is bit hectic work\\n\\nwhich companies doesnt prefer now.\\n\\nAnd so, they more to some automated solution where all the eggs are in the\\n\\nsame basket.\\n\\nReliability\\n\\nHadoop is known for the reliability and high availability and a single failure (if\\n\\nnot handled) can cause a huge loss. That is the reason for every single issue or', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='failure, we need to take care of it. But if you are working on your own\\n\\ndeveloped cluster, getting resolution is a bit tough as not many resources are\\n\\navailable and so companies prefer Hadoop vendors.\\n\\nHadoop distributors ensure that your cluster is working perfectly and\\n\\nreliability is ensured.\\n\\n5 Top Hadoop Distributions\\n\\nLets start with the 5 top Hadoop distributions available with us and see their\\n\\nfeatures. We will also discuss the Hadoop distributions pros and cons for\\n\\nbetter understanding.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='better understanding.\\n\\n1. Cloudera Hadoop Distribution\\n\\nI am personally a big fan of Hadoop Cloudera distribution and HortonWorks\\n\\ndistributions. These two I have used and can say, they are best Hadoop\\n\\nvendors in the market.\\n\\nCloudera is\\n\\none of the oldest Hadoop distributions available in the market and the most\\n\\ntrusted as well. Their certifications are globally trusted and every Hadoop\\n\\ndevelopers dream.\\n\\nCloudera started their journey in 2008 from Palo Alto, California and provides', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Apache Hadoop-based software, support and services, and training to business\\n\\ncustomers.\\n\\nThey have the operation worldwide and is a NYSE listed company. With a total\\n\\nCloudera Hadoop distribution market valuation of $4.1 Billion, current\\n\\nCloudera is leading the Hadoop market segment.\\n\\nCloudera provides a solution for Hadoop, Machine Learning, and Analytics.\\n\\nCompanies like Cisco, SanDisk, MasterCard, etc. are using Cloudera Hadoop\\n\\ndistributions for their production work.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='distributions for their production work.\\n\\nAlso Check: Default Cloudera Hue Username and Password\\n\\nThe latest version of Cloudera Manager is 5.11 and company keep on updating\\n\\nit. You can also start with Cloudera for free for personal use. Just install\\n\\nCloudera VM and install it and start working.\\n\\n2. HortonWorks Hadoop Distribution\\n\\nHortonWorks is another leading Hadoop distribution available in the market.\\n\\nWithin the very short span of time, they have captured a greater part of the\\n\\nmarket.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='market.\\n\\nHortonWorks started their journey in 2011 from Santa Clara, California,\\n\\nUnited States. It is a NASDAQ (HDP) listed company provides solution\\n\\nglobally in Hadoop and Analytics field.\\n\\nBefore making to the IPO, HortonWorks was evaluated at $1.38 Billion which\\n\\nis far less than Cloudera but the way they are progressing very soon seems to\\n\\ngive a healthy fight.\\n\\nHortonWorks\\n\\nprovides various products in the data center, sandbox, and cloud. Fortune 500\\n\\ncompanies', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='companies\\n\\nlike Samsung, Spotify, Bloomberg, and eBay are using\\n\\nHortonWorks Hadoop distributions.\\n\\nYou can download HDP and start using HortonWorks Hadoop distribution for\\n\\nfree for personal use. HortonWorks also provides certifications which are\\n\\nglobally trusted.\\n\\n3. mapR Hadoop Distribution\\n\\nTogether with Cloudera and HortonWorks, mapR is the top Hadoop\\n\\ndistributions available and choice for the corporates.\\n\\nStarted in 2009 from San Jose, California, United States, currently, they are', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='operated from 10 different locations providing solution globally.\\n\\nSo far mapR has raised $194 million from the market and is planning to go\\n\\nIPO soon. MapR is another leading Hadoop vendor providing Hadoop\\n\\ndistribution for big data Hadoop, data analytics, and insights. They too offer\\n\\nHadoop Certifications and is well accepted in the market. MapR is also a\\n\\nleading Hadoop cloud service provider.\\n\\n4. IBM Infosphere BigInsights Hadoop Distribution', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='4. IBM Infosphere BigInsights Hadoop Distribution\\n\\nIBM Infosphere BigInsights Hadoop Distribution is also an industry standard\\n\\nHadoop distribution combined with IBM cloud products. IBM provides\\n\\nBigSheets and BigInsights as a service via its SmartCloud Enterprise\\n\\nInfrastructure.\\n\\nis comparatively fast and you can easily setup the cluster and push the data in\\n\\nnext 30 minutes with 60 cents per Hadoop cluster, per hour.\\n\\n5. Microsoft Azure HDInsight Cloud-based Hadoop\\n\\nDistribution', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='5. Microsoft Azure HDInsight Cloud-based Hadoop\\n\\nDistribution\\n\\nAs per the market research firm, Forrester Microsoft Azures HDInsight has\\n\\nbeen rated 4/5 while Cloudera and HortonWorks securing 5/5.\\n\\nIt\\n\\nAlthough Microsoft is not known for the open source software but by looking\\n\\nat the potential and popularity of Hadoop, they have come up with HDInsight\\n\\nwhich supports Windows platform.\\n\\nIt runs with majorly two public products- Windows Azures HDInsight\\n\\nparticularly developed to run on Azure.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='particularly developed to run on Azure.\\n\\nCloudera vs HortonWorks vs MapR\\n\\nLets see few differences between Cloudera, HortonWorks, and mapR the\\n\\nleading Hadoop vendors.\\n\\nIf we talk about the infrastructure part, the major comparison is as follows\\n\\nAnd in terms of Hadoop distribution market share and market valuation, the\\n\\nbelow figure will give you enough details.\\n\\n\\uf0b7\\n\\nWrapping it up!\\n\\nThis was all about the top Hadoop distributions available in the market for', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Hadoop. I hope you got a clear understanding of the Hadoop vendors.\\n\\nAlthough the list is just not limited to the above 5 best Hadoop distribution\\n\\nproviders. Many big companies are trying to get into this segment and making\\n\\ntheir ways. I would like to mention few of those here.\\n\\nAmazon Elastic MapReduce \\uf0b7 Pivotal Big Data Suite \\uf0b7 Datameer Professional \\uf0b7 Datastax Enterprise Analytic \\uf0b7 Dell- Cloudera Apache Hadoop Solution\\n\\nI hope very soon we will have some more Hadoop vendors with some amazing', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='features available. I would like to know which Hadoop distribution you use.\\n\\nArchitecture and Working of Hive\\n\\nPrerequisite  Introduction to Hadoop, Apache Hive The major components of Hive and its interaction with the Hadoop is demonstrated in the figure below and all the components are described further: \\uf0b7 User Interface (UI) ', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='As the name describes User interface provide an interface between user and hive. It enables user to submit queries and other operations to the system. Hive web UI, Hive command line, and Hive HD Insight (In windows server) are supported by the user interface.\\n\\nHive Server  It is referred to as Apache Thrift Server. It accepts the request from different clients and provides it to Hive Driver.\\n\\nDriver ', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Driver \\n\\nQueries of the user after the interface are received by the driver within the Hive. Concept of session handles is implemented by driver. Execution and Fetching of APIs modelled on JDBC/ODBC interfaces is provided by the user.\\n\\nCompiler ', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Compiler \\n\\nQueries are parses, semantic analysis on the different query blocks and query expression is done by the compiler. Execution plan with the help of the table in the database and partition metadata observed from the metastore are generated by the compiler eventually.\\n\\nMetastore ', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Metastore \\n\\nAll the structured data or information of the different tables and partition in the warehouse containing attributes and attributes level information are stored in the metastore. Sequences or de-sequences necessary to read and write data and the corresponding HDFS files where the data is stored. Hive selects corresponding database servers to stock the schema or Metadata of databases, tables, attributes in a table, data types of databases, and HDFS mapping.\\n\\nExecution Engine ', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Execution Engine \\n\\nExecution of the execution plan made by the compiler is performed in the execution engine. The plan is a DAG of stages. The dependencies within the various stages of the plan is managed by execution engine as well as it executes these stages on the suitable system components.\\n\\nDiagram  Architecture of Hive that is built on the top of Hadoop In the above diagram along with architecture, job execution flow in Hive with Hadoop is demonstrated step by step. \\uf0b7 Step-1: Execute Query ', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Interface of the Hive such as Command Line or Web user interface delivers query to the driver to execute. In this, UI calls the execute interface to the driver such as ODBC or JDBC.\\n\\nStep-2: Get Plan \\n\\nDriver designs a session handle for the query and transfer the query to the compiler to make execution plan. In other words, driver interacts with the compiler.\\n\\nStep-3: Get Metadata ', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Step-3: Get Metadata \\n\\nIn this, the compiler transfers the metadata request to any database and the compiler gets the necessary metadata from the metastore.\\n\\nStep-4: Send Metadata \\n\\nMetastore transfers metadata as an acknowledgment to the compiler.\\n\\nStep-5: Send Plan \\n\\nCompiler communicating with driver with the execution plan made by the compiler to execute the query.\\n\\nStep-6: Execute Plan \\n\\nExecute plan is sent to the execution engine by the driver.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Execute plan is sent to the execution engine by the driver.\\n\\nExecute Job Job Done \\uf0b7 \\uf0b7 Dfs operation (Metadata Operation)\\n\\nStep-7: Fetch Results \\n\\nFetching results from the driver to the user interface (UI).\\n\\nStep-8: Send Results \\n\\nResult is transferred to the execution engine from the driver. Sending results to Execution engine. When the result is retrieved from data nodes to the execution engine, it returns the result to the driver and to user interface (UI).\\n\\nAdvantages of Hive Architecture:', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Scalability: Hive is a distributed system that can easily scale to handle large volumes of data by adding more nodes to the cluster. Data Accessibility: Hive allows users to access data stored in Hadoop without the need for complex programming skills. SQL-like language is used for queries and HiveQL is based on SQL syntax. Data Integration: Hive integrates easily with other tools and systems in the Hadoop ecosystem such as Pig, HBase, and MapReduce. Flexibility: Hive can handle both structured and', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='and MapReduce. Flexibility: Hive can handle both structured and unstructured data, and supports various data formats including CSV, JSON, and Parquet. Security: Hive provides security features such as authentication, authorization, and encryption to ensure data privacy.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Disadvantages of Hive Architecture:', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='High Latency: Hives performance is slower compared to traditional databases because of the overhead of running queries in a distributed system. Limited Real-time Processing: Hive is not ideal for real-time data processing as it is designed for batch processing. Complexity: Hive is complex to set up and requires a high level of expertise in Hadoop, SQL, and data warehousing concepts. Lack of Full SQL Support: HiveQL does not support all SQL operations, such as transactions and indexes, which may limit the', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='such as transactions and indexes, which may limit the usefulness of the tool for certain applications. Debugging Difficulties: Debugging Hive queries can be difficult as the queries are executed across a distributed system, and errors may occur in different nodes.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='What is Hive?\\n\\nHive is a data warehousing package built on the top of Hadoop. A Data warehouse is a\\n\\nplace where you store a massive amount of data. This data is always ready to be\\n\\naccessed, and ready to be reported so I have a BI tool like Power BI which can directly\\n\\nbe installed on the data warehousing platform and produce intellectual reports.\\n\\nHive is used for data analysis means to extract meaningful information from big\\n\\ndata. It is created for the users who are comfortable with SQL because the', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='default tool to process the data over Hadoop is Map-reduce which works with\\n\\nJava programming language and users not comfortable with java can use Hive to\\n\\nprocess the data over Hadoop.\\n\\nThe query language of the hive has a separate name known as HiveQL or HQL\\n\\n(Hive query language). It is used for managing and querying structured data.\\n\\nwhile working with Hive there is no need for java.\\n\\nHive Background\\n\\nIn 2006-2007 RDBMS was used to store and manage the data. At that time Cron jobs', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='were used for data collection (Job scheduling). And for ETL python was used. In this\\n\\nway to generate and collect data Oracle was used and could not support a large amount\\n\\nof data and needed more coding in java. SQL was easy to operate and in 2007 the Hive\\n\\nwas developed by Facebook. And today more than 500 Tb of data is managed by\\n\\nFacebook.\\n\\nFeatures of Hive\\n\\nWe know what is Hive and why it was developed by Facebook. It is also important to', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='understand other than querying and analyzing the data what features Hive serves so it\\n\\nproves to be the best Hadoop warehousing tool for data analyzing and creating BI\\n\\nreports.\\n\\n1. It provides features of Partitioning and bucketing\\n\\n2. Easy to plug in custom map-reduce code.  If we want to process map-reduce\\n\\nunstructured data so we can embed it and process it using Hive\\n\\n3. Data is stored on HDFS  The processing of data takes place in Hive but\\n\\noriginally the data always resides in Hive.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='originally the data always resides in Hive.\\n\\n4. It uses Hadoop for fault tolerance\\n\\n5. Hive is used for data mining, document indexing, predictive modelling (predicting\\n\\nuser behaviour), custom facing UI.\\n\\nLimitations of using Hive\\n\\nHive is simple, easy, and effective yet there are some of the limitations of any tool to\\n\\nwork with and the same goes with Hive. When you start to work with a hive certain\\n\\npoints need to understand and take care of and as well some points need to be avoided', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='because Hive as a data warehousing tool is created to work with a massive amount of\\n\\nstructured data stored in HDFS.\\n\\nNot recommended for Row-level updates  If you plan to insert, update, or delete\\n\\nin hive then it is not recommended on row-level updates because data is too\\n\\nlarge and who will like to play with a single row.\\n\\nLatency for queries is high  If you want the instant result from queries then hive\\n\\nis not recommended because hive works with a large amount of data and needs', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='some time to execute the query.\\n\\nNot Designed for OLTP  Not recommended for online transaction processing\\n\\nWhen we load data in SQL and if there is any delimiter problem then it will throw\\n\\nan error in data loading while Hive will load the data successfully and when you\\n\\nrun the query then it will throw the error in data type mismatch.\\n\\nRDBMS can be used for OLTP processing while Hive is not recommended use\\n\\nfor OLTP data processing.\\n\\nSetup Hive on your System', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='for OLTP data processing.\\n\\nSetup Hive on your System\\n\\nIt is the perfect time to install Hive and get ready to practically learn Hive Query\\n\\nLanguage(HQL)\\n\\nStep-1) Installing Java JDK\\n\\nHadoop completely runs on JAVA so the first thing is to install and set up the Java JDK\\n\\npath. You can install the latest JDK version through this link. If you already have java\\n\\ninstalled then no need to install it again.\\n\\nStep-2) Set Path Variables', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Step-2) Set Path Variables\\n\\nTo make java available to all users you need to move it to the /usr/local. open root\\n\\nand type the following commands.\\n\\n$ su password: # mv jdk1.7.0_71 /usr/local/ # exit\\n\\nFor setting up Java home path variables you can add the following commands to\\n\\nthe ~/.bashrc file.\\n\\nexport JAVA_HOME=/usr/local/jdk1.7.0_71 export PATH=$PATH:$JAVA_HOME/bin\\n\\nStep-3) Installing Hadoop\\n\\nHive is built on top of Hadoop so Hadoop must be installed on your system. If you have', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='already installed the Hadoop tar file then on the command prompt check the version\\n\\nusing the following command.\\n\\n$ hadoop version\\n\\nIf you have not downloaded the Hadoop then download and extract Hadoop through\\n\\nthese links. After then you can set Hadoop environment variables by appending the\\n\\nfollowing commands to the ~/.bashrc file.\\n\\nStep-4) Set Hadoop configurations\\n\\nThe core-site.xml file contains necessary information regarding port address, memory', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='allocated to file system, etc. open the XML file and add the following properties between\\n\\nconfiguration tags.\\n\\nfs.default.name hdfs://localhost:9000\\n\\nStep-5) Download and Install Hive\\n\\nDownload Hive using this link. You can use the following command to verify the\\n\\ndownload and extract hive archive.\\n\\nIf you face any problem while setting the Hive environment then you can read all steps\\n\\nin detail and find all necessary configurations required on this blog.\\n\\nBasic Hive Query Commands', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Basic Hive Query Commands\\n\\nThe HQL query language is very similar to SQL. Only there are some additional\\n\\nfunctions and syntax to deal with CSV files and a large amount of data. If you have\\n\\ninstalled the hive then you can simply open the command prompt and type the hive\\n\\nwhich will launch the hive command line interface and make sure that the hive can talk\\n\\nto the appropriate Hadoop cluster. Type the below command which will show you the\\n\\nnamenode address and IP address\\n\\nset fs.defaultFS;', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='namenode address and IP address\\n\\nset fs.defaultFS;\\n\\nTo validate we are connected with the namenode we can list the files present in\\n\\nthe root directory. So it makes sure that the hive is connected to the namenode.\\n\\ndfs -ls /user/root\\n\\nNow we are ready to run queries. To see all the databases present in the\\n\\nnamenode use the below command.\\n\\nshow databases\\n\\nswitch to any database\\n\\nuse database-name\\n\\nTo create a new database using the create command\\n\\ncreate database database-name', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='create database database-name\\n\\nTo list the tables in a particular database we can use the below command.\\n\\nshow tables\\n\\nNow if you are familiar with SQL then you must be familiar with the SQL clauses l ike\\n\\nFROM, where, group by, order by, having, etc. To understand the structure of the table\\n\\nuses the describe command.\\n\\ndescribe table-name\\n\\nDisplay all records from a particular table.\\n\\nselect * from table-name\\n\\nAnd if you want to limit the output to a particular number of rows then you can use limit', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"constraint.\\n\\nAnalyzing File Tabular Data using HQL\\n\\nNow we will load one data file or create a table and insert some data into it and try to\\n\\nretrieve the data based on certain conditions. We have an orders table wh ich has 4\\n\\ncolumns. you can find the table here.\\n\\n1) Using all Hive Query language get several orders by order status for a given\\n\\ndate 2013-12-14.\\n\\nselect order_status, count(1), FROM orders where order_date = '2013-12-14 00:00:00.0' GROUP BY order_status ORDER BY order_status;\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"When we are not using join there is only one table in from clause and after that comes\\n\\nwhere clause in which we can give an expression which can be a derived field,\\n\\nconstant, or valid expression that gives a value out of it.\\n\\n2) write a Hive QL to get the number of complete orders for each date before\\n\\n2013-12-14.\\n\\nselect order_date, count(1) from orders where order_date <= '2013-12-14 00:00:00.0' AND order_status = 'Complete' GROUP BY order_date ORDER BY order_date;\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"3) Get several pending, reviewed, and on-hold orders for each date for the month\\n\\nof 2013 December.\\n\\nselect order_date, count(1) from orders where order_date LIKE '2013-12%' AND order_status in ('PENDING', 'PENDING_PAYMENT', 'PAYMENT_REVIEW', 'ONHOLD') GROUP BY order_date ORDER BY order_date;\\n\\nWe can also use between operator to find the records for a particular month which\\n\\nstates that the record greater than the 1st date and smallest than the last date of the\\n\\nmonth.\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"month.\\n\\nselect order_date, count(1) from orders where order_date BETWEEN '2013-12-01 00:00:00.0' AND '2013-12-31 00:00:00.0' AND order_status in ('PENDING', 'PENDING_PAYMENT', 'PAYMENT_REVIEW', 'ONHOLD') GROUP BY order_date ORDER BY order_date;\\n\\nCreate a Managed Table in Hive\\n\\nManaged tables are also known as internal tables where the entire lifecycle of table\\n\\ndata is managed and controlled by Hive. All the read and write operation with managed\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"table is performed using Hive SQL commands. There is no precision in the hive in\\n\\nnumeric data type and you can define precision on the string data type.\\n\\ncreate Table orders_demo ( order_id INT, order_date DATE, order_cust_id INT, order_status VARCHAR(30) ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;\\n\\nExplanation ~ In MySQL it is date time and in Hive, we can give Date. Hive also\\n\\nsupports string data type so instead of defining Varchar you can also assign it a string.\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='After that, we need to define the row format which defines that the data that we are\\n\\ncopying in the table is separated by which delimiter like in CSV files all the rows the\\n\\nfields are separated by a comma, sometimes by a pipe so this need to be defined. We\\n\\ncan also load the files in which lines are terminated by a delimiter and mostly it is by a\\n\\nnew line character. At last, you need to define the file format in which the file is stored', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='like text file, sequence file, AVRO, etc. The default file format is a text file and yo u can\\n\\nalso define a custom format.\\n\\nTo view the data structure using describe command and if you want to observe it in\\n\\ntabular structure then we can use the format keyword along with describing command.\\n\\ndescribe formatted orders_demo\\n\\nCreating External Table in Hive\\n\\nAn external table is a table where Hive has loose coupling with table data and while', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"creating defining the external storage location is compulsory. First to load the data let us\\n\\ncreate one sample table and then we will create an external table.\\n\\nCREATE TABLE deck_of_cards ( COLOR string, SUIT string, PIP string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE;\\n\\nLoad the data from the local file system\\n\\nMost of the time you are supposed to download the data from some external means and\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"then load it in Hive and perform all the required operations. Hive has very simple syntax\\n\\nto load the data from the local file system where you only need to define the type like\\n\\nthe local file system and path where the data file is stored in your system.\\n\\nLOAD DATA LOCAL INPATH '/root/demo/data/cards/deckofcards.txt' INTO TABLE deck_of_cards;\\n\\nLoad data from HDFS\\n\\nHDFS is the main storage directory of Hadoop and being into the data analyzing team\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"most of the time organization data is directly ingested into HDFS and you are supposed\\n\\nto load the data from HDFS to Hive and perform required operations and execute\\n\\nqueries.\\n\\nLOAD DATA INPATH '/user/root/cards/deckofcards.txt' INTO TABLE deck_of_cards;\\n\\nOverwriting the Existing Data File\\n\\nSometimes wrong data might load into the system so without deleting the table structure\\n\\nyou want to only change the table content or overwrite the content in the table so we\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"use overwrite keyword with the Load command in Hive to overwrite the data over the\\n\\nexisting file.\\n\\nLOAD DATA LOCAL INPATH '/root/demo/data/cards/deckofcards.txt' OVERWRITE INTO TABLE deck_of_cards;\\n\\nTo create an external table we use the External keyword with create command.\\n\\nDownload the text file that contains the data about cards through this GitHub link. here\\n\\nand then create a new folder in the root directory as cards and places a text file in the\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='cards folder and then create an external table from the Hive terminal. And you can\\n\\ncheck that file is correctly placed inside the cards folder using the below command and\\n\\nthen create a database as cards, switch to the cards database, and create an external\\n\\ntable using the below commands.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"table using the below commands.\\n\\nhadoop fs -ls /user/root/cards create database cards use cards CREATE EXTERNAL TABLE deck_of_cards_external ( color string, suit string, pip string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/user/root/cards';\\n\\nIf you drop the external table then you will only lose the structure of the table and the\\n\\ndata is not deleted. Indeed if you drop managed table then you will lose data as well as\\n\\nthe table structure.\\n\\nCreate Hive Partitioned Table\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='the table structure.\\n\\nCreate Hive Partitioned Table\\n\\nPartitioning in hive means dividing the table into sub-parts based on the values of a\\n\\nparticular column. The advantage of partitioning is the response time of query increases\\n\\nand becomes much faster to retrieve data. It is similar is remove functional dependency\\n\\nfrom tables in DBMS. As per the data modeling perspective, you should be aware of\\n\\nwhich column values you have to partition the data if you have a date necessary in data', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"analysis so you might have to partition the data yearly, monthly, weekly, etc.\\n\\nlet us create a partition table called orders which we previously created and the only\\n\\ndifference is the partition clause.\\n\\ncreate TABLE orders ( order_id INT, order_date DATE, order_cust_id INT, order_status string ) PARTITIONED BY (order_month string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE;\\n\\nIf you list the files then you will not see anything but if you want to add a partition then\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='use add partition command as shown below. If we want to retrieve the first 10 dates and\\n\\n10 characters because the date is 10 characters from the order month then we can use\\n\\nthe Unix time function\\n\\nselect from_unixtime(cast(substr(order_date, 1, 10) AS bigint)) from orders limit 10;\\n\\nLet us print all the details including id, status, date, and month.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='select order_id, from_unixtime(cast(substr(order_date, 1, 10) AS bigint)) order_date, order_cust_id, order_status, substr(from_unixtime(cast(substr(order_date, 1, 10) AS bigint)), 1, 7) order_month from orders limit 10;\\n\\nCreate Hive Bucketed Table (Bucketing)\\n\\nBucketing is similar to hash partitioning. The syntactical difference between partitioning\\n\\nand bucketing is that in the partition you not only specify the column names but also the', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='data type. The concept of partitioning helps segregate the data in multiple directories\\n\\nwhile this feature might not be helpful in all scenarios. Suppose you want to partition\\n\\ntables based on the geographic condition or population of the country then every\\n\\ncountry will have a different number of partitions. Hence to solve this problem Bucketing\\n\\nis introduced in Hive.\\n\\nIn short bucketing is a hashing technique where the number of required buckets and', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='data in each bucket depends on a certain hash function. And the bucketed table will\\n\\ncreate almost equally distributed data buckets. To divide the table into buckets we use\\n\\nClustered By Clause.\\n\\nAdvantages of Bucketing\\n\\n1. Bucketed tables offer better sampling compared to non-bucketed tables.\\n\\n2. Bucketing can also be performed without partitioning.\\n\\n3. Similar to partitioning bucketed tables also offer a fast query response compared\\n\\nto non-bucketed tables.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"to non-bucketed tables.\\n\\n4. Bucketing offers flexibility to data storage in each bucket like you can sort the\\n\\ndata based on one or more columns.\\n\\ncreate table orders bucket ( order_id INT, order_date DATE, order_cust_id INT, order_status string ) CLUSTERED BY (order_id) INTO 16 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE;\\n\\nTransactions in Hive (Insert, Update, Delete)\\n\\nWe have learned to partition, bucketing, query, and retrieve the data from Hive. It is very\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='similar to SQL and the only benefit is it is scalable and can work with different file\\n\\nformats with efficient data analyzing and processing quality. Its time to learn how we\\n\\ncan insert, update or delete the data from Hive tables which is also known as DML in\\n\\nthe standard database. First, create a sample table to learn how to perform transactions\\n\\nin the hive.\\n\\nCREATE Table hive_transactions ( i INT, j string );', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"CREATE Table hive_transactions ( i INT, j string );\\n\\nJust like SQL the syntax for inserting the data directly into the table is the same as using\\n\\nthe Insert command.\\n\\nINSERT INTO table hive_transactions- values (1, 'Analytics Vidhya');\\n\\nYou can run the SELECT command to extract the data. Now If you want to delete the\\n\\ntransactions so you will use the delete command but the query will fail because unlike\\n\\nstandard databases the hive has certain limitations on transactions. In DBMS you have\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"the option to control the transaction through commit and rollback but in the hive, it is\\n\\nalways auto-commit which means if the transaction is successful then it will be visible\\n\\nelse not visible. Hive only supports transactions on ORC format files and the table\\n\\nneeds to be bucketed.\\n\\nCREATE Table hive_transactions (i INT, j string) CLUSTERED BY (i) INTO 4 buckets STORED AS ORC tblproperties('transactional' = 'true');\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content=\"After running the above create command again try to insert the data into a table and it\\n\\nwill execute fine. And now if we run the Update command then it will work.\\n\\nUPDATE hive_transactions set j = 'vidhya' where i = 1;\\n\\nYou can preview the data then there is another directory where the file is updated. We\\n\\ncan also perform a delete operation to delete a particular row.\\n\\nDELETE from hive_transactions WHERE i = 1;\\n\\nThis is all about transactions performed in Hive and you can try advance inserts and\", metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='inserting multiple rows at a single time in Hive.\\n\\nConclusion\\n\\nHive is a data warehousing tool that makes analyzing and processing of big data very\\n\\nsimple, and efficient over Hadoop where with help of simple SQL queries along with\\n\\nsome Delimiter and file formats you can control the massive amount of data. Hive\\n\\nproves to be the best Hadoop tool for data analysis purposes. In this article, we have\\n\\nstarted Hive with the theoretical part and then set up Hive on our system and played', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='with different Hive queries. Let us summarize the important key points that we learned\\n\\nand need to be remembered in regards to Hive.\\n\\n1. Hive is a data warehousing tool that is built on top of Hadoop used for analyzing\\n\\ndata and helping to generate intellectual reports with help of BI tools.\\n\\n2. Two main concepts to arrange and play with data in Hive are Partitioning and\\n\\nBucketing.\\n\\n3. Partitioning will only work when there is a limited number of partitions or when', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='we have an equal number of size partitions. Otherwise, we need to use\\n\\nBucketing.\\n\\n4. Bucketing also creates the evenly stored data in buckets with help of the Hash\\n\\nand modulus function.\\n\\n5. While creating a table using select clauses you cannot change or customize the\\n\\nname and type of columns which we can do in some RDBMS software.\\n\\n6. Monitor Hadoop Cluster and deploy Security.\\n\\nUNIT-IV\\n\\nWhat are Hive Optimization Techniques?', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='UNIT-IV\\n\\nWhat are Hive Optimization Techniques?\\n\\nHowever, to run queries on petabytes of data we all know that hive is a query language which is similar to SQL built on Hadoop ecosystem. So, there are several Hive optimization techniques to improve its performance which we can implement when we run our hive queries.\\n\\nTypes of Query Optimization Techniques in Hive\\n\\nFollowing are the Hive optimization techniques for Hive Performance Tuning, lets discuss them one by one:\\n\\na. Tez-Execution Engine in Hive', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='a. Tez-Execution Engine in Hive\\n\\nTez Execution Engine  Hive Optimization Techniques, to increase the Hive performance of our hive query by using our execution engine as Tez. On defining Tez, it is a new application framework built on Hadoop Yarn.\\n\\nThat executes complex-directed acyclic graphs of general data processing tasks. However, we can consider it to be a much more flexible and powerful successor to the map-reduce framework.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='In addition, to write native YARN applications on Hadoop that bridges the spectrum of interactive and batch workloads Tez offers an API framework to developers. To be more specific, to work with petabytes of data over thousands of nodes it allows those data access applications.\\n\\nb. Usage of Suitable File Format in Hive\\n\\nORCFILE File Formate  Hive Optimization Techniques, if we use appropriate file format on the basis of data. It will drastically increase our query performance.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Basically, for increasing your query performance ORC file format is best suitable. Here, ORC refers to Optimized Row Columnar. That implies we can store data in an optimized way than the other file formats.\\n\\nTo be more specific, ORC reduces the size of the original data up to 75%. Hence, data processing speed also increases. On comparing to Text, Sequence and RC file formats, ORC shows better performance.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Basically, it contains rows data in groups. Such as Stripes along with a file footer. Therefore, we can say when Hive is processing the data ORC format improves the performance.\\n\\nc. Hive Partitioning\\n\\nHive Partition  Hive Optimization Techniques, Hive reads all the data in the directory Without partitioning. Further, it applies the query filters on it. Since all data has to be read this is a slow as well as expensive.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Also, users need to filter the data on specific column values frequently. Although, users need to understand the domain of the data on which they are doing analysis, to apply the partitioning in the Hive.\\n\\nBasically, by Partitioning all the entries for the various columns of the dataset are segregated and stored in their respective partition.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Hence, While we write the query to fetch the values from the table, only the required partitions of the table are queried. Thus it reduces the time taken by the query to yield the result.\\n\\nd. Bucketing in Hive\\n\\nBucketing in Hive  Hive Optimization Techniques, lets suppose a scenario. At times, there is a huge dataset available. However, after partitioning on a particular field or fields, the partitioned file size doesnt match with the actual expectation and remains huge.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Still, we want to manage the partition results into different parts. Thus, to solve this issue of partitioning, Hive offers Bucketing concept. Basically, that allows the user to divide table data sets into more manageable parts.\\n\\nHence, to maintain parts that are more manageable we can use Bucketing. Through it, the user can set the size of the manageable parts or Buckets too.\\n\\ne. Vectorization In Hive', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='e. Vectorization In Hive\\n\\nVectorization In Hive  Hive Optimization Techniques, to improve the performance of operations we use Vectorized query execution. Here operations refer to scans, aggregations, filters, and joins. It happens by performing them in batches of 1024 rows at once instead of single row each time.\\n\\nHowever, this feature is introduced in Hive 0.13. It significantly improves query execution time, and is easily enabled with two parameters settings:\\n\\nset hive.vectorized.execution = true', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='set hive.vectorized.execution = true\\n\\nset hive.vectorized.execution.enabled = true\\n\\nf. Cost-Based Optimization in Hive (CBO)\\n\\nCost-Based Optimization in Hive  Hive Optimization Techniques, before submitting for final execution Hive optimizes each Querys logical and physical execution plan. Although, until now these optimizations are not based on the cost of the query.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='However, CBO, performs, further optimizations based on query cost in a recent addition to Hive. That results in potentially different decisions: how to order joins, which type of join to perform, the degree of parallelism and others.\\n\\nTo use CBO, set the following parameters at the beginning of your query:\\n\\nset hive.cbo.enable=true;\\n\\nset hive.compute.query.using.stats=true;\\n\\nset hive.stats.fetch.column.stats=true;\\n\\nset hive.stats.fetch.partition.stats=true;', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='set hive.stats.fetch.partition.stats=true;\\n\\nThen, prepare the data for CBO by running Hives analyze command to collect various statistics on the tables for which we want to use CBO.\\n\\ng. Hive Indexing\\n\\nHive Index  Hive Optimization Techniques, one of the best ways is Indexing. To increase your query performance indexing will definitely help. Basically, for the original table use of indexing will create a separate called index table which acts as a reference.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='As we know, there are many numbers of rows and columns, in a Hive table. Basically, it will take a large amount of time if we want to perform queries only on some columns without indexing. Because queries will be executed on all the columns present in the table.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Moreover, there is no need for the query to scan all the rows in the table while we perform a query on a table that has an index, it turned out as the major advantage of using indexing. Further, it checks the index first and then goes to the particular column and performs the operation.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='Hence, maintaining indexes will be easier for Hive query to look into the indexes first and then perform the needed operations within less amount of time. Well, time is the only factor that everyone focuses on, eventually.\\n\\nThis was all about Hive Optimization Techniques Tutorial. Hope you like our explanation of Hive Performance Tuning.', metadata={'source': '/content/docs/HADOOP ADMINISTRATION AND BDA NOTES.pdf'}),\n",
              " Document(page_content='G.H. RAISONI COLLEGE OF ENGINEERING \\n\\nAND MANAGEMENT, WAGHOLI, PUNE.\\n\\nDepartment of AI & AIML\\n\\nInternship Final Review\\n\\n                                   Guide: Prof . Nutan Raut\\t\\t\\t\\n\\nName: Varpe Aditya Navnath\\n\\nContents\\n\\nIntroduction\\n\\nInternship \\n\\nProjects\\n\\nInternship work Reference\\n\\nsnapshots\\n\\nConclusion\\n\\nIntroduction\\n\\nCompany Name: Deng InfoTech Solutions Pvt. Ltd.\\n\\nCompany Domain: IT Services and IT Consulting\\n\\nDescription:', metadata={'source': '/content/docs/Internship Review .pptx'}),\n",
              " Document(page_content='Deng InfoTech Solutions Pvt. Ltd is a leading provider of research and industrial solutions, excelling in running projects tailored to diverse fields. With 7+ years of expertise, we boast a client base exceeding 250 across various industries. Our commitment to innovation, technology, and client satisfaction drives us to deliver cutting-edge solutions. From customized research initiatives to industrial applications, Deng InfoTech Solutions Pvt. Ltd is your trusted partner for', metadata={'source': '/content/docs/Internship Review .pptx'}),\n",
              " Document(page_content='Deng InfoTech Solutions Pvt. Ltd is your trusted partner for success, ensuring excellence and growth in every project we undertake', metadata={'source': '/content/docs/Internship Review .pptx'}),\n",
              " Document(page_content='Internship\\n\\nDesignation: Machine Learning Intern\\n\\nResponsibilities:\\n\\nCollaborate with the machine learning team to assist in the development and implementation of innovative algorithms and models.\\n\\nConduct thorough data analysis, pre-processing, and feature engineering to contribute to the enhancement of machine learning solutions.\\n\\nSupport the team in optimizing and fine-tuning existing models, ensuring they meet performance benchmarks and deliver actionable insights.', metadata={'source': '/content/docs/Internship Review .pptx'}),\n",
              " Document(page_content='Stay abreast of the latest advancements in machine learning and contribute ideas for the improvement of existing processes.\\n\\nProjects \\n\\nTitle: Intrusion Detection System (IDS)\\n\\nContent:\\n\\nProject Objective:\\n\\nTo enhance cybersecurity by detecting unauthorized access and potential threats in real-time.\\n\\nKey Components and Process:\\n\\nData Collection: Integrates with monitoring systems to gather network traffic data.\\n\\nPreprocessing: Cleans and normalizes the data for accurate analysis.', metadata={'source': '/content/docs/Internship Review .pptx'}),\n",
              " Document(page_content='Modeling: Uses machine learning algorithms to identify patterns indicative of intrusions.\\n\\nIntrusion Detection: Applies the trained model to detect anomalies in real-time.\\n\\nAlarm Generation: Triggers alerts for detected intrusions and generates detailed reports.\\n\\nProjects \\n\\nSchematic Diagram:\\n\\nIDS MODEL ARCHITECTURE\\n\\nProjects \\n\\nTitle: Credit Card Fraud Detection\\n\\nContent:\\n\\nProject Objective:\\n\\nTo protect financial transactions by identifying fraudulent activities using predictive models.', metadata={'source': '/content/docs/Internship Review .pptx'}),\n",
              " Document(page_content='Key Components and Process:\\n\\nTransaction Data: Utilizes a dataset of credit card transactions.\\n\\nPreprocessing: Cleans and normalizes transaction data.\\n\\nFeature Engineering: Extracts relevant features such as transaction amount, time, and location.\\n\\nModel Training: Uses machine learning algorithms to distinguish between legitimate and fraudulent transactions.\\n\\nFraud Detection: Applies the model to new transactions to identify potential fraud.\\n\\nProjects \\n\\nTitle: Fire Detection with Alarms\\n\\nContent:', metadata={'source': '/content/docs/Internship Review .pptx'}),\n",
              " Document(page_content='Projects \\n\\nTitle: Fire Detection with Alarms\\n\\nContent:\\n\\nProject Objective:\\n\\nTo enhance safety by detecting fires early using video analysis and generating timely alarms.\\n\\nKey Components and Process:\\n\\nVideo Capture: Uses cameras to monitor the area for signs of fire.\\n\\nFrame Grabbing: Captures frames from the video feed for analysis.\\n\\nVideo Transfer Module: Converts frames into digital format.\\n\\nFire Detection Module: Analyzes frames using machine learning to detect fire.', metadata={'source': '/content/docs/Internship Review .pptx'}),\n",
              " Document(page_content='Alarm System: Triggers alarms when fire is detected.\\n\\nProjects \\n\\nSchematic Diagram:\\n\\nFire Detection with Alarms Model Architecture\\n\\nProjects \\n\\nTitle: Fake News Detection Using BERT\\n\\nContent:\\n\\nProject Objective:\\n\\nTo combat misinformation by accurately identifying fake news using advanced NLP techniques.\\n\\nKey Components and Process:\\n\\nDataset: Uses a labeled dataset of news titles.\\n\\nPreprocessing: Removes stopwords and tokenizes the text.\\n\\nModel Training: Trains a BERT model on the dataset.', metadata={'source': '/content/docs/Internship Review .pptx'}),\n",
              " Document(page_content='Model Training: Trains a BERT model on the dataset.\\n\\nFake News Detection: Classifies news titles as fake or real.\\n\\nAlert System: Flags fake news for review.\\n\\nProgress Report\\n\\nAs Machine Learning Intern with Deng InfoTech Solutions Pvt. Ltd, Now I have an Hands on Experience on Skills Like\\n\\nMachine Learning \\t\\t\\t\\t\\n\\nDeep learning \\n\\nData Cleaning\\n\\nPython\\n\\nYolo v8 \\n\\nInternship work reference\\n\\nTip: Due to No Data Disclosure Policy Of Organization, This is For Reference\\n\\nInternship work reference', metadata={'source': '/content/docs/Internship Review .pptx'}),\n",
              " Document(page_content='Internship work reference\\n\\nTip: Due to No Data Disclosure Policy Of Organization, This is For Reference\\n\\nInternship work reference\\n\\nTip: Due to No Data Disclosure Policy Of Organization, This is For Reference\\n\\nInternship work reference\\n\\nTip: Due to No Data Disclosure Policy Of Organization, This is For Reference\\n\\nConclusion', metadata={'source': '/content/docs/Internship Review .pptx'}),\n",
              " Document(page_content='The six-month internship at Deng InfoTech Solutions Pvt. Ltd. has been a pivotal experience in my professional development. During this period, I gained practical hands-on experience by working on significant projects such as the Intrusion Detection System, Credit Card Fraud Detection, Fire Detection with Alarms, and Fake News Detection using BERT. These projects allowed me to enhance my technical skills in Machine Learning, Deep Learning, and Natural Language Processing, with a particular focus on data', metadata={'source': '/content/docs/Internship Review .pptx'}),\n",
              " Document(page_content='Natural Language Processing, with a particular focus on data preprocessing, model training, and real-time anomaly detection.', metadata={'source': '/content/docs/Internship Review .pptx'}),\n",
              " Document(page_content='One of the highlights of the internship was the successful application of advanced techniques like BERT for accurate fake news detection. Throughout the internship, I benefited greatly from the regular guidance and feedback from both my internal mentor (college professor) and external mentor (company engineer), which significantly contributed to my professional growth. This experience also improved my teamwork, collaboration, and communication skills.', metadata={'source': '/content/docs/Internship Review .pptx'}),\n",
              " Document(page_content='Overall, this internship has strengthened my ability to apply theoretical knowledge to solve real-world problems, allowing me to contribute to impactful projects confidently. The skills and insights gained from this internship have laid a solid foundation for my future endeavors in the field of AI and ML, marking this period as a transformative step in my career\\n\\nThank you !', metadata={'source': '/content/docs/Internship Review .pptx'})]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "irbe3D7R_J_n"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "YoyE7fpl_pDB"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_MO1Un0NDhH",
        "outputId": "ad9594b3-da03-4705-9751-612b64af9d3e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.10/dist-packages (1.0.5)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: google-generativeai<0.6.0,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.5.4)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_google_genai) (0.2.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.41.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.3.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.4 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (0.6.4)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (2.11.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (2.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (4.12.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.4->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (1.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.0->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.66 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.0->langchain_google_genai) (0.1.72)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.0->langchain_google_genai) (8.3.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.0->langchain_google_genai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2.0->langchain_google_genai) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (2.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (1.63.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (0.1.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (1.64.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.2->langchain_google_genai) (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "def load_model(model_name):\n",
        "  if model_name==\"gemini-pro\":\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "  else:\n",
        "    llm=ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
        "\n",
        "  return llm\n",
        "\n",
        "\n",
        "llm=load_model(\"gemini-pro\")"
      ],
      "metadata": {
        "id": "PpfSX0YUM9Zx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjprGjh6NkfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sentence_transformers import SentenceTransformer\n",
        "# embedding = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "mVnI4Sc5_pxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_astradb import AstraDBVectorStore\n",
        "from langchain.indexes import VectorstoreIndexCreator"
      ],
      "metadata": {
        "id": "8TCV0FA2YwxY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ASTRA_DB_API_ENDPOINT=\"https://aa7cfa87-c077-41fd-98b2-c2ccaceff222-us-east-2.apps.astra.datastax.com\"\n",
        "ASTRA_DB_APPLICATION_TOKEN=\"AstraCS:zSbUrMqcmgsTJmbZPoHfYhjU:0a6c3bd57eaa93181111477fe6f08c1fcde65660a2c00ba250057a6b0182aa14\"\n",
        "ASTRA_DB_KEYSPACE=\"default_keyspace\""
      ],
      "metadata": {
        "id": "KLWvXEYS_WGA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vstore = AstraDBVectorStore(\n",
        "    embedding=embedding,\n",
        "    collection_name=\"multidoc_vector\",\n",
        "    api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
        "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
        "    namespace=ASTRA_DB_KEYSPACE,\n",
        ")"
      ],
      "metadata": {
        "id": "qwf9jP-mFsho"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inserted_ids = vstore.add_documents(docs)"
      ],
      "metadata": {
        "id": "PB0OTyiPZYtj"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nInserted {len(inserted_ids)} documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCZF7rhmOEBQ",
        "outputId": "5920329b-404e-4809-d8ab-8dbff4aab341"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inserted 204 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = vstore.similarity_search(\"title of project? \", k=3)\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "id": "ly5kwU3uU7nF",
        "outputId": "093c6ffe-6680-42c6-bdc1-75fdc90872a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* BTECH FINAL YEAR PROJECT SYNOPSIS A Synopsis Submitted in Partial Fulfillment of the Requirements for the Degree of BACHELOR OF TECHNOLOGY in Department of Artificial Intelligence\n",
            "\n",
            "Guided by : Prof. Shubhangi Ingale\n",
            "\n",
            "Project Topic / Title : Movie Review Sentiment Analysis on IMDB Data by Vishal Kshirsagar AIB14 Aditya Varpe AIA07 Shivam Kharat AIA62 Vivek Vaidya  BCOB121 [{'source': '/content/docs/syno.txt'}]\n",
            "* BTECH FINAL YEAR PROJECT SYNOPSIS A Synopsis Submitted in Partial Fulfillment of the Requirements for the Degree of BACHELOR OF TECHNOLOGY in Department of Artificial Intelligence\n",
            "\n",
            "Guided by : Prof. Shubhangi Ingale\n",
            "\n",
            "Project Topic / Title : Movie Review Sentiment Analysis on IMDB Data by Vishal Kshirsagar AIB14 Aditya Varpe AIA07 Shivam Kharat AIA62 Vivek Vaidya  BCOB121 [{'source': '/content/docs/syno.txt'}]\n",
            "* BTECH FINAL YEAR PROJECT SYNOPSIS A Synopsis Submitted in Partial Fulfillment of the Requirements for the Degree of BACHELOR OF TECHNOLOGY in Department of Artificial Intelligence\n",
            "\n",
            "Guided by : Prof. Shubhangi Ingale\n",
            "\n",
            "Project Topic / Title : Movie Review Sentiment Analysis on IMDB Data by Vishal Kshirsagar AIB14 Aditya Varpe AIA07 Shivam Kharat AIA62 Vivek Vaidya  BCOB121 [{'source': '/content/docs/syno.txt'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "You are an AI philosopher drawing insights from the of \"rag,\" \"llama3,\" and \"genai.\"\n",
        "Craft thoughtful answers based on this roadmap, mixing and matching existing paths.\n",
        "Your responses should be concise and strictly related to the provided context.\n",
        "\n",
        "ROADMAP CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "YOUR ANSWER:\"\"\""
      ],
      "metadata": {
        "id": "U8IkQRVzF9pP"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(prompt_template)"
      ],
      "metadata": {
        "id": "DQp4n2tCG-F_"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vstore.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "HLTlpaHDGg6n"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdvsgC2UG2F4",
        "outputId": "6f167988-ab3f-49eb-8650-9220d1c07a0a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['AstraDBVectorStore', 'HuggingFaceBgeEmbeddings'], vectorstore=<langchain_astradb.vectorstores.AstraDBVectorStore object at 0x78a6a501e6b0>, search_kwargs={'k': 3})"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI()"
      ],
      "metadata": {
        "id": "jp8EyMrWGxUx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "cdd176cc-4eb3-48cb-a097-e9d21ab6213e"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-1f2ff8f9afda>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mobject_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__dict__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt_template\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "7HITJ2t3GtNf"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"can you tell title of project ???\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uYnIVzpTcauK",
        "outputId": "bddd16de-e00c-4d07-d8d6-b25f2b7a2e55"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Movie Review Sentiment Analysis on IMDB Data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "jVag171QaHi2",
        "outputId": "b99cd96e-c72d-4d74-9f71-c1801cbd76ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Llama (Large Language Model Meta AI) is a family of autoregressive large language models released by Meta AI. The latest version is Llama 3, which was released in April 2024. Llama models have shown superior performance compared to other large language models, such as GPT-3 and PaLM. These models range in parameter sizes from 7B to 70B and have been made accessible for both academic and commercial use.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M0NfhTCIaRMF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}