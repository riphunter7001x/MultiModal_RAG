{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "53f58ee6f5aa46ec93003d88ebf12855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bae2fa1b147042f699e6c9c30117da0a",
              "IPY_MODEL_560e16e464bf4662aa801ef199ab0956",
              "IPY_MODEL_256a26cda4ce498f99abbb27db65d942"
            ],
            "layout": "IPY_MODEL_c6be50d1f2f74c1a8e46dbb4d890970a"
          }
        },
        "bae2fa1b147042f699e6c9c30117da0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce73868ed3d040329130152c794be9a0",
            "placeholder": "​",
            "style": "IPY_MODEL_7b69a57e83964b9a9b754ff8ecb1e2b9",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "560e16e464bf4662aa801ef199ab0956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52452296e9d64c8cb7e00f272c10bd19",
            "max": 1431,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27a8766088e2497c8b068aa56c99f2e4",
            "value": 1431
          }
        },
        "256a26cda4ce498f99abbb27db65d942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1e739df1bcb4032b4fa6a69c0d548cd",
            "placeholder": "​",
            "style": "IPY_MODEL_b2c36e07731148d6ab86163cf1a528e4",
            "value": " 1.43k/1.43k [00:00&lt;00:00, 75.6kB/s]"
          }
        },
        "c6be50d1f2f74c1a8e46dbb4d890970a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce73868ed3d040329130152c794be9a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b69a57e83964b9a9b754ff8ecb1e2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52452296e9d64c8cb7e00f272c10bd19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27a8766088e2497c8b068aa56c99f2e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1e739df1bcb4032b4fa6a69c0d548cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2c36e07731148d6ab86163cf1a528e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06b6fd7de50e4ef48e781ea83c5f342a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a2ad315da42402d8d792416960d7b27",
              "IPY_MODEL_67d1d4b2ebb348e591a80af6b7c754e5",
              "IPY_MODEL_f814f37f58064a85b5024602681a6525"
            ],
            "layout": "IPY_MODEL_e2371fd70f2d4fd7bfb53d3dea4cbdde"
          }
        },
        "3a2ad315da42402d8d792416960d7b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8dadb060a34bde81b3d878ff2455b8",
            "placeholder": "​",
            "style": "IPY_MODEL_0bd491bd24ef41ac8e664e02bda12b52",
            "value": "tokenizer.model: 100%"
          }
        },
        "67d1d4b2ebb348e591a80af6b7c754e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40cddba4c84b4edd8c7052ac1d114ad9",
            "max": 493443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1b7d5fe367b4716b1c53ecb160a6f36",
            "value": 493443
          }
        },
        "f814f37f58064a85b5024602681a6525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31ca1781f18f4bdc89640c84f11b65c4",
            "placeholder": "​",
            "style": "IPY_MODEL_3253c00e498e4047ad0b071aff672d53",
            "value": " 493k/493k [00:00&lt;00:00, 5.36MB/s]"
          }
        },
        "e2371fd70f2d4fd7bfb53d3dea4cbdde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a8dadb060a34bde81b3d878ff2455b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bd491bd24ef41ac8e664e02bda12b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40cddba4c84b4edd8c7052ac1d114ad9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1b7d5fe367b4716b1c53ecb160a6f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31ca1781f18f4bdc89640c84f11b65c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3253c00e498e4047ad0b071aff672d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9151d237d7f54c34b59ae9c1c8e22633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_328a0c82cb49451fac546fea7782b247",
              "IPY_MODEL_bf1d58fee7054e36825bf09e8f9ff70f",
              "IPY_MODEL_57b223c45816498ca5c4db861c9da7c5"
            ],
            "layout": "IPY_MODEL_957517398bf942ea882db4d44986059f"
          }
        },
        "328a0c82cb49451fac546fea7782b247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b040ae8597c745b68353bee35afee585",
            "placeholder": "​",
            "style": "IPY_MODEL_537a73efdb894e62b76a503503158482",
            "value": "tokenizer.json: 100%"
          }
        },
        "bf1d58fee7054e36825bf09e8f9ff70f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34d1e1989dce42ca9866784358dbde89",
            "max": 1795303,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f498a6d4fd64241aa09a416b05cd033",
            "value": 1795303
          }
        },
        "57b223c45816498ca5c4db861c9da7c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4291a31f02046798f5af0c6927c9289",
            "placeholder": "​",
            "style": "IPY_MODEL_7963986685d04b098f4bcc0458819410",
            "value": " 1.80M/1.80M [00:00&lt;00:00, 7.69MB/s]"
          }
        },
        "957517398bf942ea882db4d44986059f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b040ae8597c745b68353bee35afee585": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "537a73efdb894e62b76a503503158482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34d1e1989dce42ca9866784358dbde89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f498a6d4fd64241aa09a416b05cd033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4291a31f02046798f5af0c6927c9289": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7963986685d04b098f4bcc0458819410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8623d539a8904055a1b21df5d17b7e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_313c7271a53c42b0ae9cdc5daa76306b",
              "IPY_MODEL_140d75a3a3fe42c9834870ad8a5d6a0b",
              "IPY_MODEL_81d9506285cc495496198fb1d6925615"
            ],
            "layout": "IPY_MODEL_03d38ab1eb2b48aa869ad3369a6e58c5"
          }
        },
        "313c7271a53c42b0ae9cdc5daa76306b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bc0839442244dafade6d252e30d177a",
            "placeholder": "​",
            "style": "IPY_MODEL_2c712f4d23784eb5914dfa3320d54eac",
            "value": "added_tokens.json: 100%"
          }
        },
        "140d75a3a3fe42c9834870ad8a5d6a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_014c58315aea463b9cacd2410114ffd1",
            "max": 42,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ddc27e9fcde845a3a1574e7521976cd2",
            "value": 42
          }
        },
        "81d9506285cc495496198fb1d6925615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4169b6db693447548ce2a643e2df1911",
            "placeholder": "​",
            "style": "IPY_MODEL_5bedc381baf94b07ae286fcd3860942d",
            "value": " 42.0/42.0 [00:00&lt;00:00, 3.28kB/s]"
          }
        },
        "03d38ab1eb2b48aa869ad3369a6e58c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bc0839442244dafade6d252e30d177a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c712f4d23784eb5914dfa3320d54eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "014c58315aea463b9cacd2410114ffd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddc27e9fcde845a3a1574e7521976cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4169b6db693447548ce2a643e2df1911": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bedc381baf94b07ae286fcd3860942d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a6d27aa69204ea7a19802d5c31cdc38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95e05f52470441b0855503fe36af4719",
              "IPY_MODEL_71a5c2e50fbc48648f9bbe26344d1a4f",
              "IPY_MODEL_1f4625c6f0dc4f6c96b259a8af9ebcc9"
            ],
            "layout": "IPY_MODEL_5cf756e2286f40868de5cc30a2beb327"
          }
        },
        "95e05f52470441b0855503fe36af4719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b09cbb9a08d54db0855c73fa3937f9f9",
            "placeholder": "​",
            "style": "IPY_MODEL_28df23ea24434d34889c1def7e187c5c",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "71a5c2e50fbc48648f9bbe26344d1a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b389e2ada75f4e8b83eda17cbc81daa0",
            "max": 168,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a9c1cb7f80549fbbfeadbc6cdfe19dd",
            "value": 168
          }
        },
        "1f4625c6f0dc4f6c96b259a8af9ebcc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c2dd498142d4126aeade70437c706f6",
            "placeholder": "​",
            "style": "IPY_MODEL_c53e616b1c254747be2b4ab243226493",
            "value": " 168/168 [00:00&lt;00:00, 11.6kB/s]"
          }
        },
        "5cf756e2286f40868de5cc30a2beb327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b09cbb9a08d54db0855c73fa3937f9f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28df23ea24434d34889c1def7e187c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b389e2ada75f4e8b83eda17cbc81daa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a9c1cb7f80549fbbfeadbc6cdfe19dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c2dd498142d4126aeade70437c706f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c53e616b1c254747be2b4ab243226493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4086270e017141b19e60931898868693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7310b24a7e44522b513146eb3bd3369",
              "IPY_MODEL_01274a83eae84226896a879ed3ab6c35",
              "IPY_MODEL_7bb8be8b3bc6485f85ea1650782e6984"
            ],
            "layout": "IPY_MODEL_a90870d6aa7b467ba3b2cca24301f74f"
          }
        },
        "c7310b24a7e44522b513146eb3bd3369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2db6f1a3b6944b4801995bef0c9c484",
            "placeholder": "​",
            "style": "IPY_MODEL_dbf51f2715a64c38944c109a16f671b2",
            "value": "config.json: 100%"
          }
        },
        "01274a83eae84226896a879ed3ab6c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41b9b29281bc414e8302e837020efeb8",
            "max": 638,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f24b24d6ba2430088a2a2fc30452d0e",
            "value": 638
          }
        },
        "7bb8be8b3bc6485f85ea1650782e6984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0990dfed0d994a0c80dba7713027f381",
            "placeholder": "​",
            "style": "IPY_MODEL_7d9d036f6f1a48d7a7bc9aba2dcf51b0",
            "value": " 638/638 [00:00&lt;00:00, 46.1kB/s]"
          }
        },
        "a90870d6aa7b467ba3b2cca24301f74f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2db6f1a3b6944b4801995bef0c9c484": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbf51f2715a64c38944c109a16f671b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41b9b29281bc414e8302e837020efeb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f24b24d6ba2430088a2a2fc30452d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0990dfed0d994a0c80dba7713027f381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d9d036f6f1a48d7a7bc9aba2dcf51b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3a79b387f99429782921b4339a12852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0c68b87311b40dfa898882670a131dd",
              "IPY_MODEL_f748aa05dcac4425a80af9ed4b0dd666",
              "IPY_MODEL_10e168bd327d48c695bee0d6039ec367"
            ],
            "layout": "IPY_MODEL_59ae26c982a74f73a529012902ccf9bb"
          }
        },
        "e0c68b87311b40dfa898882670a131dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cc6a5703267478da969cec70f37b754",
            "placeholder": "​",
            "style": "IPY_MODEL_df6ff402b30d443d9d0b7eff9eaf2d9d",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "f748aa05dcac4425a80af9ed4b0dd666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5d2a5b00a2244fc86c5979caeba9be3",
            "max": 23950,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b727aeab0d034881bbd6e0f500bede24",
            "value": 23950
          }
        },
        "10e168bd327d48c695bee0d6039ec367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_488ecfaff7394f77af7253e56a5817f7",
            "placeholder": "​",
            "style": "IPY_MODEL_b987888f442541b4966635aed8578144",
            "value": " 23.9k/23.9k [00:00&lt;00:00, 1.20MB/s]"
          }
        },
        "59ae26c982a74f73a529012902ccf9bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cc6a5703267478da969cec70f37b754": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df6ff402b30d443d9d0b7eff9eaf2d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5d2a5b00a2244fc86c5979caeba9be3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b727aeab0d034881bbd6e0f500bede24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "488ecfaff7394f77af7253e56a5817f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b987888f442541b4966635aed8578144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e78c454eaa81416680c21f3698c9aa23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43f1b4091e724785bc586929f7710ea2",
              "IPY_MODEL_ab0ea68a6ef445979b76385adcaa30be",
              "IPY_MODEL_0b48e1fb932241b1bc30e1244b11734a"
            ],
            "layout": "IPY_MODEL_a519c6f07b9046be853e57b14fd574d7"
          }
        },
        "43f1b4091e724785bc586929f7710ea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35460fa101004fbb8d9457279073c396",
            "placeholder": "​",
            "style": "IPY_MODEL_e9183eeac3004322856d5ac486ec9093",
            "value": "Downloading shards: 100%"
          }
        },
        "ab0ea68a6ef445979b76385adcaa30be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1c91db1f1474582b5df5bbdf6ad1818",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f02030122489444c8aa19a308a7c536f",
            "value": 8
          }
        },
        "0b48e1fb932241b1bc30e1244b11734a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ac4098b5e7d4094aec5fe66ac22caec",
            "placeholder": "​",
            "style": "IPY_MODEL_23ecfda5738541ffbee239e832e6abc4",
            "value": " 8/8 [02:07&lt;00:00, 12.90s/it]"
          }
        },
        "a519c6f07b9046be853e57b14fd574d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35460fa101004fbb8d9457279073c396": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9183eeac3004322856d5ac486ec9093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1c91db1f1474582b5df5bbdf6ad1818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f02030122489444c8aa19a308a7c536f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ac4098b5e7d4094aec5fe66ac22caec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23ecfda5738541ffbee239e832e6abc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8cfc95e1e5074f0ca81c131b14b38049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f75202df93924c5098bcfb0a62aa3414",
              "IPY_MODEL_24026656c84f4558aa99c46543fb5479",
              "IPY_MODEL_34c65d535b5749758bafedc87695c4cb"
            ],
            "layout": "IPY_MODEL_73e211d4745d4187a01f876d74dda68b"
          }
        },
        "f75202df93924c5098bcfb0a62aa3414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e3cc4a7cd1742fab8ab84ea626687bc",
            "placeholder": "​",
            "style": "IPY_MODEL_a49bbaac4b804ef499ee02248d029f1d",
            "value": "model-00001-of-00008.safetensors: 100%"
          }
        },
        "24026656c84f4558aa99c46543fb5479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8c52a3270d445f0a52d5943f616167d",
            "max": 1889587040,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9906c688bc984a8dbc914b6bd7968857",
            "value": 1889587040
          }
        },
        "34c65d535b5749758bafedc87695c4cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfcf1514f272401bb57059824774a99e",
            "placeholder": "​",
            "style": "IPY_MODEL_02ef723efe394178939ae5f2f4e4d3a1",
            "value": " 1.89G/1.89G [00:18&lt;00:00, 155MB/s]"
          }
        },
        "73e211d4745d4187a01f876d74dda68b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e3cc4a7cd1742fab8ab84ea626687bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a49bbaac4b804ef499ee02248d029f1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8c52a3270d445f0a52d5943f616167d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9906c688bc984a8dbc914b6bd7968857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bfcf1514f272401bb57059824774a99e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02ef723efe394178939ae5f2f4e4d3a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b1c2cef68f049a28c75a3bb3d43a476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89e94c03ff564d768eb4be2543e97d3c",
              "IPY_MODEL_6f8e4e4cd471406e80aa2cedd27cb117",
              "IPY_MODEL_bad0c8b8a61747f1b10a70d649fe1761"
            ],
            "layout": "IPY_MODEL_7c3625932cd24aa48aab0f92d7d7eaeb"
          }
        },
        "89e94c03ff564d768eb4be2543e97d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_252209b99b1847b680493b1cb74ba11f",
            "placeholder": "​",
            "style": "IPY_MODEL_fac49ea5269d4c14a2f388a9e17272f1",
            "value": "model-00002-of-00008.safetensors: 100%"
          }
        },
        "6f8e4e4cd471406e80aa2cedd27cb117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee9a080971764e689e053c9a2c5e279a",
            "max": 1946243936,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_802b5bd82bcc4d22987a9f06b215ec57",
            "value": 1946243936
          }
        },
        "bad0c8b8a61747f1b10a70d649fe1761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8571f569ee99419e9b4c32fa462b8d78",
            "placeholder": "​",
            "style": "IPY_MODEL_75221587905a44df98b8026dd19547d7",
            "value": " 1.95G/1.95G [00:20&lt;00:00, 153MB/s]"
          }
        },
        "7c3625932cd24aa48aab0f92d7d7eaeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "252209b99b1847b680493b1cb74ba11f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fac49ea5269d4c14a2f388a9e17272f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee9a080971764e689e053c9a2c5e279a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802b5bd82bcc4d22987a9f06b215ec57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8571f569ee99419e9b4c32fa462b8d78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75221587905a44df98b8026dd19547d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb387361e0dd4d29b11cdb3e403eda70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aaa013a0182d4b3ba9f2e344656720c7",
              "IPY_MODEL_7784b423af50412d870236c19bcac1f3",
              "IPY_MODEL_8a7869c7bbbc449cbb836d4c9b96fe57"
            ],
            "layout": "IPY_MODEL_80c4dd87bdfc4f5a8dbcf5c14c252c25"
          }
        },
        "aaa013a0182d4b3ba9f2e344656720c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b298c0fe64a4b6cb4e6681a60f09052",
            "placeholder": "​",
            "style": "IPY_MODEL_fcf5f3cd048646508a423512608b75a5",
            "value": "model-00003-of-00008.safetensors: 100%"
          }
        },
        "7784b423af50412d870236c19bcac1f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4f301665f1a4df3a7504804229249b7",
            "max": 1979781432,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f6945f248584515b6e0ee1434b19644",
            "value": 1979781432
          }
        },
        "8a7869c7bbbc449cbb836d4c9b96fe57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27085c1f2cdb4843b6c8bbb0ad95bbb5",
            "placeholder": "​",
            "style": "IPY_MODEL_24b979e29e894e448b3f3971ec9dee83",
            "value": " 1.98G/1.98G [00:15&lt;00:00, 186MB/s]"
          }
        },
        "80c4dd87bdfc4f5a8dbcf5c14c252c25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b298c0fe64a4b6cb4e6681a60f09052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcf5f3cd048646508a423512608b75a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4f301665f1a4df3a7504804229249b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f6945f248584515b6e0ee1434b19644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27085c1f2cdb4843b6c8bbb0ad95bbb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b979e29e894e448b3f3971ec9dee83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1af22c51e36a480ab66f72e5038c0f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_043e170883b54bc58a0970057d7dcb3d",
              "IPY_MODEL_888b02bf36a54a3aa43919027743f99b",
              "IPY_MODEL_528de1ff988a46fdb12274506c3a856e"
            ],
            "layout": "IPY_MODEL_e3e58f14236d4efda03b7d98f28d6dd8"
          }
        },
        "043e170883b54bc58a0970057d7dcb3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0a01b6de5c045c988049e7effc6d1cf",
            "placeholder": "​",
            "style": "IPY_MODEL_5164816960464e13afcb624c0f0b4f96",
            "value": "model-00004-of-00008.safetensors: 100%"
          }
        },
        "888b02bf36a54a3aa43919027743f99b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b832d5fbdb540489cfded36ef3561bb",
            "max": 1946243984,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9351ca47c35d4d90bea9a6f163e595aa",
            "value": 1946243984
          }
        },
        "528de1ff988a46fdb12274506c3a856e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb7abf2a950e4fedb3223403d95f036d",
            "placeholder": "​",
            "style": "IPY_MODEL_3fbdeaa21d404e8e8e539c0311b238ee",
            "value": " 1.95G/1.95G [00:18&lt;00:00, 182MB/s]"
          }
        },
        "e3e58f14236d4efda03b7d98f28d6dd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0a01b6de5c045c988049e7effc6d1cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5164816960464e13afcb624c0f0b4f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b832d5fbdb540489cfded36ef3561bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9351ca47c35d4d90bea9a6f163e595aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb7abf2a950e4fedb3223403d95f036d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fbdeaa21d404e8e8e539c0311b238ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2040ca1e649c46e39e1ffaf401122c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8ee4c9c14904ccd83d9ed0a2f591c26",
              "IPY_MODEL_3f80d6d2e13f4dc188c1bf89741f1df9",
              "IPY_MODEL_5f59dba06a5247649038458be5417f17"
            ],
            "layout": "IPY_MODEL_2d2aac6225e64247ac04529ad7266a08"
          }
        },
        "e8ee4c9c14904ccd83d9ed0a2f591c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84300e604d394494ac3e752e9ea53bf2",
            "placeholder": "​",
            "style": "IPY_MODEL_ad256f7f6bde4f5d990f646f4be64d69",
            "value": "model-00005-of-00008.safetensors: 100%"
          }
        },
        "3f80d6d2e13f4dc188c1bf89741f1df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11e2a8f257be40aa98c3957415923256",
            "max": 1979781448,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4bb2b09e93d41649ec2cd492d8f9bc1",
            "value": 1979781448
          }
        },
        "5f59dba06a5247649038458be5417f17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_273f36f0f0e94ca3a184514a34e08abc",
            "placeholder": "​",
            "style": "IPY_MODEL_db423b9ab3974e5aba44b7ab3c445459",
            "value": " 1.98G/1.98G [00:20&lt;00:00, 175MB/s]"
          }
        },
        "2d2aac6225e64247ac04529ad7266a08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84300e604d394494ac3e752e9ea53bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad256f7f6bde4f5d990f646f4be64d69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11e2a8f257be40aa98c3957415923256": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4bb2b09e93d41649ec2cd492d8f9bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "273f36f0f0e94ca3a184514a34e08abc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db423b9ab3974e5aba44b7ab3c445459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fc4dbd949984d2d84306dc9750484d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a2e7ab06fb14a76908d470b8a7e2552",
              "IPY_MODEL_a7b64429e5674a5d8b49f344b0e418cd",
              "IPY_MODEL_1d559cddf6ee43dea9bf06ec6befcddb"
            ],
            "layout": "IPY_MODEL_536be78c72c847da8d92fe5e8e36d761"
          }
        },
        "9a2e7ab06fb14a76908d470b8a7e2552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_887deea6f6d9433aa0e57ce1cb4f67a9",
            "placeholder": "​",
            "style": "IPY_MODEL_85bbe8d6cc0c49b7889061e666e28426",
            "value": "model-00006-of-00008.safetensors: 100%"
          }
        },
        "a7b64429e5674a5d8b49f344b0e418cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdbbe2a17569413ba80de4d27f721661",
            "max": 1946243984,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da11b29a2d4948ad85f106cf76afe3fc",
            "value": 1946243984
          }
        },
        "1d559cddf6ee43dea9bf06ec6befcddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7f492356c5a4cb1b4b0079b17f2b37a",
            "placeholder": "​",
            "style": "IPY_MODEL_8fbaecd7f6c7437a906bc13608aba558",
            "value": " 1.95G/1.95G [00:09&lt;00:00, 251MB/s]"
          }
        },
        "536be78c72c847da8d92fe5e8e36d761": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "887deea6f6d9433aa0e57ce1cb4f67a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85bbe8d6cc0c49b7889061e666e28426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdbbe2a17569413ba80de4d27f721661": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da11b29a2d4948ad85f106cf76afe3fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7f492356c5a4cb1b4b0079b17f2b37a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fbaecd7f6c7437a906bc13608aba558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b7f3486cd314f07a39887dc0e36dc9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db04d51105d3491184c630278afac829",
              "IPY_MODEL_6c85812168bb45f781f6b0671090df72",
              "IPY_MODEL_148bae1bf89b41a8ad33f50b5c558f38"
            ],
            "layout": "IPY_MODEL_282ca489318c40ffb07c68d4f5b13848"
          }
        },
        "db04d51105d3491184c630278afac829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82d3107291c244808065fc98541baffe",
            "placeholder": "​",
            "style": "IPY_MODEL_80ec7661e53847d1a0666f41babaccb7",
            "value": "model-00007-of-00008.safetensors: 100%"
          }
        },
        "6c85812168bb45f781f6b0671090df72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97de8d88504a4608836fabfb69529bc5",
            "max": 1979781448,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b622b97a7314de5bc7c46fb5b9b28d9",
            "value": 1979781448
          }
        },
        "148bae1bf89b41a8ad33f50b5c558f38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cd1c3a267474091b460c578146e00c1",
            "placeholder": "​",
            "style": "IPY_MODEL_4bbed38f62e942c08e3be90a034b8ad9",
            "value": " 1.98G/1.98G [00:17&lt;00:00, 177MB/s]"
          }
        },
        "282ca489318c40ffb07c68d4f5b13848": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82d3107291c244808065fc98541baffe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80ec7661e53847d1a0666f41babaccb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97de8d88504a4608836fabfb69529bc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b622b97a7314de5bc7c46fb5b9b28d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2cd1c3a267474091b460c578146e00c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bbed38f62e942c08e3be90a034b8ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb5be858db3740d78029956a3a1a2641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f57d1d2745843cba5bffa8f53bb7365",
              "IPY_MODEL_e9bafe7cfc5c48e7b867dc1d3781ae1b",
              "IPY_MODEL_11416b7e27ac4267a469e4b701322acc"
            ],
            "layout": "IPY_MODEL_67f37409fbc843a2a17414ef2fc13ded"
          }
        },
        "5f57d1d2745843cba5bffa8f53bb7365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a51fc7626fb4e61ac2a96e3dac5e782",
            "placeholder": "​",
            "style": "IPY_MODEL_8495fcba197d4157bc6313b29189db9e",
            "value": "model-00008-of-00008.safetensors: 100%"
          }
        },
        "e9bafe7cfc5c48e7b867dc1d3781ae1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97113dda0d14468b8af3b9646c189bdf",
            "max": 815834680,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09532c82c2a64cb18a750578a1cfd020",
            "value": 815834680
          }
        },
        "11416b7e27ac4267a469e4b701322acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_282f1f6b707d406e884bf44fc4709086",
            "placeholder": "​",
            "style": "IPY_MODEL_144bed5f64a04bd5a0d18a1c412a75ed",
            "value": " 816M/816M [00:05&lt;00:00, 222MB/s]"
          }
        },
        "67f37409fbc843a2a17414ef2fc13ded": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a51fc7626fb4e61ac2a96e3dac5e782": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8495fcba197d4157bc6313b29189db9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97113dda0d14468b8af3b9646c189bdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09532c82c2a64cb18a750578a1cfd020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "282f1f6b707d406e884bf44fc4709086": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "144bed5f64a04bd5a0d18a1c412a75ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36cf162b64e7461dafe1cb89b4b442a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_834fbed52e914fcfbd7390bf51649ffd",
              "IPY_MODEL_db4b29e662114e148428453db1bfcd10",
              "IPY_MODEL_511592cb2c884438a71198dd2007aa63"
            ],
            "layout": "IPY_MODEL_021230120c0d4c4ea0c0ef4757c011a7"
          }
        },
        "834fbed52e914fcfbd7390bf51649ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a9d72b22f0446c5a5629d900dcfc1c9",
            "placeholder": "​",
            "style": "IPY_MODEL_5aaa57ae943a4c82bd0b4e56a79c0c9c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "db4b29e662114e148428453db1bfcd10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_512d0d33dbf3433aade571bdd867ea41",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a42ae6812fa413a8adee99af9bd3a01",
            "value": 8
          }
        },
        "511592cb2c884438a71198dd2007aa63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0d2743219b74133bf923e3bc71295dd",
            "placeholder": "​",
            "style": "IPY_MODEL_6a3c4ae20f744bd7987350074c50d34f",
            "value": " 8/8 [01:02&lt;00:00,  6.75s/it]"
          }
        },
        "021230120c0d4c4ea0c0ef4757c011a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a9d72b22f0446c5a5629d900dcfc1c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aaa57ae943a4c82bd0b4e56a79c0c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "512d0d33dbf3433aade571bdd867ea41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a42ae6812fa413a8adee99af9bd3a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0d2743219b74133bf923e3bc71295dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a3c4ae20f744bd7987350074c50d34f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a409e3f1b834007bc61e96598a699d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e9002eda08242a481caab330aa8612f",
              "IPY_MODEL_37a71550259847bdb8c7ecf035be5385",
              "IPY_MODEL_b3759d4aa8fc4216b550f6e5f028c602"
            ],
            "layout": "IPY_MODEL_7d3f33158a224d608b4e700f8a98e0e3"
          }
        },
        "4e9002eda08242a481caab330aa8612f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0e2e7c0999a4cb5adbb6a8f7a657eab",
            "placeholder": "​",
            "style": "IPY_MODEL_27d82455195e4b049b0324dfabe10dc3",
            "value": "generation_config.json: 100%"
          }
        },
        "37a71550259847bdb8c7ecf035be5385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e47af2168044474a222a99c023fcdb5",
            "max": 111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0c7c2ab11e942789979e13d032ceedc",
            "value": 111
          }
        },
        "b3759d4aa8fc4216b550f6e5f028c602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85ab9d7898d249128dbc87ef689d8ebf",
            "placeholder": "​",
            "style": "IPY_MODEL_ba864e8fa64043ad848a8deef853fc12",
            "value": " 111/111 [00:00&lt;00:00, 6.69kB/s]"
          }
        },
        "7d3f33158a224d608b4e700f8a98e0e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0e2e7c0999a4cb5adbb6a8f7a657eab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27d82455195e4b049b0324dfabe10dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e47af2168044474a222a99c023fcdb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c7c2ab11e942789979e13d032ceedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85ab9d7898d249128dbc87ef689d8ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba864e8fa64043ad848a8deef853fc12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riphunter7001x/RAG/blob/main/Hybrid_Search_and_reranking_in_RAG_open_source.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://s4ds.org/\n",
        "\n",
        "https://www.icdmai.org/\n"
      ],
      "metadata": {
        "id": "ZHlE17nUjXnp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmp_SaX69q18",
        "outputId": "4d52dec0-8d0b-4c1e-d4b5-4547a5c94c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting weaviate-client\n",
            "  Downloading weaviate_client-4.6.4-py3-none-any.whl (325 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/325.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/325.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (2.31.0)\n",
            "Collecting httpx<=0.27.0,>=0.25.0 (from weaviate-client)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting validators==0.28.3 (from weaviate-client)\n",
            "  Downloading validators-0.28.3-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting authlib<2.0.0,>=1.2.1 (from weaviate-client)\n",
            "  Downloading Authlib-1.3.1-py2.py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.8/223.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (2.7.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.64.1)\n",
            "Collecting grpcio-tools<2.0.0,>=1.57.0 (from weaviate-client)\n",
            "  Downloading grpcio_tools-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio-health-checking<2.0.0,>=1.57.0 (from weaviate-client)\n",
            "  Downloading grpcio_health_checking-1.64.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (42.0.7)\n",
            "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-health-checking<2.0.0,>=1.57.0->weaviate-client)\n",
            "  Downloading protobuf-5.27.1-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.2/309.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools<2.0.0,>=1.57.0->weaviate-client) (67.7.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<=0.27.0,>=0.25.0->weaviate-client)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<=0.27.0,>=0.25.0->weaviate-client) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<=0.27.0,>=0.25.0->weaviate-client)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (2.0.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<=0.27.0,>=0.25.0->weaviate-client) (1.2.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (2.22)\n",
            "Installing collected packages: validators, protobuf, h11, httpcore, grpcio-tools, grpcio-health-checking, httpx, authlib, weaviate-client\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.52.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-iam 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-language 2.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "proto-plus 1.23.0 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 5.27.1 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.1 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed authlib-1.3.1 grpcio-health-checking-1.64.1 grpcio-tools-1.64.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 protobuf-5.27.1 validators-0.28.3 weaviate-client-4.6.4\n"
          ]
        }
      ],
      "source": [
        "!pip install weaviate-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQLSw3iJ_0RX",
        "outputId": "ea4ba18d-4db6-4374-c301-beafe1cff2b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.2-py3-none-any.whl (973 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/973.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/973.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.5/973.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/973.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.6/973.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_core-0.2.4-py3-none-any.whl (310 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.4/310.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.74-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.8/124.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Installing collected packages: packaging, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.52.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.1 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.2 langchain-core-0.2.4 langchain-text-splitters-0.2.1 langsmith-0.1.74 orjson-3.10.3 packaging-23.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lpn398P__vR",
        "outputId": "0cd5df6c-b92f-4fcb-b3bf-6b7e3f3fa54d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.3-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.2)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.74)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.6 langchain-community-0.2.3 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate"
      ],
      "metadata": {
        "id": "RSik_tYq-JRN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WEAVIATE_CLUSTER=\"https://hybridsearch-0b5xxxaz.weaviate.network\"\n",
        "WEAVIATE_API_KEY=\"cXmzfmVPouZOYhLHDQG49NYMvpqeujBlVeg8\""
      ],
      "metadata": {
        "id": "M5rKS1Co-22r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WEAVIATE_URL = WEAVIATE_CLUSTER\n",
        "WEAVIATE_API_KEY = WEAVIATE_API_KEY"
      ],
      "metadata": {
        "id": "ovLN44VY-6tU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN=\"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\""
      ],
      "metadata": {
        "id": "Z93YcxMF_iCN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "JUDJ74Ut_N-M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = weaviate.Client(\n",
        "    url=WEAVIATE_URL, auth_client_secret=weaviate.AuthApiKey(WEAVIATE_API_KEY),\n",
        "    additional_headers={\n",
        "         \"X-HuggingFace-Api-Key\": HF_TOKEN\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "YFrBhzvM--rd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.is_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQJQDj68Cy4J",
        "outputId": "bde2a500-1bfe-4177-c2af-d40bb80c00f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.schema.get()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ouOrLG2B9wj",
        "outputId": "b2365a75-39ed-40d7-e822-b139b6572412"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'classes': []}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.schema.delete_all()"
      ],
      "metadata": {
        "id": "9zR5jAGHC3bS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = {\n",
        "    \"classes\": [\n",
        "        {\n",
        "            \"class\": \"RAG\",\n",
        "            \"description\": \"Documents for RAG\",\n",
        "            \"vectorizer\": \"text2vec-huggingface\",\n",
        "            \"moduleConfig\": {\"text2vec-huggingface\": {\"model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"type\": \"text\"}},\n",
        "            \"properties\": [\n",
        "                {\n",
        "                    \"dataType\": [\"text\"],\n",
        "                    \"description\": \"The content of the paragraph\",\n",
        "                    \"moduleConfig\": {\n",
        "                        \"text2vec-huggingface\": {\n",
        "                            \"skip\": False,\n",
        "                            \"vectorizePropertyName\": False,\n",
        "                        }\n",
        "                    },\n",
        "                    \"name\": \"content\",\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "7l8nTgbRDCWt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.schema.create(schema)"
      ],
      "metadata": {
        "id": "XxlykBOsD4oW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.schema.get()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boKhfW7xD8je",
        "outputId": "7ec4fd89-f89d-4a67-99f5-11f4eaa543de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'classes': [{'class': 'RAG',\n",
              "   'description': 'Documents for RAG',\n",
              "   'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2},\n",
              "    'cleanupIntervalSeconds': 60,\n",
              "    'stopwords': {'additions': None, 'preset': 'en', 'removals': None}},\n",
              "   'moduleConfig': {'text2vec-huggingface': {'model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
              "     'type': 'text',\n",
              "     'vectorizeClassName': True}},\n",
              "   'multiTenancyConfig': {'autoTenantActivation': False,\n",
              "    'autoTenantCreation': False,\n",
              "    'enabled': False},\n",
              "   'properties': [{'dataType': ['text'],\n",
              "     'description': 'The content of the paragraph',\n",
              "     'indexFilterable': True,\n",
              "     'indexSearchable': True,\n",
              "     'moduleConfig': {'text2vec-huggingface': {'skip': False,\n",
              "       'vectorizePropertyName': False}},\n",
              "     'name': 'content',\n",
              "     'tokenization': 'word'}],\n",
              "   'replicationConfig': {'factor': 1},\n",
              "   'shardingConfig': {'actualCount': 1,\n",
              "    'actualVirtualCount': 128,\n",
              "    'desiredCount': 1,\n",
              "    'desiredVirtualCount': 128,\n",
              "    'function': 'murmur3',\n",
              "    'key': '_id',\n",
              "    'strategy': 'hash',\n",
              "    'virtualPerPhysical': 128},\n",
              "   'vectorIndexConfig': {'bq': {'enabled': False},\n",
              "    'cleanupIntervalSeconds': 300,\n",
              "    'distance': 'cosine',\n",
              "    'dynamicEfFactor': 8,\n",
              "    'dynamicEfMax': 500,\n",
              "    'dynamicEfMin': 100,\n",
              "    'ef': -1,\n",
              "    'efConstruction': 128,\n",
              "    'flatSearchCutoff': 40000,\n",
              "    'maxConnections': 64,\n",
              "    'pq': {'bitCompression': False,\n",
              "     'centroids': 256,\n",
              "     'enabled': False,\n",
              "     'encoder': {'distribution': 'log-normal', 'type': 'kmeans'},\n",
              "     'segments': 0,\n",
              "     'trainingLimit': 100000},\n",
              "    'skip': False,\n",
              "    'vectorCacheMaxObjects': 1000000000000},\n",
              "   'vectorIndexType': 'hnsw',\n",
              "   'vectorizer': 'text2vec-huggingface'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.weaviate_hybrid_search import WeaviateHybridSearchRetriever"
      ],
      "metadata": {
        "id": "9fYFxszF_lTL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = WeaviateHybridSearchRetriever(\n",
        "    alpha = 0.5,               # defaults to 0.5, which is equal weighting between keyword and semantic search\n",
        "    client = client,           # keyword arguments to pass to the Weaviate client\n",
        "    index_name = \"RAG\",  # The name of the index to use\n",
        "    text_key = \"content\",         # The name of the text key to use\n",
        "    attributes = [], # The attributes to return in the results\n",
        "    create_schema_if_missing=True,\n",
        ")"
      ],
      "metadata": {
        "id": "xDD_FAKZ_sZK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\""
      ],
      "metadata": {
        "id": "RJLYAGHbE1Z5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w6ml1DsEv-q",
        "outputId": "ae2dbf08-a11a-40b0-9b85-e115057ac4ea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->bitsandbytes)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtJsxhOmEzWX",
        "outputId": "2b428155-e019-4e14-9e80-73b7dd917bb7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import ( AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline, )\n",
        "from langchain import HuggingFacePipeline"
      ],
      "metadata": {
        "id": "7YcsnAveEiFy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for loading 4-bit quantized model\n",
        "def load_quantized_model(model_name: str):\n",
        "    \"\"\"\n",
        "    model_name: Name or path of the model to be loaded.\n",
        "    return: Loaded quantized model.\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config,\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "Yflg19-qEiJs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing tokenizer\n",
        "def initialize_tokenizer(model_name: str):\n",
        "    \"\"\"\n",
        "    model_name: Name or path of the model for tokenizer initialization.\n",
        "    return: Initialized tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, return_token_type_ids=False)\n",
        "    tokenizer.bos_token_id = 1  # Set beginning of sentence token id\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "Pfdzn1ukEiMd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = initialize_tokenizer(model_name)"
      ],
      "metadata": {
        "id": "a8UgT93sEiQK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306,
          "referenced_widgets": [
            "53f58ee6f5aa46ec93003d88ebf12855",
            "bae2fa1b147042f699e6c9c30117da0a",
            "560e16e464bf4662aa801ef199ab0956",
            "256a26cda4ce498f99abbb27db65d942",
            "c6be50d1f2f74c1a8e46dbb4d890970a",
            "ce73868ed3d040329130152c794be9a0",
            "7b69a57e83964b9a9b754ff8ecb1e2b9",
            "52452296e9d64c8cb7e00f272c10bd19",
            "27a8766088e2497c8b068aa56c99f2e4",
            "f1e739df1bcb4032b4fa6a69c0d548cd",
            "b2c36e07731148d6ab86163cf1a528e4",
            "06b6fd7de50e4ef48e781ea83c5f342a",
            "3a2ad315da42402d8d792416960d7b27",
            "67d1d4b2ebb348e591a80af6b7c754e5",
            "f814f37f58064a85b5024602681a6525",
            "e2371fd70f2d4fd7bfb53d3dea4cbdde",
            "3a8dadb060a34bde81b3d878ff2455b8",
            "0bd491bd24ef41ac8e664e02bda12b52",
            "40cddba4c84b4edd8c7052ac1d114ad9",
            "c1b7d5fe367b4716b1c53ecb160a6f36",
            "31ca1781f18f4bdc89640c84f11b65c4",
            "3253c00e498e4047ad0b071aff672d53",
            "9151d237d7f54c34b59ae9c1c8e22633",
            "328a0c82cb49451fac546fea7782b247",
            "bf1d58fee7054e36825bf09e8f9ff70f",
            "57b223c45816498ca5c4db861c9da7c5",
            "957517398bf942ea882db4d44986059f",
            "b040ae8597c745b68353bee35afee585",
            "537a73efdb894e62b76a503503158482",
            "34d1e1989dce42ca9866784358dbde89",
            "9f498a6d4fd64241aa09a416b05cd033",
            "e4291a31f02046798f5af0c6927c9289",
            "7963986685d04b098f4bcc0458819410",
            "8623d539a8904055a1b21df5d17b7e02",
            "313c7271a53c42b0ae9cdc5daa76306b",
            "140d75a3a3fe42c9834870ad8a5d6a0b",
            "81d9506285cc495496198fb1d6925615",
            "03d38ab1eb2b48aa869ad3369a6e58c5",
            "5bc0839442244dafade6d252e30d177a",
            "2c712f4d23784eb5914dfa3320d54eac",
            "014c58315aea463b9cacd2410114ffd1",
            "ddc27e9fcde845a3a1574e7521976cd2",
            "4169b6db693447548ce2a643e2df1911",
            "5bedc381baf94b07ae286fcd3860942d",
            "1a6d27aa69204ea7a19802d5c31cdc38",
            "95e05f52470441b0855503fe36af4719",
            "71a5c2e50fbc48648f9bbe26344d1a4f",
            "1f4625c6f0dc4f6c96b259a8af9ebcc9",
            "5cf756e2286f40868de5cc30a2beb327",
            "b09cbb9a08d54db0855c73fa3937f9f9",
            "28df23ea24434d34889c1def7e187c5c",
            "b389e2ada75f4e8b83eda17cbc81daa0",
            "7a9c1cb7f80549fbbfeadbc6cdfe19dd",
            "4c2dd498142d4126aeade70437c706f6",
            "c53e616b1c254747be2b4ab243226493"
          ]
        },
        "outputId": "85032a96-a274-42fb-f427-838e0883083d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53f58ee6f5aa46ec93003d88ebf12855"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06b6fd7de50e4ef48e781ea83c5f342a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9151d237d7f54c34b59ae9c1c8e22633"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8623d539a8904055a1b21df5d17b7e02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a6d27aa69204ea7a19802d5c31cdc38"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_quantized_model(model_name)"
      ],
      "metadata": {
        "id": "Csv9lG6cErbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469,
          "referenced_widgets": [
            "4086270e017141b19e60931898868693",
            "c7310b24a7e44522b513146eb3bd3369",
            "01274a83eae84226896a879ed3ab6c35",
            "7bb8be8b3bc6485f85ea1650782e6984",
            "a90870d6aa7b467ba3b2cca24301f74f",
            "d2db6f1a3b6944b4801995bef0c9c484",
            "dbf51f2715a64c38944c109a16f671b2",
            "41b9b29281bc414e8302e837020efeb8",
            "7f24b24d6ba2430088a2a2fc30452d0e",
            "0990dfed0d994a0c80dba7713027f381",
            "7d9d036f6f1a48d7a7bc9aba2dcf51b0",
            "a3a79b387f99429782921b4339a12852",
            "e0c68b87311b40dfa898882670a131dd",
            "f748aa05dcac4425a80af9ed4b0dd666",
            "10e168bd327d48c695bee0d6039ec367",
            "59ae26c982a74f73a529012902ccf9bb",
            "3cc6a5703267478da969cec70f37b754",
            "df6ff402b30d443d9d0b7eff9eaf2d9d",
            "f5d2a5b00a2244fc86c5979caeba9be3",
            "b727aeab0d034881bbd6e0f500bede24",
            "488ecfaff7394f77af7253e56a5817f7",
            "b987888f442541b4966635aed8578144",
            "e78c454eaa81416680c21f3698c9aa23",
            "43f1b4091e724785bc586929f7710ea2",
            "ab0ea68a6ef445979b76385adcaa30be",
            "0b48e1fb932241b1bc30e1244b11734a",
            "a519c6f07b9046be853e57b14fd574d7",
            "35460fa101004fbb8d9457279073c396",
            "e9183eeac3004322856d5ac486ec9093",
            "e1c91db1f1474582b5df5bbdf6ad1818",
            "f02030122489444c8aa19a308a7c536f",
            "7ac4098b5e7d4094aec5fe66ac22caec",
            "23ecfda5738541ffbee239e832e6abc4",
            "8cfc95e1e5074f0ca81c131b14b38049",
            "f75202df93924c5098bcfb0a62aa3414",
            "24026656c84f4558aa99c46543fb5479",
            "34c65d535b5749758bafedc87695c4cb",
            "73e211d4745d4187a01f876d74dda68b",
            "9e3cc4a7cd1742fab8ab84ea626687bc",
            "a49bbaac4b804ef499ee02248d029f1d",
            "d8c52a3270d445f0a52d5943f616167d",
            "9906c688bc984a8dbc914b6bd7968857",
            "bfcf1514f272401bb57059824774a99e",
            "02ef723efe394178939ae5f2f4e4d3a1",
            "2b1c2cef68f049a28c75a3bb3d43a476",
            "89e94c03ff564d768eb4be2543e97d3c",
            "6f8e4e4cd471406e80aa2cedd27cb117",
            "bad0c8b8a61747f1b10a70d649fe1761",
            "7c3625932cd24aa48aab0f92d7d7eaeb",
            "252209b99b1847b680493b1cb74ba11f",
            "fac49ea5269d4c14a2f388a9e17272f1",
            "ee9a080971764e689e053c9a2c5e279a",
            "802b5bd82bcc4d22987a9f06b215ec57",
            "8571f569ee99419e9b4c32fa462b8d78",
            "75221587905a44df98b8026dd19547d7",
            "bb387361e0dd4d29b11cdb3e403eda70",
            "aaa013a0182d4b3ba9f2e344656720c7",
            "7784b423af50412d870236c19bcac1f3",
            "8a7869c7bbbc449cbb836d4c9b96fe57",
            "80c4dd87bdfc4f5a8dbcf5c14c252c25",
            "2b298c0fe64a4b6cb4e6681a60f09052",
            "fcf5f3cd048646508a423512608b75a5",
            "c4f301665f1a4df3a7504804229249b7",
            "0f6945f248584515b6e0ee1434b19644",
            "27085c1f2cdb4843b6c8bbb0ad95bbb5",
            "24b979e29e894e448b3f3971ec9dee83",
            "1af22c51e36a480ab66f72e5038c0f34",
            "043e170883b54bc58a0970057d7dcb3d",
            "888b02bf36a54a3aa43919027743f99b",
            "528de1ff988a46fdb12274506c3a856e",
            "e3e58f14236d4efda03b7d98f28d6dd8",
            "a0a01b6de5c045c988049e7effc6d1cf",
            "5164816960464e13afcb624c0f0b4f96",
            "8b832d5fbdb540489cfded36ef3561bb",
            "9351ca47c35d4d90bea9a6f163e595aa",
            "bb7abf2a950e4fedb3223403d95f036d",
            "3fbdeaa21d404e8e8e539c0311b238ee",
            "2040ca1e649c46e39e1ffaf401122c65",
            "e8ee4c9c14904ccd83d9ed0a2f591c26",
            "3f80d6d2e13f4dc188c1bf89741f1df9",
            "5f59dba06a5247649038458be5417f17",
            "2d2aac6225e64247ac04529ad7266a08",
            "84300e604d394494ac3e752e9ea53bf2",
            "ad256f7f6bde4f5d990f646f4be64d69",
            "11e2a8f257be40aa98c3957415923256",
            "a4bb2b09e93d41649ec2cd492d8f9bc1",
            "273f36f0f0e94ca3a184514a34e08abc",
            "db423b9ab3974e5aba44b7ab3c445459",
            "4fc4dbd949984d2d84306dc9750484d4",
            "9a2e7ab06fb14a76908d470b8a7e2552",
            "a7b64429e5674a5d8b49f344b0e418cd",
            "1d559cddf6ee43dea9bf06ec6befcddb",
            "536be78c72c847da8d92fe5e8e36d761",
            "887deea6f6d9433aa0e57ce1cb4f67a9",
            "85bbe8d6cc0c49b7889061e666e28426",
            "fdbbe2a17569413ba80de4d27f721661",
            "da11b29a2d4948ad85f106cf76afe3fc",
            "a7f492356c5a4cb1b4b0079b17f2b37a",
            "8fbaecd7f6c7437a906bc13608aba558",
            "7b7f3486cd314f07a39887dc0e36dc9d",
            "db04d51105d3491184c630278afac829",
            "6c85812168bb45f781f6b0671090df72",
            "148bae1bf89b41a8ad33f50b5c558f38",
            "282ca489318c40ffb07c68d4f5b13848",
            "82d3107291c244808065fc98541baffe",
            "80ec7661e53847d1a0666f41babaccb7",
            "97de8d88504a4608836fabfb69529bc5",
            "9b622b97a7314de5bc7c46fb5b9b28d9",
            "2cd1c3a267474091b460c578146e00c1",
            "4bbed38f62e942c08e3be90a034b8ad9",
            "cb5be858db3740d78029956a3a1a2641",
            "5f57d1d2745843cba5bffa8f53bb7365",
            "e9bafe7cfc5c48e7b867dc1d3781ae1b",
            "11416b7e27ac4267a469e4b701322acc",
            "67f37409fbc843a2a17414ef2fc13ded",
            "6a51fc7626fb4e61ac2a96e3dac5e782",
            "8495fcba197d4157bc6313b29189db9e",
            "97113dda0d14468b8af3b9646c189bdf",
            "09532c82c2a64cb18a750578a1cfd020",
            "282f1f6b707d406e884bf44fc4709086",
            "144bed5f64a04bd5a0d18a1c412a75ed",
            "36cf162b64e7461dafe1cb89b4b442a6",
            "834fbed52e914fcfbd7390bf51649ffd",
            "db4b29e662114e148428453db1bfcd10",
            "511592cb2c884438a71198dd2007aa63",
            "021230120c0d4c4ea0c0ef4757c011a7",
            "2a9d72b22f0446c5a5629d900dcfc1c9",
            "5aaa57ae943a4c82bd0b4e56a79c0c9c",
            "512d0d33dbf3433aade571bdd867ea41",
            "3a42ae6812fa413a8adee99af9bd3a01",
            "f0d2743219b74133bf923e3bc71295dd",
            "6a3c4ae20f744bd7987350074c50d34f",
            "2a409e3f1b834007bc61e96598a699d4",
            "4e9002eda08242a481caab330aa8612f",
            "37a71550259847bdb8c7ecf035be5385",
            "b3759d4aa8fc4216b550f6e5f028c602",
            "7d3f33158a224d608b4e700f8a98e0e3",
            "a0e2e7c0999a4cb5adbb6a8f7a657eab",
            "27d82455195e4b049b0324dfabe10dc3",
            "6e47af2168044474a222a99c023fcdb5",
            "c0c7c2ab11e942789979e13d032ceedc",
            "85ab9d7898d249128dbc87ef689d8ebf",
            "ba864e8fa64043ad848a8deef853fc12"
          ]
        },
        "outputId": "458a7600-269e-4c53-940f-462ac49b6a6b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['low_cpu_mem_usage']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4086270e017141b19e60931898868693"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3a79b387f99429782921b4339a12852"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e78c454eaa81416680c21f3698c9aa23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cfc95e1e5074f0ca81c131b14b38049"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b1c2cef68f049a28c75a3bb3d43a476"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb387361e0dd4d29b11cdb3e403eda70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1af22c51e36a480ab66f72e5038c0f34"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2040ca1e649c46e39e1ffaf401122c65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fc4dbd949984d2d84306dc9750484d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b7f3486cd314f07a39887dc0e36dc9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb5be858db3740d78029956a3a1a2641"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36cf162b64e7461dafe1cb89b4b442a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a409e3f1b834007bc61e96598a699d4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    use_cache=True,\n",
        "    device_map=\"auto\",\n",
        "    #max_length=2048,\n",
        "    do_sample=True,\n",
        "    top_k=5,\n",
        "    max_new_tokens=100,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "IplrZgxvEreX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=pipeline)"
      ],
      "metadata": {
        "id": "Uo348jKvErhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e11c75-9ab3-45ce-e5c5-7a60134034d0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_path=\"/content/9739-Article Text-17322-1-10-20210708.pdf\""
      ],
      "metadata": {
        "id": "uva-5Nkqpr8w"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTNRvdSNp9jC",
        "outputId": "5561e54d-c113-40ea-9d7a-aae39c16d538"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/290.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m256.0/290.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.1)\n",
            "Installing collected packages: pypdf\n",
            "Successfully installed pypdf-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev8_SeQIp_4A",
        "outputId": "e178dee0-8bf8-4d6e-b85c-130f67a168db"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.6)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.2)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.74)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "3n-7-QWyp_8x"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(doc_path)"
      ],
      "metadata": {
        "id": "nhBRpl8dsHw6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "xHegGUGssHzV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpshkBhjvLlC",
        "outputId": "8d861efa-fc97-4c84-eb55-6647cd373026"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Turkish Journal of Computer and Mathematics Education  \\n \\n \\n______________________________________________________________________________ ____  \\n5522    \\n  \\nResearch Article    Vol.12 No. 6 (2021), 5522 -5529 \\nA Deep Convolutional Neural Network for Traffic Sign Classification and \\nDetection with Voice Recognition  \\n \\nK.Prakash  a. \\na M-Tech student, VNR Vignana Jyothi Institute of Engineering and Technology (Autonomous), \\nNizampet , Affiliation to JNTUH.  \\n \\nArticle History: Received: 10 November 2020; Revised 12 January 2021 Accepted: 27 January 2021; \\nPublished online: 5 April 2021  \\n____________________________________________________________________________________________________  \\n \\nAbstract: Traffic  Sign De tection and Classification play  an important major role now adays in our daily life. \\nAnyway , Traffic Signs are present on roads. Even though, often the drivers do mistakes. It’s very hard to \\nrecognize and detect traffic signs while travelling on roads. Drivers may misinterpret traffic si gns, this leads to \\nAccidents and results in damage to the vehicle. To overcome this problem this project introduces a concept \\nnamed Traffic Sign Detection and Classification with Voice Recognition. This model is built by using CNN to \\nextract the images and  classify the traffic signs. Here DCCNN model is built to improve overall accuracy and \\nspeed. Here Classification process is also tested with AlexNet, VGG16, VGG19. This system reveals output that \\nrecognizes the traffic signs automatically that helps to de tect the street condition and alerts driver soon and this \\nenables to build a smart vehicle.  \\n \\nKeywords: DCCNN, VGG16, Deep Learning , Traffic Sign Detection , Automation.  \\n___________________________________________________________________________  \\n \\n1. Introduct ion  \\nNow a day’s various modes of Transport facilities are very important in our daily life [1].  At the same time \\nsafety also plays a very important role. For Safety Purpose in Roads already traffic signs are placed on roads to \\novercome accidents and to follow the traffic rules [2]. Traffic Signs guide us to be in our safety and they help \\nus to travel in our vehicle in the desired path. Even though the traffic signs are present on roads, the drivers \\noften do mistakes [3]. It’s very hard to detect and recognize the traffic signs while driving the vehicle. With the \\nrise in a huge number of road accidents happening every year there has been need ing to develop a safety \\nsystem that helps in contributing to the pedestrian vehicle and also driver’s safety [4].  \\n Traffic signs play a very important role and crucial role while driving vehicles. A traffic sign is also a part \\nof road infrastructure [ 5]. Here is an automated system that help s us to notify and identify and can help a lot. \\nThis aims to recognize and detect the traffic sign much faster and enables to build of an autonomous vehicle \\n[6].  \\n2. Background Theory  \\n \\nThe traffic signs  are placed on the road with different sizes and shape some traffic sign s are very blur and \\nsmall in size, which is covered by snow and heavy rainfall. Even though it‘s in any condition,  Our aim to \\ndetect traffic sign s [7]. The traffic sign classification is a process tha t receives the input of the image to detect \\nthe traffic sign ie.., U -Turn Ahead, Speed Limit 40, Caution Signs , etc… These signs recommend drivers to get \\nby giving input commands through progra mming the data in such to classify the traffic sign automatical ly in \\nvehicle s [8]. Traffic Sign Detection is a process that detects the traffic sign and performs the feature extraction \\nprocess, clustering , and filtering process and then the labeling process and testing process has to be taken place \\nthen detected resul t is to be displayed on the dashboard [9].  \\nThe dataset consists of various formats. With respect to size, shape structure. Images preprocessing enables \\nthe improvement of images and reduces the distortion effect [10]. Here I also used the Gaussian effect to \\neliminate the image noise [11].  The dataset processing is to be done to ensure the data is arranged suitably  to \\ndetect the traffic signs much faster.  \\n3. Challenges  of  the Study  \\n• Can build  a smart autonomous smart vehicle further to improve  the safety mechanism for drivers, \\nPedestrians , and Vehicles.  \\n• To Implement Navigating Mechanism for drivers safety.  ', metadata={'source': '/content/9739-Article Text-17322-1-10-20210708.pdf', 'page': 0}),\n",
              " Document(page_content='A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \\n____________________________________________________________________________________________________________  \\n______________________________________________________________________________ ____  \\n5523  4. Objectives Of t he Study  \\n• To implement Traffic sign Classification in order to detect the Traffic sign much faster.  \\n• To find Traffic Sign Detection to detect the traffic sign when traffic signal approaches near to us and \\nalerts us and notifies us.  \\n• To find out Traffic sign Recognition in order to detect traffic signal s from Real -time Webcam s. \\n \\n5.  Methodology   \\nVarious Methods are tested to detec t which methods will perform the task much faster with more accuracy \\nand to test which method is more efficient.  \\n5.1. CNN (Convolutional Neural Network)  \\nTraffic Signs are captured and detected and then pre -processed by CNN. After the Detection of traffic sign s \\nthen Feature Extraction is taken place. In Feature Extraction , the traffic sign is classified and segmented into \\nmultiple pixels [12]. This feature extraction results in the extraction of multiple features of the image. Then the \\ntraining process is carried on by input image pixels which can differentiate from each other [13]. Clustering is \\na process that segregates different objects in such a way simi lar objects are placed into similar groups. The \\nClassification of traffic signs is filtered based upon the size and shape of the image [14]. Filtration is a process \\nto detect the edge of image blur image and a shaded image is detected with the kernel slidi ng in the extracted \\npixel of the image [15]. Normally a Convolution Neural Network recognizes this field and learns everything by \\nthe basic shapes by evolving the many features in the training process. The CNN learns everything in such a \\nway that this can distinguish from one sign to the other sign. This Max -pooling is a technique that decreases \\nthe density and to classify the respective sign. In the detection of traffic Sign Video , the Gaussian technique is \\nintroduced to reduce the noise.  So Here the Rotat ing kernel filter is calculated as follows  \\n𝑔(𝑎,𝑏)=∑ ∑  𝜌(𝑑𝑥+𝑑𝑦)𝑓(𝑎+𝑑𝑥,𝑏+𝑑𝑦)𝑦\\n𝑎𝑎=−𝑦𝑥\\n𝑎𝑏=−𝑥   ….. (1)  \\nWhere −𝑥<𝑑𝑥<𝑥 and 𝑦<𝑑𝑦<𝑦 \\nHere 𝑔 (𝑎,𝑏) is a filtering image  𝜌 is filter Kernel. By this Equation kernel level of the filter is calculated \\nas above.  \\nUnder CNN this model also tested with AlexNet, VGG16, VGG19  \\nVGG16:  \\nHere the input is fed to filters that use convolutional layers where filters are with tiny receptive fields of 3 X \\n3 size. This uses mostly small size layers. The Very first layer is big. The n ext layers are most ly very small. It \\nsupports only  a 1X1 filter . So, it takes more time to compute time more.  \\nVGG19:  \\nThis VGG19 contains 16 layers of kernel 3X3 size of each pixel to cover an entire whole image. It is a \\nGood Classi fication compared to VGG16. This contains activation function RELU that produce s less \\ncomputation. It is implemented with the modification of method VGG16 . \\nAlexNet:  \\nAlexNet  can deal with big datasets and achieves high accuracy, The AlexNet  is implemented to reduce \\noverfitting problem s. To reduce the overfitting model This is implemented with Technique Data Augmentation \\nand Dropout.  \\n5.2. Dual Channel Convolutional Neural Ne twork (DCCNN)  \\nIn the DCCNN model additionally , one individual channel is present. This includes additional processing of \\nthe image. First , the drawbacks present in CNN are as follows. The entire images which are present on roads \\nare not unique. Some will blur, shadow, snow, shaded half of the part of the image will be lost due to some \\ncircumstances and some images are very small on roads. Th is results in very low frequency and unbalanced \\ndata. This results in very low accuracy and very poor data insuffi ciency  even. To overcome these issues I \\nintroduce DCCNN here [16]. Here in these two channels of input  is taken and output is generated by one  \\nentity.  Two channels are needed to be get trained with datasets. Here the data which is inconsisten t and \\ninadequate gets fed with two inputs of data in different channels. This results in an increas ed detection speed of \\ndata. Finally , the output can be t raced with the help of the final entity.  ', metadata={'source': '/content/9739-Article Text-17322-1-10-20210708.pdf', 'page': 1}),\n",
              " Document(page_content='Turkish Journal of Computer and Mathematics Education  \\n \\n \\n______________________________________________________________________________ ____  \\n5524    \\n  \\nResearch Article    Vol.12 No. 6 (2021), 5522 -5529 \\nThe m ain Contributions of DCCNN is as follows : \\n1. This involves two channels that solve issues of complicated tracing of the image, Poor resolution of \\nimages , etc. \\n2. This involves two Convolutional Neural Networks; quant ities are adjusted automatically between the \\ntwo different channels until the desired is satisfactory. By this model , inadequate training problem can \\neasily overcome and it can trace very fast compared to CNN.  \\n3. Here the decision is taken by using two channe ls, the outputs of two channels are fed to the fusion \\npoint, and then the decision can be taken at the classification stage.   \\n \\nHere DCCN N Leaning Mechanism  is mentioned as follows as shown in Fig 1.  \\n \\nFigure.1: Proposed Learning DCCNN Algorithm  \\nThe DCCNN is implemented as follows; it consists of two channels of two networks. Each channel is \\nimplemented by its features. Here every channel is implemented by its own 4 fully connected layers.  \\nHere Layers in  DCCNN is get trained and implemented as follow s: \\n• Input Layer: 30 x30 input Number of filters are 48  \\n• Hidden Layers  \\n─ Channel 1:Fork11,  \\n─ Channel 2:Fork12, Both Channels performs convolutional on input.  \\n─ Merge: This layer combines the output of channel1 and channel2.  \\n─ Flatten: It converts its input to a one-dimensional array  \\n• Output Layer: Dense Layer uses SoftMax  activation function, the shape of output is equal to several  classes \\n43. \\nHere Dropout is implemented in 50% in the model with fully connected layers of output nodes is 20. By \\nconsidering Activation SoftMax function is present. This enables the convergence function to maintain stability \\nin all activations.   \\nThis dataset consist s of low unbalanced data, normalized samples . The unbalanced trained data gets processe d \\nseparately with the help of dual channels. Here I train every image of every class will be in a cycle until the \\nimage will match its suitable respective labels. Here the images are very huge in number. Every Image is \\nsegregated to its respective class as sociated with it. In the Classification process , the dataset consists of various \\nclasses of images it consists of 43 classes and each class has up to 400 images overall 8600 images are present \\nin dataset. The dataset of Traffic signs is shown here is as fo llows in Fig 2.  \\n', metadata={'source': '/content/9739-Article Text-17322-1-10-20210708.pdf', 'page': 2}),\n",
              " Document(page_content='A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \\n____________________________________________________________________________________________________________  \\n______________________________________________________________________________ ____  \\n5525   \\nFigure.1: Dataset  of traffic signs  \\n \\n6. Traffic Sign Classification  \\nTraffic Sign Classification is a process that takes place to detect the traffic sign with the help of an image only.  \\nThe flow diagram of Traffic sign Classification is shown as follows in Fig 3.  \\n \\nFigure. 2: Flow  Diagram of Traffic Sign Classification  \\nThe above diagra m indicates the proposed model of the Traffic Sign Classification System.  \\nThe Procedure of this work is implemented as follows:  \\n• Traffic signs will be in different size s and shapes. Resizing has to take place to achieve fast detection of \\ntraffic sign s.  \\n• In this process , the image is captured through the network. Identification of image s is taken place with the \\nhelp of Object Detection API.  \\n•  The pre -process ing includes tracing the path of the traffic sign and assigning the labels for it. The traffic Sign \\nclasses are arranged in the form of an array with the help of its respective data associated with i t to retrieve \\nthe data easily. This paper also implement s a loop to trace the data until the data associated with the \\nrespective image is traced.  \\n• Now bu ilding the model has been t aken place to train by using an Algorithm . Then training and testing \\nprocess is carried for the training model.  \\n• After the training and testing process is carried on then we can check its accuracy and confusion matrix then \\ntest w ith the desired image so the resultant traffic sign information is obtained.  \\n', metadata={'source': '/content/9739-Article Text-17322-1-10-20210708.pdf', 'page': 3}),\n",
              " Document(page_content='Turkish Journal of Computer and Mathematics Education  \\n \\n \\n______________________________________________________________________________ ____  \\n5526    \\n  \\nResearch Article    Vol.12 No. 6 (2021), 5522 -5529 \\n7. Traffic Sign Detection  \\nTraffic Sign detection is the detection of traffic signs that takes place within the autonomous smart vehicle \\nitself. The detection of traffic sign is taken  place as follows as shown in Fig 4.  \\n \\nFigure. 3: Flow  Diagram of Traffic Sign detection  \\nThe s mart vehicle can be equipped  with the software.  The above diagram indicates the proposed work of the \\nTraffic sign detection system . \\nThe procedure of this work is implemented as follows:  \\n• First, we take a sample video  as an input here,  then the first detection of traffic sign in a video is taken place \\nwith the help of  object detection Even though it’s in any shape  and size no matter it detects and this fed to \\nAgain traffic sign classification Process  \\n• The detect traffic sign is fed to the Feature Engineering Process that divides the whole image into a huge \\nnumber of pixels and then extracts the Features and then clustering and F iltration operation process is also \\nperformed.  \\n• By the above process , the Image is then extracted and detected and then the image is compared with the \\ntesting process which is performed in the traffic sign Classification.   \\n• The label map is done to check wi th the testing feature to verify the predicted result is correct and after \\nverification , this display s the result.  \\n• The Traffic Sign Detection Process is performed with the help of a dataset that gets trained and tested by the \\nCNN in the Traffic sign Classi fication Process.     \\n        \\n8. Experimentation Results:  \\nThe Experiment is done with help of a flask that make s use of even HTML CSS JS along with python etc.  \\nThe Traffic Sign Classification is proved as best in automatically recognizing the traffic signs a nd helps in very \\nfast detection of traffic signs. The interface of his project of traffic sign classification is shown as follows in \\nFig 5.   \\n                                                      \\nFigure. 4: Result  of Traffic Sign Classification  (on left) Input Image  (on right ) Predicted Traffic sign Image.  \\n', metadata={'source': '/content/9739-Article Text-17322-1-10-20210708.pdf', 'page': 4}),\n",
              " Document(page_content='A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \\n____________________________________________________________________________________________________________  \\n______________________________________________________________________________ ____  \\n5527  The above diagrams indicate the output of Traffic Sign Classification. First Fig  5(on left ) indicates the input \\nof the image and another one when I upload traffic this shows the desired output of an image. Traffic Sign \\nClassification is very necessary to identify traffic signs much faster even though the traffic sign is in any size or \\nshape. Even though the traffic sign is a blur and faded then also with this we can detect it very easily  as shown \\nin Fig 5 (on right ). Traffic sign detection is very necessary to take place detection automatically when we are in \\ntravelling in a vehicle. This help s to detect the traffic sign al before the tr affic sign is arrived .  This alert s the \\nwhen the person is in sleepy mode. This also guide s the way of road automatically. The following fig of traffic \\nsign detection is shown in Fig  6 as follows.  Fig 6 (on left ) indicates here the input video frame without detection \\nof any traffic sign signal. In Figure 6 (on right ) The detection of Traffic sign detection has been taken place . \\n    \\nFigure. 5: Result of Traffic Sign Detection  (on left ) Input video (on right ) Traffic Sig n detection  \\n \\nHere the output of Traffic sign Classification and Detection is fed to the input of GTTS library in python is \\nused to implement the same result with speech. The Traffic Sign is Recognition is performed from a Real-time \\nWebcam camera with voice  software i.e., My Groovy music. This involves the same training method as Traffic \\nsign detection but the procedure is followed with OpenCV python. The Traffic Sign Recognition is performed \\nas follows as shown in Fig 7.  \\n \\nFigure. 6: Traffic  Sign Recognition  \\n9. Comparative  Results   \\n     Here the Model is implemented with VGG16, VGG19, AlexNet. The performance of various algorithms is \\nimplemented as follows:  \\nThe VGG16 is implemented for the traffic sign c lassification of images. VGG16 holds 82.77% Accuracy, \\nbut this is used to handle a huge amount of datasets. Here the VGG 19 is implemented the Results are \\nsomewhat  better  compared to VGG16 . VGG19 holds  an accuracy of 88.39%. Here also input images of the \\ndataset are fed to VGG19 . To improve this model I use DCCNN (Dual Channel Convolutional Neural \\nNetwork). Here the DCCN N achieves high accuracy compared to the Normal CNN method. The DCCNN \\n', metadata={'source': '/content/9739-Article Text-17322-1-10-20210708.pdf', 'page': 5}),\n",
              " Document(page_content='Turkish Journal of Computer and Mathematics Education  \\n \\n \\n______________________________________________________________________________ ____  \\n5528    \\n  \\nResearch Article    Vol.12 No. 6 (2021), 5522 -5529 \\ncontai ns two CNN Channels this so h ere with the help of DCCNN anyway gains high accuracy compared to \\nCNN.  \\nTable 1. Overall  Performance of Algorithms  \\nS.No  Methods  Accuracy  Performance  \\n1 AlexNet  66.53%  Poor  \\n2 VGG16  82.77%  Good  \\n3 VGG19  88.39%  Better  \\n4 DCCNN  94.15%  Very Fast  \\n \\nThe accuracy gained by the DCCNN algorithm for this proposed model is 94.15 %. So t he accuracy  is \\ngained by DCCNN is finally 6 % higher than the CNN. During the t raining process , the images which are \\npresent with unbalanced data are fed to the DCCNN in order add setup a high training frequency of images and \\nto improve trainin g speed and accuracy. So Here Confusion Matrix for the final DCCNN Algorithm is as \\nshown in Fig 8.  \\nFigure.8:  Confusion Matrix of DCCNN  \\n \\n10. Conclusion  \\nThe Proposed system is an effective met hod for performing the traffic sign Classification and detection system. \\nEven though the traffic sign is any condition, whether it is  to blur small , or it is small size or improper display, \\nThis mechanism helps to detect traffic signs much faster and alerts us.  Among all the above algorithms, the \\nDCCNN is only the best technique that can detect the traffic Sign much faster with better accuracy. This project \\nis well suits for quotation “one -stop solution to all traffic issues”. Traffic sign detection and classification are \\nvery important to have the safety to drivers pedestrians and vehicles. These results can guide the way present \\non-road and provides information related to roads and improves saf ety mechanism s. With help of this \\nMechanism in the future , we can build a smart autonomous vehicle.  \\n \\n', metadata={'source': '/content/9739-Article Text-17322-1-10-20210708.pdf', 'page': 6}),\n",
              " Document(page_content='A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \\n____________________________________________________________________________________________________________  \\n______________________________________________________________________________ ____  \\n5529  11. Acknowledgment  \\nThe author  gratefully acknowledges the support provided by the department of inf ormation technology, \\nVNRVJIET, Hyderabad, India . \\n  \\nReferences   \\n1. Ryley, Tim J., et al. \"Investigating the contribution of Demand Responsive Transport to a sustainable local public \\ntransport system.\"  Research in Transportation Economics  48 (2014): 364 -372. \\n2. Greenblatt, Jeffery B., and Susan Shaheen. \"Automate d vehicles, on -demand mobility, and environmental \\nimpacts.\"  Current sustainable/renewable energy reports  2.3 (2015): 74 -81. \\n Austin, Kevin. \"The identification of mistakes in road accident records: Part 1, locational variables.\"  Accident Analysis \\n& Prevention  27.2 (2015): 261 -276.https://timesofindia.indiatimes.com/city/mumbai/sleepy -driver -crashes -suv-on-\\nmumbai -pune -expressway -7-passengers -killed/articleshow/18801253.cms .  \\n3. Sonkin, Beth, et al. \"Walking, cycling and transport safety: an analysis of child road deaths.\"  Journal of the Royal \\nSociety of Medicine  99.8 (2016): 402 -405. http://archive.indianexpress.com/news/accidents -on-expressway -prompt -\\nfresh -survey -of-state-highways/1083181/  \\n4. Khadaroo, Jameel, and Boopen Seetan ah. \"Transport infrastructure and tourism development.\"  Annals of tourism \\nresearch  34.4 (2017): 1021 -1032.  https:/ /timesofindia.indiatimes.com/india/traffic -junctions -account -for-half-of-road-\\ndeaths -in-india/articleshow/40956759.cms  \\n5. Haboucha, Chana J., Robert Ishaq, and Yoram Shiftan. \"User preferences regarding autonomous \\nvehicles.\"  Transportation Research Part C: Em erging Technologies  78 (2017): 37 -49. \\n6. De La Escalera, Arturo, et al. \"Road traffic sign detection and classification.\"  IEEE transactions on industrial \\nelectronics  44.6 (2015 ): 848 -859. \\n7. Zhu, Zhe, et al. \"Traffic -sign detection and classification in the wild.\"  Proceedings of the IEEE conference on computer \\nvision and pattern recognition . 2016..  \\n8. Gavrila, Dariu M. \"Traffic sign recognition revisited.\"  Mustererkennung 1999 . Springer, Berlin, Heidelberg, 2018 . 86-\\n93. \\n9. Houben, Sebastian, e t al. \"Detection of traffic signs in real -world images: The German Traffic Sign Detection \\nBenchmark.\"  The 2013 international joint conference on neural networks (IJCNN) . Ieee, 2013.  \\n10. Guo, J., Zhang, H., Zhen, D., Shi, Z., Gu, F., & Ball, A. D. (2020). An en hanced modulation signal bispectrum analysis \\nfor bearing fault detection based on non -Gaussian noise suppression.  Measurement , 151, 107240.  \\n11. Jogin, Manjunath, et al. \"Feature extraction using convolution neural networks (CNN) and deep learning.\"  2018 3rd \\nIEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT). \\nIEEE, 2018.  \\n12. Chua, Leon O., and Tamas Roska. \"The CNN paradigm.\"  IEEE Transactions on Circuits and Systems I: Fundamental \\nTheory and Applications  40.3 (1993): 147 -156. \\n13. Zheng, Yufeng, et al. \"CNN classification based on global and local features.\"  Real-Time Image Processing and Deep \\nLearning 2019 . Vol. 10996. International Society for Optics and Photonics, 2019.  \\n14. Wei, Wang, et al. \"Image object recogni tion via deep feature -based adaptive joint sparse representation.\"  Computational \\nIntelligence and Neuroscience  2019 (2019).  \\n15. Cao, Jianfang, et al. \"An improved convolutional neural network algorithm and its application in multilabel image \\nlabeling.\"  Computa tional Intelligence and Neuroscience  2019 (2019).  \\n16. Yu, Wei, et al. \"Visualizing and comparing AlexNet and VGG using deconvolutional layers.\"  Proceedings of the 33 rd \\nInternational Conference on Machine Learning . 2016.  \\n ', metadata={'source': '/content/9739-Article Text-17322-1-10-20210708.pdf', 'page': 7})]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DNTiAvgvNYC",
        "outputId": "9512b397-ffae-446c-adf0-d508dd689e21"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.add_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux831sq2pq3C",
        "outputId": "d2af1ba4-fc99-460f-bb67-1271501d1c15"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['4c970120-460b-472c-96f9-3a45d0a32541',\n",
              " 'd3d06b96-aa30-426a-9048-2fdfaa01f86d',\n",
              " '95ec17f6-f45e-482f-913b-c39d1f669215',\n",
              " '4a4f6219-5ce4-42d8-82a4-eab18806a4b2',\n",
              " '0ef64d41-ccbc-4774-a831-c65d150e9df3',\n",
              " '6de34c93-af1e-4c07-8222-dc34a9e20cf1',\n",
              " '927cee10-52ff-4ad0-bc65-1a9d3ed3592c',\n",
              " 'c4aa2a37-5dac-4478-a81d-1e9c5290e1a6']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(retriever.invoke(\"Traffic sign detection system?\")[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRoDhLHjsy5f",
        "outputId": "d14974da-9484-4e93-a509-c660dc3868bb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Turkish Journal of Computer and Mathematics Education  \n",
            " \n",
            " \n",
            "______________________________________________________________________________ ____  \n",
            "5526    \n",
            "  \n",
            "Research Article    Vol.12 No. 6 (2021), 5522 -5529 \n",
            "7. Traffic Sign Detection  \n",
            "Traffic Sign detection is the detection of traffic signs that takes place within the autonomous smart vehicle \n",
            "itself. The detection of traffic sign is taken  place as follows as shown in Fig 4.  \n",
            " \n",
            "Figure. 3: Flow  Diagram of Traffic Sign detection  \n",
            "The s mart vehicle can be equipped  with the software.  The above diagram indicates the proposed work of the \n",
            "Traffic sign detection system . \n",
            "The procedure of this work is implemented as follows:  \n",
            "• First, we take a sample video  as an input here,  then the first detection of traffic sign in a video is taken place \n",
            "with the help of  object detection Even though it’s in any shape  and size no matter it detects and this fed to \n",
            "Again traffic sign classification Process  \n",
            "• The detect traffic sign is fed to the Feature Engineering Process that divides the whole image into a huge \n",
            "number of pixels and then extracts the Features and then clustering and F iltration operation process is also \n",
            "performed.  \n",
            "• By the above process , the Image is then extracted and detected and then the image is compared with the \n",
            "testing process which is performed in the traffic sign Classification.   \n",
            "• The label map is done to check wi th the testing feature to verify the predicted result is correct and after \n",
            "verification , this display s the result.  \n",
            "• The Traffic Sign Detection Process is performed with the help of a dataset that gets trained and tested by the \n",
            "CNN in the Traffic sign Classi fication Process.     \n",
            "        \n",
            "8. Experimentation Results:  \n",
            "The Experiment is done with help of a flask that make s use of even HTML CSS JS along with python etc.  \n",
            "The Traffic Sign Classification is proved as best in automatically recognizing the traffic signs a nd helps in very \n",
            "fast detection of traffic signs. The interface of his project of traffic sign classification is shown as follows in \n",
            "Fig 5.   \n",
            "                                                      \n",
            "Figure. 4: Result  of Traffic Sign Classification  (on left) Input Image  (on right ) Predicted Traffic sign Image.  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\n",
        "    \"Traffic sign detection system?\",\n",
        "    score=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHdda33buBrS",
        "outputId": "c8bd89e0-b391-465e-fd86-a9e0032ffca5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Turkish Journal of Computer and Mathematics Education  \\n \\n \\n______________________________________________________________________________ ____  \\n5526    \\n  \\nResearch Article    Vol.12 No. 6 (2021), 5522 -5529 \\n7. Traffic Sign Detection  \\nTraffic Sign detection is the detection of traffic signs that takes place within the autonomous smart vehicle \\nitself. The detection of traffic sign is taken  place as follows as shown in Fig 4.  \\n \\nFigure. 3: Flow  Diagram of Traffic Sign detection  \\nThe s mart vehicle can be equipped  with the software.  The above diagram indicates the proposed work of the \\nTraffic sign detection system . \\nThe procedure of this work is implemented as follows:  \\n• First, we take a sample video  as an input here,  then the first detection of traffic sign in a video is taken place \\nwith the help of  object detection Even though it’s in any shape  and size no matter it detects and this fed to \\nAgain traffic sign classification Process  \\n• The detect traffic sign is fed to the Feature Engineering Process that divides the whole image into a huge \\nnumber of pixels and then extracts the Features and then clustering and F iltration operation process is also \\nperformed.  \\n• By the above process , the Image is then extracted and detected and then the image is compared with the \\ntesting process which is performed in the traffic sign Classification.   \\n• The label map is done to check wi th the testing feature to verify the predicted result is correct and after \\nverification , this display s the result.  \\n• The Traffic Sign Detection Process is performed with the help of a dataset that gets trained and tested by the \\nCNN in the Traffic sign Classi fication Process.     \\n        \\n8. Experimentation Results:  \\nThe Experiment is done with help of a flask that make s use of even HTML CSS JS along with python etc.  \\nThe Traffic Sign Classification is proved as best in automatically recognizing the traffic signs a nd helps in very \\nfast detection of traffic signs. The interface of his project of traffic sign classification is shown as follows in \\nFig 5.   \\n                                                      \\nFigure. 4: Result  of Traffic Sign Classification  (on left) Input Image  (on right ) Predicted Traffic sign Image.  \\n', metadata={'_additional': {'explainScore': '\\nHybrid (Result Set keyword,bm25) Document 0ef64d41-ccbc-4774-a831-c65d150e9df3: original score 0.57016164, normalized score: 0.49536237 - \\nHybrid (Result Set vector,hybridVector) Document 0ef64d41-ccbc-4774-a831-c65d150e9df3: original score 0.7246884, normalized score: 0.5', 'score': '0.9953624'}}),\n",
              " Document(page_content='Turkish Journal of Computer and Mathematics Education  \\n \\n \\n______________________________________________________________________________ ____  \\n5522    \\n  \\nResearch Article    Vol.12 No. 6 (2021), 5522 -5529 \\nA Deep Convolutional Neural Network for Traffic Sign Classification and \\nDetection with Voice Recognition  \\n \\nK.Prakash  a. \\na M-Tech student, VNR Vignana Jyothi Institute of Engineering and Technology (Autonomous), \\nNizampet , Affiliation to JNTUH.  \\n \\nArticle History: Received: 10 November 2020; Revised 12 January 2021 Accepted: 27 January 2021; \\nPublished online: 5 April 2021  \\n____________________________________________________________________________________________________  \\n \\nAbstract: Traffic  Sign De tection and Classification play  an important major role now adays in our daily life. \\nAnyway , Traffic Signs are present on roads. Even though, often the drivers do mistakes. It’s very hard to \\nrecognize and detect traffic signs while travelling on roads. Drivers may misinterpret traffic si gns, this leads to \\nAccidents and results in damage to the vehicle. To overcome this problem this project introduces a concept \\nnamed Traffic Sign Detection and Classification with Voice Recognition. This model is built by using CNN to \\nextract the images and  classify the traffic signs. Here DCCNN model is built to improve overall accuracy and \\nspeed. Here Classification process is also tested with AlexNet, VGG16, VGG19. This system reveals output that \\nrecognizes the traffic signs automatically that helps to de tect the street condition and alerts driver soon and this \\nenables to build a smart vehicle.  \\n \\nKeywords: DCCNN, VGG16, Deep Learning , Traffic Sign Detection , Automation.  \\n___________________________________________________________________________  \\n \\n1. Introduct ion  \\nNow a day’s various modes of Transport facilities are very important in our daily life [1].  At the same time \\nsafety also plays a very important role. For Safety Purpose in Roads already traffic signs are placed on roads to \\novercome accidents and to follow the traffic rules [2]. Traffic Signs guide us to be in our safety and they help \\nus to travel in our vehicle in the desired path. Even though the traffic signs are present on roads, the drivers \\noften do mistakes [3]. It’s very hard to detect and recognize the traffic signs while driving the vehicle. With the \\nrise in a huge number of road accidents happening every year there has been need ing to develop a safety \\nsystem that helps in contributing to the pedestrian vehicle and also driver’s safety [4].  \\n Traffic signs play a very important role and crucial role while driving vehicles. A traffic sign is also a part \\nof road infrastructure [ 5]. Here is an automated system that help s us to notify and identify and can help a lot. \\nThis aims to recognize and detect the traffic sign much faster and enables to build of an autonomous vehicle \\n[6].  \\n2. Background Theory  \\n \\nThe traffic signs  are placed on the road with different sizes and shape some traffic sign s are very blur and \\nsmall in size, which is covered by snow and heavy rainfall. Even though it‘s in any condition,  Our aim to \\ndetect traffic sign s [7]. The traffic sign classification is a process tha t receives the input of the image to detect \\nthe traffic sign ie.., U -Turn Ahead, Speed Limit 40, Caution Signs , etc… These signs recommend drivers to get \\nby giving input commands through progra mming the data in such to classify the traffic sign automatical ly in \\nvehicle s [8]. Traffic Sign Detection is a process that detects the traffic sign and performs the feature extraction \\nprocess, clustering , and filtering process and then the labeling process and testing process has to be taken place \\nthen detected resul t is to be displayed on the dashboard [9].  \\nThe dataset consists of various formats. With respect to size, shape structure. Images preprocessing enables \\nthe improvement of images and reduces the distortion effect [10]. Here I also used the Gaussian effect to \\neliminate the image noise [11].  The dataset processing is to be done to ensure the data is arranged suitably  to \\ndetect the traffic signs much faster.  \\n3. Challenges  of  the Study  \\n• Can build  a smart autonomous smart vehicle further to improve  the safety mechanism for drivers, \\nPedestrians , and Vehicles.  \\n• To Implement Navigating Mechanism for drivers safety.  ', metadata={'_additional': {'explainScore': '\\nHybrid (Result Set keyword,bm25) Document 4c970120-460b-472c-96f9-3a45d0a32541: original score 0.57531273, normalized score: 0.5 - \\nHybrid (Result Set vector,hybridVector) Document 4c970120-460b-472c-96f9-3a45d0a32541: original score 0.59813917, normalized score: 0.3900264', 'score': '0.8900264'}}),\n",
              " Document(page_content='A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \\n____________________________________________________________________________________________________________  \\n______________________________________________________________________________ ____  \\n5525   \\nFigure.1: Dataset  of traffic signs  \\n \\n6. Traffic Sign Classification  \\nTraffic Sign Classification is a process that takes place to detect the traffic sign with the help of an image only.  \\nThe flow diagram of Traffic sign Classification is shown as follows in Fig 3.  \\n \\nFigure. 2: Flow  Diagram of Traffic Sign Classification  \\nThe above diagra m indicates the proposed model of the Traffic Sign Classification System.  \\nThe Procedure of this work is implemented as follows:  \\n• Traffic signs will be in different size s and shapes. Resizing has to take place to achieve fast detection of \\ntraffic sign s.  \\n• In this process , the image is captured through the network. Identification of image s is taken place with the \\nhelp of Object Detection API.  \\n•  The pre -process ing includes tracing the path of the traffic sign and assigning the labels for it. The traffic Sign \\nclasses are arranged in the form of an array with the help of its respective data associated with i t to retrieve \\nthe data easily. This paper also implement s a loop to trace the data until the data associated with the \\nrespective image is traced.  \\n• Now bu ilding the model has been t aken place to train by using an Algorithm . Then training and testing \\nprocess is carried for the training model.  \\n• After the training and testing process is carried on then we can check its accuracy and confusion matrix then \\ntest w ith the desired image so the resultant traffic sign information is obtained.  \\n', metadata={'_additional': {'explainScore': '\\nHybrid (Result Set keyword,bm25) Document 4a4f6219-5ce4-42d8-82a4-eab18806a4b2: original score 0.5544258, normalized score: 0.4811951 - \\nHybrid (Result Set vector,hybridVector) Document 4a4f6219-5ce4-42d8-82a4-eab18806a4b2: original score 0.554675, normalized score: 0.35225523', 'score': '0.8334503'}}),\n",
              " Document(page_content='A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \\n____________________________________________________________________________________________________________  \\n______________________________________________________________________________ ____  \\n5529  11. Acknowledgment  \\nThe author  gratefully acknowledges the support provided by the department of inf ormation technology, \\nVNRVJIET, Hyderabad, India . \\n  \\nReferences   \\n1. Ryley, Tim J., et al. \"Investigating the contribution of Demand Responsive Transport to a sustainable local public \\ntransport system.\"  Research in Transportation Economics  48 (2014): 364 -372. \\n2. Greenblatt, Jeffery B., and Susan Shaheen. \"Automate d vehicles, on -demand mobility, and environmental \\nimpacts.\"  Current sustainable/renewable energy reports  2.3 (2015): 74 -81. \\n Austin, Kevin. \"The identification of mistakes in road accident records: Part 1, locational variables.\"  Accident Analysis \\n& Prevention  27.2 (2015): 261 -276.https://timesofindia.indiatimes.com/city/mumbai/sleepy -driver -crashes -suv-on-\\nmumbai -pune -expressway -7-passengers -killed/articleshow/18801253.cms .  \\n3. Sonkin, Beth, et al. \"Walking, cycling and transport safety: an analysis of child road deaths.\"  Journal of the Royal \\nSociety of Medicine  99.8 (2016): 402 -405. http://archive.indianexpress.com/news/accidents -on-expressway -prompt -\\nfresh -survey -of-state-highways/1083181/  \\n4. Khadaroo, Jameel, and Boopen Seetan ah. \"Transport infrastructure and tourism development.\"  Annals of tourism \\nresearch  34.4 (2017): 1021 -1032.  https:/ /timesofindia.indiatimes.com/india/traffic -junctions -account -for-half-of-road-\\ndeaths -in-india/articleshow/40956759.cms  \\n5. Haboucha, Chana J., Robert Ishaq, and Yoram Shiftan. \"User preferences regarding autonomous \\nvehicles.\"  Transportation Research Part C: Em erging Technologies  78 (2017): 37 -49. \\n6. De La Escalera, Arturo, et al. \"Road traffic sign detection and classification.\"  IEEE transactions on industrial \\nelectronics  44.6 (2015 ): 848 -859. \\n7. Zhu, Zhe, et al. \"Traffic -sign detection and classification in the wild.\"  Proceedings of the IEEE conference on computer \\nvision and pattern recognition . 2016..  \\n8. Gavrila, Dariu M. \"Traffic sign recognition revisited.\"  Mustererkennung 1999 . Springer, Berlin, Heidelberg, 2018 . 86-\\n93. \\n9. Houben, Sebastian, e t al. \"Detection of traffic signs in real -world images: The German Traffic Sign Detection \\nBenchmark.\"  The 2013 international joint conference on neural networks (IJCNN) . Ieee, 2013.  \\n10. Guo, J., Zhang, H., Zhen, D., Shi, Z., Gu, F., & Ball, A. D. (2020). An en hanced modulation signal bispectrum analysis \\nfor bearing fault detection based on non -Gaussian noise suppression.  Measurement , 151, 107240.  \\n11. Jogin, Manjunath, et al. \"Feature extraction using convolution neural networks (CNN) and deep learning.\"  2018 3rd \\nIEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT). \\nIEEE, 2018.  \\n12. Chua, Leon O., and Tamas Roska. \"The CNN paradigm.\"  IEEE Transactions on Circuits and Systems I: Fundamental \\nTheory and Applications  40.3 (1993): 147 -156. \\n13. Zheng, Yufeng, et al. \"CNN classification based on global and local features.\"  Real-Time Image Processing and Deep \\nLearning 2019 . Vol. 10996. International Society for Optics and Photonics, 2019.  \\n14. Wei, Wang, et al. \"Image object recogni tion via deep feature -based adaptive joint sparse representation.\"  Computational \\nIntelligence and Neuroscience  2019 (2019).  \\n15. Cao, Jianfang, et al. \"An improved convolutional neural network algorithm and its application in multilabel image \\nlabeling.\"  Computa tional Intelligence and Neuroscience  2019 (2019).  \\n16. Yu, Wei, et al. \"Visualizing and comparing AlexNet and VGG using deconvolutional layers.\"  Proceedings of the 33 rd \\nInternational Conference on Machine Learning . 2016.  \\n ', metadata={'_additional': {'explainScore': '\\nHybrid (Result Set keyword,bm25) Document c4aa2a37-5dac-4478-a81d-1e9c5290e1a6: original score 0.40382224, normalized score: 0.34560388 - \\nHybrid (Result Set vector,hybridVector) Document c4aa2a37-5dac-4478-a81d-1e9c5290e1a6: original score 0.5380318, normalized score: 0.337792', 'score': '0.68339586'}})]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "Vt5vaVuLEdY9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HkhbVjqiMJXJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "heu-l-l176Pp"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = (\n",
        "    \"Use the given context to answer the question. \"\n",
        "    \"If you don't know the answer, say you don't know. \"\n",
        "    \"Use three sentence maximum and keep the answer concise. \"\n",
        "    \"Context: {context}\"\n",
        ")"
      ],
      "metadata": {
        "id": "RrEl6Nm87_Vi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Gg0TRf_Q72P6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "template = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you do not have the relevant information needed to provide a verified answer, don't try to make up an answer.\n",
        "When providing an answer, aim for clarity and precision. Position yourself as a knowledgeable authority on the topic, but also be mindful to explain the information in a manner that is accessible and comprehensible to those without a technical background.\n",
        "Always say \"Do you have any more questions pertaining to this instrument?\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "GNPZSFun-4Ka"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain"
      ],
      "metadata": {
        "id": "Q3lt9jMW8hxK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)"
      ],
      "metadata": {
        "id": "ppRiYOIa8b6y"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever,)\n"
      ],
      "metadata": {
        "id": "3t7fVtBaAOfq"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = hybrid_chain.invoke(\"what Traffic sign detection system?\")\n",
        "print(result1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0DfMLiJ6lbr",
        "outputId": "a3de5e2f-3866-4542-d228-beb6d218e2a8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'what Traffic sign detection system?', 'result': \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nTurkish Journal of Computer and Mathematics Education  \\n \\n \\n______________________________________________________________________________ ____  \\n5526    \\n  \\nResearch Article    Vol.12 No. 6 (2021), 5522 -5529 \\n7. Traffic Sign Detection  \\nTraffic Sign detection is the detection of traffic signs that takes place within the autonomous smart vehicle \\nitself. The detection of traffic sign is taken  place as follows as shown in Fig 4.  \\n \\nFigure. 3: Flow  Diagram of Traffic Sign detection  \\nThe s mart vehicle can be equipped  with the software.  The above diagram indicates the proposed work of the \\nTraffic sign detection system . \\nThe procedure of this work is implemented as follows:  \\n• First, we take a sample video  as an input here,  then the first detection of traffic sign in a video is taken place \\nwith the help of  object detection Even though it’s in any shape  and size no matter it detects and this fed to \\nAgain traffic sign classification Process  \\n• The detect traffic sign is fed to the Feature Engineering Process that divides the whole image into a huge \\nnumber of pixels and then extracts the Features and then clustering and F iltration operation process is also \\nperformed.  \\n• By the above process , the Image is then extracted and detected and then the image is compared with the \\ntesting process which is performed in the traffic sign Classification.   \\n• The label map is done to check wi th the testing feature to verify the predicted result is correct and after \\nverification , this display s the result.  \\n• The Traffic Sign Detection Process is performed with the help of a dataset that gets trained and tested by the \\nCNN in the Traffic sign Classi fication Process.     \\n        \\n8. Experimentation Results:  \\nThe Experiment is done with help of a flask that make s use of even HTML CSS JS along with python etc.  \\nThe Traffic Sign Classification is proved as best in automatically recognizing the traffic signs a nd helps in very \\nfast detection of traffic signs. The interface of his project of traffic sign classification is shown as follows in \\nFig 5.   \\n                                                      \\nFigure. 4: Result  of Traffic Sign Classification  (on left) Input Image  (on right ) Predicted Traffic sign Image.  \\n\\n\\nTurkish Journal of Computer and Mathematics Education  \\n \\n \\n______________________________________________________________________________ ____  \\n5522    \\n  \\nResearch Article    Vol.12 No. 6 (2021), 5522 -5529 \\nA Deep Convolutional Neural Network for Traffic Sign Classification and \\nDetection with Voice Recognition  \\n \\nK.Prakash  a. \\na M-Tech student, VNR Vignana Jyothi Institute of Engineering and Technology (Autonomous), \\nNizampet , Affiliation to JNTUH.  \\n \\nArticle History: Received: 10 November 2020; Revised 12 January 2021 Accepted: 27 January 2021; \\nPublished online: 5 April 2021  \\n____________________________________________________________________________________________________  \\n \\nAbstract: Traffic  Sign De tection and Classification play  an important major role now adays in our daily life. \\nAnyway , Traffic Signs are present on roads. Even though, often the drivers do mistakes. It’s very hard to \\nrecognize and detect traffic signs while travelling on roads. Drivers may misinterpret traffic si gns, this leads to \\nAccidents and results in damage to the vehicle. To overcome this problem this project introduces a concept \\nnamed Traffic Sign Detection and Classification with Voice Recognition. This model is built by using CNN to \\nextract the images and  classify the traffic signs. Here DCCNN model is built to improve overall accuracy and \\nspeed. Here Classification process is also tested with AlexNet, VGG16, VGG19. This system reveals output that \\nrecognizes the traffic signs automatically that helps to de tect the street condition and alerts driver soon and this \\nenables to build a smart vehicle.  \\n \\nKeywords: DCCNN, VGG16, Deep Learning , Traffic Sign Detection , Automation.  \\n___________________________________________________________________________  \\n \\n1. Introduct ion  \\nNow a day’s various modes of Transport facilities are very important in our daily life [1].  At the same time \\nsafety also plays a very important role. For Safety Purpose in Roads already traffic signs are placed on roads to \\novercome accidents and to follow the traffic rules [2]. Traffic Signs guide us to be in our safety and they help \\nus to travel in our vehicle in the desired path. Even though the traffic signs are present on roads, the drivers \\noften do mistakes [3]. It’s very hard to detect and recognize the traffic signs while driving the vehicle. With the \\nrise in a huge number of road accidents happening every year there has been need ing to develop a safety \\nsystem that helps in contributing to the pedestrian vehicle and also driver’s safety [4].  \\n Traffic signs play a very important role and crucial role while driving vehicles. A traffic sign is also a part \\nof road infrastructure [ 5]. Here is an automated system that help s us to notify and identify and can help a lot. \\nThis aims to recognize and detect the traffic sign much faster and enables to build of an autonomous vehicle \\n[6].  \\n2. Background Theory  \\n \\nThe traffic signs  are placed on the road with different sizes and shape some traffic sign s are very blur and \\nsmall in size, which is covered by snow and heavy rainfall. Even though it‘s in any condition,  Our aim to \\ndetect traffic sign s [7]. The traffic sign classification is a process tha t receives the input of the image to detect \\nthe traffic sign ie.., U -Turn Ahead, Speed Limit 40, Caution Signs , etc… These signs recommend drivers to get \\nby giving input commands through progra mming the data in such to classify the traffic sign automatical ly in \\nvehicle s [8]. Traffic Sign Detection is a process that detects the traffic sign and performs the feature extraction \\nprocess, clustering , and filtering process and then the labeling process and testing process has to be taken place \\nthen detected resul t is to be displayed on the dashboard [9].  \\nThe dataset consists of various formats. With respect to size, shape structure. Images preprocessing enables \\nthe improvement of images and reduces the distortion effect [10]. Here I also used the Gaussian effect to \\neliminate the image noise [11].  The dataset processing is to be done to ensure the data is arranged suitably  to \\ndetect the traffic signs much faster.  \\n3. Challenges  of  the Study  \\n• Can build  a smart autonomous smart vehicle further to improve  the safety mechanism for drivers, \\nPedestrians , and Vehicles.  \\n• To Implement Navigating Mechanism for drivers safety.  \\n\\nA Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \\n____________________________________________________________________________________________________________  \\n______________________________________________________________________________ ____  \\n5525   \\nFigure.1: Dataset  of traffic signs  \\n \\n6. Traffic Sign Classification  \\nTraffic Sign Classification is a process that takes place to detect the traffic sign with the help of an image only.  \\nThe flow diagram of Traffic sign Classification is shown as follows in Fig 3.  \\n \\nFigure. 2: Flow  Diagram of Traffic Sign Classification  \\nThe above diagra m indicates the proposed model of the Traffic Sign Classification System.  \\nThe Procedure of this work is implemented as follows:  \\n• Traffic signs will be in different size s and shapes. Resizing has to take place to achieve fast detection of \\ntraffic sign s.  \\n• In this process , the image is captured through the network. Identification of image s is taken place with the \\nhelp of Object Detection API.  \\n•  The pre -process ing includes tracing the path of the traffic sign and assigning the labels for it. The traffic Sign \\nclasses are arranged in the form of an array with the help of its respective data associated with i t to retrieve \\nthe data easily. This paper also implement s a loop to trace the data until the data associated with the \\nrespective image is traced.  \\n• Now bu ilding the model has been t aken place to train by using an Algorithm . Then training and testing \\nprocess is carried for the training model.  \\n• After the training and testing process is carried on then we can check its accuracy and confusion matrix then \\ntest w ith the desired image so the resultant traffic sign information is obtained.  \\n\\n\\nA Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \\n____________________________________________________________________________________________________________  \\n______________________________________________________________________________ ____  \\n5527  The above diagrams indicate the output of Traffic Sign Classification. First Fig  5(on left ) indicates the input \\nof the image and another one when I upload traffic this shows the desired output of an image. Traffic Sign \\nClassification is very necessary to identify traffic signs much faster even though the traffic sign is in any size or \\nshape. Even though the traffic sign is a blur and faded then also with this we can detect it very easily  as shown \\nin Fig 5 (on right ). Traffic sign detection is very necessary to take place detection automatically when we are in \\ntravelling in a vehicle. This help s to detect the traffic sign al before the tr affic sign is arrived .  This alert s the \\nwhen the person is in sleepy mode. This also guide s the way of road automatically. The following fig of traffic \\nsign detection is shown in Fig  6 as follows.  Fig 6 (on left ) indicates here the input video frame without detection \\nof any traffic sign signal. In Figure 6 (on right ) The detection of Traffic sign detection has been taken place . \\n    \\nFigure. 5: Result of Traffic Sign Detection  (on left ) Input video (on right ) Traffic Sig n detection  \\n \\nHere the output of Traffic sign Classification and Detection is fed to the input of GTTS library in python is \\nused to implement the same result with speech. The Traffic Sign is Recognition is performed from a Real-time \\nWebcam camera with voice  software i.e., My Groovy music. This involves the same training method as Traffic \\nsign detection but the procedure is followed with OpenCV python. The Traffic Sign Recognition is performed \\nas follows as shown in Fig 7.  \\n \\nFigure. 6: Traffic  Sign Recognition  \\n9. Comparative  Results   \\n     Here the Model is implemented with VGG16, VGG19, AlexNet. The performance of various algorithms is \\nimplemented as follows:  \\nThe VGG16 is implemented for the traffic sign c lassification of images. VGG16 holds 82.77% Accuracy, \\nbut this is used to handle a huge amount of datasets. Here the VGG 19 is implemented the Results are \\nsomewhat  better  compared to VGG16 . VGG19 holds  an accuracy of 88.39%. Here also input images of the \\ndataset are fed to VGG19 . To improve this model I use DCCNN (Dual Channel Convolutional Neural \\nNetwork). Here the DCCN N achieves high accuracy compared to the Normal CNN method. The DCCNN \\n\\n\\nQuestion: what Traffic sign detection system?\\nHelpful Answer: The proposed work in this article is for a Traffic Sign Detection System that uses a CNN (Convolutional Neural Network) to detect Traffic Signs in real-time. The system can be implemented in autonomous smart vehicles to improve safety by automatically detecting and recognizing Traffic Signs, even when they are in any condition or size. The system can also be trained and tested using different algorithms such as AlexNet, VGG16, and VGG19,\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result1['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Flsjn21WMypT",
        "outputId": "87a7eece-dc37-4749-db4d-73d3333adb3d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "Turkish Journal of Computer and Mathematics Education  \n",
            " \n",
            " \n",
            "______________________________________________________________________________ ____  \n",
            "5526    \n",
            "  \n",
            "Research Article    Vol.12 No. 6 (2021), 5522 -5529 \n",
            "7. Traffic Sign Detection  \n",
            "Traffic Sign detection is the detection of traffic signs that takes place within the autonomous smart vehicle \n",
            "itself. The detection of traffic sign is taken  place as follows as shown in Fig 4.  \n",
            " \n",
            "Figure. 3: Flow  Diagram of Traffic Sign detection  \n",
            "The s mart vehicle can be equipped  with the software.  The above diagram indicates the proposed work of the \n",
            "Traffic sign detection system . \n",
            "The procedure of this work is implemented as follows:  \n",
            "• First, we take a sample video  as an input here,  then the first detection of traffic sign in a video is taken place \n",
            "with the help of  object detection Even though it’s in any shape  and size no matter it detects and this fed to \n",
            "Again traffic sign classification Process  \n",
            "• The detect traffic sign is fed to the Feature Engineering Process that divides the whole image into a huge \n",
            "number of pixels and then extracts the Features and then clustering and F iltration operation process is also \n",
            "performed.  \n",
            "• By the above process , the Image is then extracted and detected and then the image is compared with the \n",
            "testing process which is performed in the traffic sign Classification.   \n",
            "• The label map is done to check wi th the testing feature to verify the predicted result is correct and after \n",
            "verification , this display s the result.  \n",
            "• The Traffic Sign Detection Process is performed with the help of a dataset that gets trained and tested by the \n",
            "CNN in the Traffic sign Classi fication Process.     \n",
            "        \n",
            "8. Experimentation Results:  \n",
            "The Experiment is done with help of a flask that make s use of even HTML CSS JS along with python etc.  \n",
            "The Traffic Sign Classification is proved as best in automatically recognizing the traffic signs a nd helps in very \n",
            "fast detection of traffic signs. The interface of his project of traffic sign classification is shown as follows in \n",
            "Fig 5.   \n",
            "                                                      \n",
            "Figure. 4: Result  of Traffic Sign Classification  (on left) Input Image  (on right ) Predicted Traffic sign Image.  \n",
            "\n",
            "\n",
            "Turkish Journal of Computer and Mathematics Education  \n",
            " \n",
            " \n",
            "______________________________________________________________________________ ____  \n",
            "5522    \n",
            "  \n",
            "Research Article    Vol.12 No. 6 (2021), 5522 -5529 \n",
            "A Deep Convolutional Neural Network for Traffic Sign Classification and \n",
            "Detection with Voice Recognition  \n",
            " \n",
            "K.Prakash  a. \n",
            "a M-Tech student, VNR Vignana Jyothi Institute of Engineering and Technology (Autonomous), \n",
            "Nizampet , Affiliation to JNTUH.  \n",
            " \n",
            "Article History: Received: 10 November 2020; Revised 12 January 2021 Accepted: 27 January 2021; \n",
            "Published online: 5 April 2021  \n",
            "____________________________________________________________________________________________________  \n",
            " \n",
            "Abstract: Traffic  Sign De tection and Classification play  an important major role now adays in our daily life. \n",
            "Anyway , Traffic Signs are present on roads. Even though, often the drivers do mistakes. It’s very hard to \n",
            "recognize and detect traffic signs while travelling on roads. Drivers may misinterpret traffic si gns, this leads to \n",
            "Accidents and results in damage to the vehicle. To overcome this problem this project introduces a concept \n",
            "named Traffic Sign Detection and Classification with Voice Recognition. This model is built by using CNN to \n",
            "extract the images and  classify the traffic signs. Here DCCNN model is built to improve overall accuracy and \n",
            "speed. Here Classification process is also tested with AlexNet, VGG16, VGG19. This system reveals output that \n",
            "recognizes the traffic signs automatically that helps to de tect the street condition and alerts driver soon and this \n",
            "enables to build a smart vehicle.  \n",
            " \n",
            "Keywords: DCCNN, VGG16, Deep Learning , Traffic Sign Detection , Automation.  \n",
            "___________________________________________________________________________  \n",
            " \n",
            "1. Introduct ion  \n",
            "Now a day’s various modes of Transport facilities are very important in our daily life [1].  At the same time \n",
            "safety also plays a very important role. For Safety Purpose in Roads already traffic signs are placed on roads to \n",
            "overcome accidents and to follow the traffic rules [2]. Traffic Signs guide us to be in our safety and they help \n",
            "us to travel in our vehicle in the desired path. Even though the traffic signs are present on roads, the drivers \n",
            "often do mistakes [3]. It’s very hard to detect and recognize the traffic signs while driving the vehicle. With the \n",
            "rise in a huge number of road accidents happening every year there has been need ing to develop a safety \n",
            "system that helps in contributing to the pedestrian vehicle and also driver’s safety [4].  \n",
            " Traffic signs play a very important role and crucial role while driving vehicles. A traffic sign is also a part \n",
            "of road infrastructure [ 5]. Here is an automated system that help s us to notify and identify and can help a lot. \n",
            "This aims to recognize and detect the traffic sign much faster and enables to build of an autonomous vehicle \n",
            "[6].  \n",
            "2. Background Theory  \n",
            " \n",
            "The traffic signs  are placed on the road with different sizes and shape some traffic sign s are very blur and \n",
            "small in size, which is covered by snow and heavy rainfall. Even though it‘s in any condition,  Our aim to \n",
            "detect traffic sign s [7]. The traffic sign classification is a process tha t receives the input of the image to detect \n",
            "the traffic sign ie.., U -Turn Ahead, Speed Limit 40, Caution Signs , etc… These signs recommend drivers to get \n",
            "by giving input commands through progra mming the data in such to classify the traffic sign automatical ly in \n",
            "vehicle s [8]. Traffic Sign Detection is a process that detects the traffic sign and performs the feature extraction \n",
            "process, clustering , and filtering process and then the labeling process and testing process has to be taken place \n",
            "then detected resul t is to be displayed on the dashboard [9].  \n",
            "The dataset consists of various formats. With respect to size, shape structure. Images preprocessing enables \n",
            "the improvement of images and reduces the distortion effect [10]. Here I also used the Gaussian effect to \n",
            "eliminate the image noise [11].  The dataset processing is to be done to ensure the data is arranged suitably  to \n",
            "detect the traffic signs much faster.  \n",
            "3. Challenges  of  the Study  \n",
            "• Can build  a smart autonomous smart vehicle further to improve  the safety mechanism for drivers, \n",
            "Pedestrians , and Vehicles.  \n",
            "• To Implement Navigating Mechanism for drivers safety.  \n",
            "\n",
            "A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \n",
            "____________________________________________________________________________________________________________  \n",
            "______________________________________________________________________________ ____  \n",
            "5525   \n",
            "Figure.1: Dataset  of traffic signs  \n",
            " \n",
            "6. Traffic Sign Classification  \n",
            "Traffic Sign Classification is a process that takes place to detect the traffic sign with the help of an image only.  \n",
            "The flow diagram of Traffic sign Classification is shown as follows in Fig 3.  \n",
            " \n",
            "Figure. 2: Flow  Diagram of Traffic Sign Classification  \n",
            "The above diagra m indicates the proposed model of the Traffic Sign Classification System.  \n",
            "The Procedure of this work is implemented as follows:  \n",
            "• Traffic signs will be in different size s and shapes. Resizing has to take place to achieve fast detection of \n",
            "traffic sign s.  \n",
            "• In this process , the image is captured through the network. Identification of image s is taken place with the \n",
            "help of Object Detection API.  \n",
            "•  The pre -process ing includes tracing the path of the traffic sign and assigning the labels for it. The traffic Sign \n",
            "classes are arranged in the form of an array with the help of its respective data associated with i t to retrieve \n",
            "the data easily. This paper also implement s a loop to trace the data until the data associated with the \n",
            "respective image is traced.  \n",
            "• Now bu ilding the model has been t aken place to train by using an Algorithm . Then training and testing \n",
            "process is carried for the training model.  \n",
            "• After the training and testing process is carried on then we can check its accuracy and confusion matrix then \n",
            "test w ith the desired image so the resultant traffic sign information is obtained.  \n",
            "\n",
            "\n",
            "A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \n",
            "____________________________________________________________________________________________________________  \n",
            "______________________________________________________________________________ ____  \n",
            "5527  The above diagrams indicate the output of Traffic Sign Classification. First Fig  5(on left ) indicates the input \n",
            "of the image and another one when I upload traffic this shows the desired output of an image. Traffic Sign \n",
            "Classification is very necessary to identify traffic signs much faster even though the traffic sign is in any size or \n",
            "shape. Even though the traffic sign is a blur and faded then also with this we can detect it very easily  as shown \n",
            "in Fig 5 (on right ). Traffic sign detection is very necessary to take place detection automatically when we are in \n",
            "travelling in a vehicle. This help s to detect the traffic sign al before the tr affic sign is arrived .  This alert s the \n",
            "when the person is in sleepy mode. This also guide s the way of road automatically. The following fig of traffic \n",
            "sign detection is shown in Fig  6 as follows.  Fig 6 (on left ) indicates here the input video frame without detection \n",
            "of any traffic sign signal. In Figure 6 (on right ) The detection of Traffic sign detection has been taken place . \n",
            "    \n",
            "Figure. 5: Result of Traffic Sign Detection  (on left ) Input video (on right ) Traffic Sig n detection  \n",
            " \n",
            "Here the output of Traffic sign Classification and Detection is fed to the input of GTTS library in python is \n",
            "used to implement the same result with speech. The Traffic Sign is Recognition is performed from a Real-time \n",
            "Webcam camera with voice  software i.e., My Groovy music. This involves the same training method as Traffic \n",
            "sign detection but the procedure is followed with OpenCV python. The Traffic Sign Recognition is performed \n",
            "as follows as shown in Fig 7.  \n",
            " \n",
            "Figure. 6: Traffic  Sign Recognition  \n",
            "9. Comparative  Results   \n",
            "     Here the Model is implemented with VGG16, VGG19, AlexNet. The performance of various algorithms is \n",
            "implemented as follows:  \n",
            "The VGG16 is implemented for the traffic sign c lassification of images. VGG16 holds 82.77% Accuracy, \n",
            "but this is used to handle a huge amount of datasets. Here the VGG 19 is implemented the Results are \n",
            "somewhat  better  compared to VGG16 . VGG19 holds  an accuracy of 88.39%. Here also input images of the \n",
            "dataset are fed to VGG19 . To improve this model I use DCCNN (Dual Channel Convolutional Neural \n",
            "Network). Here the DCCN N achieves high accuracy compared to the Normal CNN method. The DCCNN \n",
            "\n",
            "\n",
            "Question: what Traffic sign detection system?\n",
            "Helpful Answer: The proposed work in this article is for a Traffic Sign Detection System that uses a CNN (Convolutional Neural Network) to detect Traffic Signs in real-time. The system can be implemented in autonomous smart vehicles to improve safety by automatically detecting and recognizing Traffic Signs, even when they are in any condition or size. The system can also be trained and tested using different algorithms such as AlexNet, VGG16, and VGG19,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"Dataset  of traffic signs?\""
      ],
      "metadata": {
        "id": "QhG3Krz99APy"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = hybrid_chain.invoke({\"query\":query})"
      ],
      "metadata": {
        "id": "hmmRp1O_ArC9"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough"
      ],
      "metadata": {
        "id": "LZ-Id5sW-LLR"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the RAG chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()} |\n",
        "    prompt |\n",
        "    llm\n",
        ")"
      ],
      "metadata": {
        "id": "b1DvxugA-DIC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"Dataset  of traffic signs ?\""
      ],
      "metadata": {
        "id": "OTU5Wycg-l9y"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=rag_chain.invoke(query)"
      ],
      "metadata": {
        "id": "ykAekNO_-bkZ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqKKvHQ-_05x",
        "outputId": "b100290e-b89c-4087-a740-8feced1903b1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Use the following pieces of context to answer the question at the end.\n",
            "If you don't know the answer, just say that you do not have the relevant information needed to provide a verified answer, don't try to make up an answer.\n",
            "When providing an answer, aim for clarity and precision. Position yourself as a knowledgeable authority on the topic, but also be mindful to explain the information in a manner that is accessible and comprehensible to those without a technical background.\n",
            "Always say \"Do you have any more questions pertaining to this instrument?\" at the end of the answer.\n",
            "[Document(page_content='A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \\n____________________________________________________________________________________________________________  \\n______________________________________________________________________________ ____  \\n5525   \\nFigure.1: Dataset  of traffic signs  \\n \\n6. Traffic Sign Classification  \\nTraffic Sign Classification is a process that takes place to detect the traffic sign with the help of an image only.  \\nThe flow diagram of Traffic sign Classification is shown as follows in Fig 3.  \\n \\nFigure. 2: Flow  Diagram of Traffic Sign Classification  \\nThe above diagra m indicates the proposed model of the Traffic Sign Classification System.  \\nThe Procedure of this work is implemented as follows:  \\n• Traffic signs will be in different size s and shapes. Resizing has to take place to achieve fast detection of \\ntraffic sign s.  \\n• In this process , the image is captured through the network. Identification of image s is taken place with the \\nhelp of Object Detection API.  \\n•  The pre -process ing includes tracing the path of the traffic sign and assigning the labels for it. The traffic Sign \\nclasses are arranged in the form of an array with the help of its respective data associated with i t to retrieve \\nthe data easily. This paper also implement s a loop to trace the data until the data associated with the \\nrespective image is traced.  \\n• Now bu ilding the model has been t aken place to train by using an Algorithm . Then training and testing \\nprocess is carried for the training model.  \\n• After the training and testing process is carried on then we can check its accuracy and confusion matrix then \\ntest w ith the desired image so the resultant traffic sign information is obtained.  \\n'), Document(page_content='Turkish Journal of Computer and Mathematics Education  \\n \\n \\n______________________________________________________________________________ ____  \\n5526    \\n  \\nResearch Article    Vol.12 No. 6 (2021), 5522 -5529 \\n7. Traffic Sign Detection  \\nTraffic Sign detection is the detection of traffic signs that takes place within the autonomous smart vehicle \\nitself. The detection of traffic sign is taken  place as follows as shown in Fig 4.  \\n \\nFigure. 3: Flow  Diagram of Traffic Sign detection  \\nThe s mart vehicle can be equipped  with the software.  The above diagram indicates the proposed work of the \\nTraffic sign detection system . \\nThe procedure of this work is implemented as follows:  \\n• First, we take a sample video  as an input here,  then the first detection of traffic sign in a video is taken place \\nwith the help of  object detection Even though it’s in any shape  and size no matter it detects and this fed to \\nAgain traffic sign classification Process  \\n• The detect traffic sign is fed to the Feature Engineering Process that divides the whole image into a huge \\nnumber of pixels and then extracts the Features and then clustering and F iltration operation process is also \\nperformed.  \\n• By the above process , the Image is then extracted and detected and then the image is compared with the \\ntesting process which is performed in the traffic sign Classification.   \\n• The label map is done to check wi th the testing feature to verify the predicted result is correct and after \\nverification , this display s the result.  \\n• The Traffic Sign Detection Process is performed with the help of a dataset that gets trained and tested by the \\nCNN in the Traffic sign Classi fication Process.     \\n        \\n8. Experimentation Results:  \\nThe Experiment is done with help of a flask that make s use of even HTML CSS JS along with python etc.  \\nThe Traffic Sign Classification is proved as best in automatically recognizing the traffic signs a nd helps in very \\nfast detection of traffic signs. The interface of his project of traffic sign classification is shown as follows in \\nFig 5.   \\n                                                      \\nFigure. 4: Result  of Traffic Sign Classification  (on left) Input Image  (on right ) Predicted Traffic sign Image.  \\n'), Document(page_content='Turkish Journal of Computer and Mathematics Education  \\n \\n \\n______________________________________________________________________________ ____  \\n5522    \\n  \\nResearch Article    Vol.12 No. 6 (2021), 5522 -5529 \\nA Deep Convolutional Neural Network for Traffic Sign Classification and \\nDetection with Voice Recognition  \\n \\nK.Prakash  a. \\na M-Tech student, VNR Vignana Jyothi Institute of Engineering and Technology (Autonomous), \\nNizampet , Affiliation to JNTUH.  \\n \\nArticle History: Received: 10 November 2020; Revised 12 January 2021 Accepted: 27 January 2021; \\nPublished online: 5 April 2021  \\n____________________________________________________________________________________________________  \\n \\nAbstract: Traffic  Sign De tection and Classification play  an important major role now adays in our daily life. \\nAnyway , Traffic Signs are present on roads. Even though, often the drivers do mistakes. It’s very hard to \\nrecognize and detect traffic signs while travelling on roads. Drivers may misinterpret traffic si gns, this leads to \\nAccidents and results in damage to the vehicle. To overcome this problem this project introduces a concept \\nnamed Traffic Sign Detection and Classification with Voice Recognition. This model is built by using CNN to \\nextract the images and  classify the traffic signs. Here DCCNN model is built to improve overall accuracy and \\nspeed. Here Classification process is also tested with AlexNet, VGG16, VGG19. This system reveals output that \\nrecognizes the traffic signs automatically that helps to de tect the street condition and alerts driver soon and this \\nenables to build a smart vehicle.  \\n \\nKeywords: DCCNN, VGG16, Deep Learning , Traffic Sign Detection , Automation.  \\n___________________________________________________________________________  \\n \\n1. Introduct ion  \\nNow a day’s various modes of Transport facilities are very important in our daily life [1].  At the same time \\nsafety also plays a very important role. For Safety Purpose in Roads already traffic signs are placed on roads to \\novercome accidents and to follow the traffic rules [2]. Traffic Signs guide us to be in our safety and they help \\nus to travel in our vehicle in the desired path. Even though the traffic signs are present on roads, the drivers \\noften do mistakes [3]. It’s very hard to detect and recognize the traffic signs while driving the vehicle. With the \\nrise in a huge number of road accidents happening every year there has been need ing to develop a safety \\nsystem that helps in contributing to the pedestrian vehicle and also driver’s safety [4].  \\n Traffic signs play a very important role and crucial role while driving vehicles. A traffic sign is also a part \\nof road infrastructure [ 5]. Here is an automated system that help s us to notify and identify and can help a lot. \\nThis aims to recognize and detect the traffic sign much faster and enables to build of an autonomous vehicle \\n[6].  \\n2. Background Theory  \\n \\nThe traffic signs  are placed on the road with different sizes and shape some traffic sign s are very blur and \\nsmall in size, which is covered by snow and heavy rainfall. Even though it‘s in any condition,  Our aim to \\ndetect traffic sign s [7]. The traffic sign classification is a process tha t receives the input of the image to detect \\nthe traffic sign ie.., U -Turn Ahead, Speed Limit 40, Caution Signs , etc… These signs recommend drivers to get \\nby giving input commands through progra mming the data in such to classify the traffic sign automatical ly in \\nvehicle s [8]. Traffic Sign Detection is a process that detects the traffic sign and performs the feature extraction \\nprocess, clustering , and filtering process and then the labeling process and testing process has to be taken place \\nthen detected resul t is to be displayed on the dashboard [9].  \\nThe dataset consists of various formats. With respect to size, shape structure. Images preprocessing enables \\nthe improvement of images and reduces the distortion effect [10]. Here I also used the Gaussian effect to \\neliminate the image noise [11].  The dataset processing is to be done to ensure the data is arranged suitably  to \\ndetect the traffic signs much faster.  \\n3. Challenges  of  the Study  \\n• Can build  a smart autonomous smart vehicle further to improve  the safety mechanism for drivers, \\nPedestrians , and Vehicles.  \\n• To Implement Navigating Mechanism for drivers safety.  '), Document(page_content='A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \\n____________________________________________________________________________________________________________  \\n______________________________________________________________________________ ____  \\n5527  The above diagrams indicate the output of Traffic Sign Classification. First Fig  5(on left ) indicates the input \\nof the image and another one when I upload traffic this shows the desired output of an image. Traffic Sign \\nClassification is very necessary to identify traffic signs much faster even though the traffic sign is in any size or \\nshape. Even though the traffic sign is a blur and faded then also with this we can detect it very easily  as shown \\nin Fig 5 (on right ). Traffic sign detection is very necessary to take place detection automatically when we are in \\ntravelling in a vehicle. This help s to detect the traffic sign al before the tr affic sign is arrived .  This alert s the \\nwhen the person is in sleepy mode. This also guide s the way of road automatically. The following fig of traffic \\nsign detection is shown in Fig  6 as follows.  Fig 6 (on left ) indicates here the input video frame without detection \\nof any traffic sign signal. In Figure 6 (on right ) The detection of Traffic sign detection has been taken place . \\n    \\nFigure. 5: Result of Traffic Sign Detection  (on left ) Input video (on right ) Traffic Sig n detection  \\n \\nHere the output of Traffic sign Classification and Detection is fed to the input of GTTS library in python is \\nused to implement the same result with speech. The Traffic Sign is Recognition is performed from a Real-time \\nWebcam camera with voice  software i.e., My Groovy music. This involves the same training method as Traffic \\nsign detection but the procedure is followed with OpenCV python. The Traffic Sign Recognition is performed \\nas follows as shown in Fig 7.  \\n \\nFigure. 6: Traffic  Sign Recognition  \\n9. Comparative  Results   \\n     Here the Model is implemented with VGG16, VGG19, AlexNet. The performance of various algorithms is \\nimplemented as follows:  \\nThe VGG16 is implemented for the traffic sign c lassification of images. VGG16 holds 82.77% Accuracy, \\nbut this is used to handle a huge amount of datasets. Here the VGG 19 is implemented the Results are \\nsomewhat  better  compared to VGG16 . VGG19 holds  an accuracy of 88.39%. Here also input images of the \\ndataset are fed to VGG19 . To improve this model I use DCCNN (Dual Channel Convolutional Neural \\nNetwork). Here the DCCN N achieves high accuracy compared to the Normal CNN method. The DCCNN \\n')]\n",
            "Question: Dataset  of traffic signs ?\n",
            "Helpful Answer: Yes, the author mentions the availability of a dataset in Section 3, Background Theory. They explain that dataset preprocessing is necessary to ensure the data is arranged correctly in order to efficiently detect traffic signs. The author also discusses the importance of handling different sizes and shapes of traffic signs in the dataset, as well as dealing with blurred or faded signs. Gaussian effect is used to eliminate image noise. The author also briefly describes the process for training and testing the models using CNN, as well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "KWe11B_3H6Yc",
        "outputId": "249255e0-d0ae-4746-a75c-e6c3d58d7b03"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nUse the following pieces of context to answer the question at the end.\\nIf you don\\'t know the answer, just say that you do not have the relevant information needed to provide a verified answer, don\\'t try to make up an answer.\\nWhen providing an answer, aim for clarity and precision. Position yourself as a knowledgeable authority on the topic, but also be mindful to explain the information in a manner that is accessible and comprehensible to those without a technical background.\\nAlways say \"Do you have any more questions pertaining to this instrument?\" at the end of the answer.\\n[Document(page_content=\\'A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \\\\n____________________________________________________________________________________________________________  \\\\n______________________________________________________________________________ ____  \\\\n5525   \\\\nFigure.1: Dataset  of traffic signs  \\\\n \\\\n6. Traffic Sign Classification  \\\\nTraffic Sign Classification is a process that takes place to detect the traffic sign with the help of an image only.  \\\\nThe flow diagram of Traffic sign Classification is shown as follows in Fig 3.  \\\\n \\\\nFigure. 2: Flow  Diagram of Traffic Sign Classification  \\\\nThe above diagra m indicates the proposed model of the Traffic Sign Classification System.  \\\\nThe Procedure of this work is implemented as follows:  \\\\n• Traffic signs will be in different size s and shapes. Resizing has to take place to achieve fast detection of \\\\ntraffic sign s.  \\\\n• In this process , the image is captured through the network. Identification of image s is taken place with the \\\\nhelp of Object Detection API.  \\\\n•  The pre -process ing includes tracing the path of the traffic sign and assigning the labels for it. The traffic Sign \\\\nclasses are arranged in the form of an array with the help of its respective data associated with i t to retrieve \\\\nthe data easily. This paper also implement s a loop to trace the data until the data associated with the \\\\nrespective image is traced.  \\\\n• Now bu ilding the model has been t aken place to train by using an Algorithm . Then training and testing \\\\nprocess is carried for the training model.  \\\\n• After the training and testing process is carried on then we can check its accuracy and confusion matrix then \\\\ntest w ith the desired image so the resultant traffic sign information is obtained.  \\\\n\\'), Document(page_content=\\'Turkish Journal of Computer and Mathematics Education  \\\\n \\\\n \\\\n______________________________________________________________________________ ____  \\\\n5526    \\\\n  \\\\nResearch Article    Vol.12 No. 6 (2021), 5522 -5529 \\\\n7. Traffic Sign Detection  \\\\nTraffic Sign detection is the detection of traffic signs that takes place within the autonomous smart vehicle \\\\nitself. The detection of traffic sign is taken  place as follows as shown in Fig 4.  \\\\n \\\\nFigure. 3: Flow  Diagram of Traffic Sign detection  \\\\nThe s mart vehicle can be equipped  with the software.  The above diagram indicates the proposed work of the \\\\nTraffic sign detection system . \\\\nThe procedure of this work is implemented as follows:  \\\\n• First, we take a sample video  as an input here,  then the first detection of traffic sign in a video is taken place \\\\nwith the help of  object detection Even though it’s in any shape  and size no matter it detects and this fed to \\\\nAgain traffic sign classification Process  \\\\n• The detect traffic sign is fed to the Feature Engineering Process that divides the whole image into a huge \\\\nnumber of pixels and then extracts the Features and then clustering and F iltration operation process is also \\\\nperformed.  \\\\n• By the above process , the Image is then extracted and detected and then the image is compared with the \\\\ntesting process which is performed in the traffic sign Classification.   \\\\n• The label map is done to check wi th the testing feature to verify the predicted result is correct and after \\\\nverification , this display s the result.  \\\\n• The Traffic Sign Detection Process is performed with the help of a dataset that gets trained and tested by the \\\\nCNN in the Traffic sign Classi fication Process.     \\\\n        \\\\n8. Experimentation Results:  \\\\nThe Experiment is done with help of a flask that make s use of even HTML CSS JS along with python etc.  \\\\nThe Traffic Sign Classification is proved as best in automatically recognizing the traffic signs a nd helps in very \\\\nfast detection of traffic signs. The interface of his project of traffic sign classification is shown as follows in \\\\nFig 5.   \\\\n                                                      \\\\nFigure. 4: Result  of Traffic Sign Classification  (on left) Input Image  (on right ) Predicted Traffic sign Image.  \\\\n\\'), Document(page_content=\\'Turkish Journal of Computer and Mathematics Education  \\\\n \\\\n \\\\n______________________________________________________________________________ ____  \\\\n5522    \\\\n  \\\\nResearch Article    Vol.12 No. 6 (2021), 5522 -5529 \\\\nA Deep Convolutional Neural Network for Traffic Sign Classification and \\\\nDetection with Voice Recognition  \\\\n \\\\nK.Prakash  a. \\\\na M-Tech student, VNR Vignana Jyothi Institute of Engineering and Technology (Autonomous), \\\\nNizampet , Affiliation to JNTUH.  \\\\n \\\\nArticle History: Received: 10 November 2020; Revised 12 January 2021 Accepted: 27 January 2021; \\\\nPublished online: 5 April 2021  \\\\n____________________________________________________________________________________________________  \\\\n \\\\nAbstract: Traffic  Sign De tection and Classification play  an important major role now adays in our daily life. \\\\nAnyway , Traffic Signs are present on roads. Even though, often the drivers do mistakes. It’s very hard to \\\\nrecognize and detect traffic signs while travelling on roads. Drivers may misinterpret traffic si gns, this leads to \\\\nAccidents and results in damage to the vehicle. To overcome this problem this project introduces a concept \\\\nnamed Traffic Sign Detection and Classification with Voice Recognition. This model is built by using CNN to \\\\nextract the images and  classify the traffic signs. Here DCCNN model is built to improve overall accuracy and \\\\nspeed. Here Classification process is also tested with AlexNet, VGG16, VGG19. This system reveals output that \\\\nrecognizes the traffic signs automatically that helps to de tect the street condition and alerts driver soon and this \\\\nenables to build a smart vehicle.  \\\\n \\\\nKeywords: DCCNN, VGG16, Deep Learning , Traffic Sign Detection , Automation.  \\\\n___________________________________________________________________________  \\\\n \\\\n1. Introduct ion  \\\\nNow a day’s various modes of Transport facilities are very important in our daily life [1].  At the same time \\\\nsafety also plays a very important role. For Safety Purpose in Roads already traffic signs are placed on roads to \\\\novercome accidents and to follow the traffic rules [2]. Traffic Signs guide us to be in our safety and they help \\\\nus to travel in our vehicle in the desired path. Even though the traffic signs are present on roads, the drivers \\\\noften do mistakes [3]. It’s very hard to detect and recognize the traffic signs while driving the vehicle. With the \\\\nrise in a huge number of road accidents happening every year there has been need ing to develop a safety \\\\nsystem that helps in contributing to the pedestrian vehicle and also driver’s safety [4].  \\\\n Traffic signs play a very important role and crucial role while driving vehicles. A traffic sign is also a part \\\\nof road infrastructure [ 5]. Here is an automated system that help s us to notify and identify and can help a lot. \\\\nThis aims to recognize and detect the traffic sign much faster and enables to build of an autonomous vehicle \\\\n[6].  \\\\n2. Background Theory  \\\\n \\\\nThe traffic signs  are placed on the road with different sizes and shape some traffic sign s are very blur and \\\\nsmall in size, which is covered by snow and heavy rainfall. Even though it‘s in any condition,  Our aim to \\\\ndetect traffic sign s [7]. The traffic sign classification is a process tha t receives the input of the image to detect \\\\nthe traffic sign ie.., U -Turn Ahead, Speed Limit 40, Caution Signs , etc… These signs recommend drivers to get \\\\nby giving input commands through progra mming the data in such to classify the traffic sign automatical ly in \\\\nvehicle s [8]. Traffic Sign Detection is a process that detects the traffic sign and performs the feature extraction \\\\nprocess, clustering , and filtering process and then the labeling process and testing process has to be taken place \\\\nthen detected resul t is to be displayed on the dashboard [9].  \\\\nThe dataset consists of various formats. With respect to size, shape structure. Images preprocessing enables \\\\nthe improvement of images and reduces the distortion effect [10]. Here I also used the Gaussian effect to \\\\neliminate the image noise [11].  The dataset processing is to be done to ensure the data is arranged suitably  to \\\\ndetect the traffic signs much faster.  \\\\n3. Challenges  of  the Study  \\\\n• Can build  a smart autonomous smart vehicle further to improve  the safety mechanism for drivers, \\\\nPedestrians , and Vehicles.  \\\\n• To Implement Navigating Mechanism for drivers safety.  \\'), Document(page_content=\\'A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \\\\n____________________________________________________________________________________________________________  \\\\n______________________________________________________________________________ ____  \\\\n5527  The above diagrams indicate the output of Traffic Sign Classification. First Fig  5(on left ) indicates the input \\\\nof the image and another one when I upload traffic this shows the desired output of an image. Traffic Sign \\\\nClassification is very necessary to identify traffic signs much faster even though the traffic sign is in any size or \\\\nshape. Even though the traffic sign is a blur and faded then also with this we can detect it very easily  as shown \\\\nin Fig 5 (on right ). Traffic sign detection is very necessary to take place detection automatically when we are in \\\\ntravelling in a vehicle. This help s to detect the traffic sign al before the tr affic sign is arrived .  This alert s the \\\\nwhen the person is in sleepy mode. This also guide s the way of road automatically. The following fig of traffic \\\\nsign detection is shown in Fig  6 as follows.  Fig 6 (on left ) indicates here the input video frame without detection \\\\nof any traffic sign signal. In Figure 6 (on right ) The detection of Traffic sign detection has been taken place . \\\\n    \\\\nFigure. 5: Result of Traffic Sign Detection  (on left ) Input video (on right ) Traffic Sig n detection  \\\\n \\\\nHere the output of Traffic sign Classification and Detection is fed to the input of GTTS library in python is \\\\nused to implement the same result with speech. The Traffic Sign is Recognition is performed from a Real-time \\\\nWebcam camera with voice  software i.e., My Groovy music. This involves the same training method as Traffic \\\\nsign detection but the procedure is followed with OpenCV python. The Traffic Sign Recognition is performed \\\\nas follows as shown in Fig 7.  \\\\n \\\\nFigure. 6: Traffic  Sign Recognition  \\\\n9. Comparative  Results   \\\\n     Here the Model is implemented with VGG16, VGG19, AlexNet. The performance of various algorithms is \\\\nimplemented as follows:  \\\\nThe VGG16 is implemented for the traffic sign c lassification of images. VGG16 holds 82.77% Accuracy, \\\\nbut this is used to handle a huge amount of datasets. Here the VGG 19 is implemented the Results are \\\\nsomewhat  better  compared to VGG16 . VGG19 holds  an accuracy of 88.39%. Here also input images of the \\\\ndataset are fed to VGG19 . To improve this model I use DCCNN (Dual Channel Convolutional Neural \\\\nNetwork). Here the DCCN N achieves high accuracy compared to the Normal CNN method. The DCCNN \\\\n\\')]\\nQuestion: Dataset  of traffic signs ?\\nHelpful Answer: Yes, the author mentions the availability of a dataset in Section 3, Background Theory. They explain that dataset preprocessing is necessary to ensure the data is arranged correctly in order to efficiently detect traffic signs. The author also discusses the importance of handling different sizes and shapes of traffic signs in the dataset, as well as dealing with blurred or faded signs. Gaussian effect is used to eliminate image noise. The author also briefly describes the process for training and testing the models using CNN, as well'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "_Y6DcD3Z5lZp",
        "outputId": "e7e5869d-ab6e-41a7-f027-0434a67c9a9f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "string indices must be integers",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-4c244a2187fa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"result\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank"
      ],
      "metadata": {
        "id": "0A3hrUdwJ3pC"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VewE8gRKCla",
        "outputId": "6d2ed0d1-44e3-4151-b54c-7314092290aa"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.5.5-py3-none-any.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.8/169.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
            "  Downloading boto3-1.34.120-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.27.0)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.7.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Collecting tokenizers<0.16,>=0.15 (from cohere)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20240602-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.1)\n",
            "Collecting botocore<1.35.0,>=1.34.120 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading botocore-1.34.120-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.7)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<0.16,>=0.15->cohere) (0.23.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.120->boto3<2.0.0,>=1.34.0->cohere) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16,>=0.15->cohere) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16,>=0.15->cohere) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16,>=0.15->cohere) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16,>=0.15->cohere) (6.0.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16,>=0.15->cohere) (4.66.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.120->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n",
            "Installing collected packages: types-requests, jmespath, httpx-sse, fastavro, botocore, tokenizers, s3transfer, boto3, cohere\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.41.2 requires tokenizers<0.20,>=0.19, but you have tokenizers 0.15.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.34.120 botocore-1.34.120 cohere-5.5.5 fastavro-1.9.4 httpx-sse-0.4.0 jmespath-1.0.1 s3transfer-0.10.1 tokenizers-0.15.2 types-requests-2.32.0.20240602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compressor = CohereRerank(cohere_api_key=\"nbDqU1hTVxWmXGbLYI6OnYhp4Cx40MZ5hOmO5oKX\")"
      ],
      "metadata": {
        "id": "OE0vUax4J-Ij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01bd7f0f-564f-4053-bd00-30ff2a74c00d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 0.3.0. An updated version of the class exists in the langchain-cohere package and should be used instead. To use it run `pip install -U langchain-cohere` and import as `from langchain_cohere import CohereRerank`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        "    )"
      ],
      "metadata": {
        "id": "b3Kmr4CIKG7n"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
        "# Print the relevant documents from using the embeddings and reranker\n",
        "print(compressed_docs)\n"
      ],
      "metadata": {
        "id": "f7m22qlCiUAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646897e0-5ae0-4b3a-c288-12283e850618"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \\n____________________________________________________________________________________________________________  \\n______________________________________________________________________________ ____  \\n5525   \\nFigure.1: Dataset  of traffic signs  \\n \\n6. Traffic Sign Classification  \\nTraffic Sign Classification is a process that takes place to detect the traffic sign with the help of an image only.  \\nThe flow diagram of Traffic sign Classification is shown as follows in Fig 3.  \\n \\nFigure. 2: Flow  Diagram of Traffic Sign Classification  \\nThe above diagra m indicates the proposed model of the Traffic Sign Classification System.  \\nThe Procedure of this work is implemented as follows:  \\n• Traffic signs will be in different size s and shapes. Resizing has to take place to achieve fast detection of \\ntraffic sign s.  \\n• In this process , the image is captured through the network. Identification of image s is taken place with the \\nhelp of Object Detection API.  \\n•  The pre -process ing includes tracing the path of the traffic sign and assigning the labels for it. The traffic Sign \\nclasses are arranged in the form of an array with the help of its respective data associated with i t to retrieve \\nthe data easily. This paper also implement s a loop to trace the data until the data associated with the \\nrespective image is traced.  \\n• Now bu ilding the model has been t aken place to train by using an Algorithm . Then training and testing \\nprocess is carried for the training model.  \\n• After the training and testing process is carried on then we can check its accuracy and confusion matrix then \\ntest w ith the desired image so the resultant traffic sign information is obtained.  \\n', metadata={'relevance_score': 0.9769348}), Document(page_content='Turkish Journal of Computer and Mathematics Education  \\n \\n \\n______________________________________________________________________________ ____  \\n5522    \\n  \\nResearch Article    Vol.12 No. 6 (2021), 5522 -5529 \\nA Deep Convolutional Neural Network for Traffic Sign Classification and \\nDetection with Voice Recognition  \\n \\nK.Prakash  a. \\na M-Tech student, VNR Vignana Jyothi Institute of Engineering and Technology (Autonomous), \\nNizampet , Affiliation to JNTUH.  \\n \\nArticle History: Received: 10 November 2020; Revised 12 January 2021 Accepted: 27 January 2021; \\nPublished online: 5 April 2021  \\n____________________________________________________________________________________________________  \\n \\nAbstract: Traffic  Sign De tection and Classification play  an important major role now adays in our daily life. \\nAnyway , Traffic Signs are present on roads. Even though, often the drivers do mistakes. It’s very hard to \\nrecognize and detect traffic signs while travelling on roads. Drivers may misinterpret traffic si gns, this leads to \\nAccidents and results in damage to the vehicle. To overcome this problem this project introduces a concept \\nnamed Traffic Sign Detection and Classification with Voice Recognition. This model is built by using CNN to \\nextract the images and  classify the traffic signs. Here DCCNN model is built to improve overall accuracy and \\nspeed. Here Classification process is also tested with AlexNet, VGG16, VGG19. This system reveals output that \\nrecognizes the traffic signs automatically that helps to de tect the street condition and alerts driver soon and this \\nenables to build a smart vehicle.  \\n \\nKeywords: DCCNN, VGG16, Deep Learning , Traffic Sign Detection , Automation.  \\n___________________________________________________________________________  \\n \\n1. Introduct ion  \\nNow a day’s various modes of Transport facilities are very important in our daily life [1].  At the same time \\nsafety also plays a very important role. For Safety Purpose in Roads already traffic signs are placed on roads to \\novercome accidents and to follow the traffic rules [2]. Traffic Signs guide us to be in our safety and they help \\nus to travel in our vehicle in the desired path. Even though the traffic signs are present on roads, the drivers \\noften do mistakes [3]. It’s very hard to detect and recognize the traffic signs while driving the vehicle. With the \\nrise in a huge number of road accidents happening every year there has been need ing to develop a safety \\nsystem that helps in contributing to the pedestrian vehicle and also driver’s safety [4].  \\n Traffic signs play a very important role and crucial role while driving vehicles. A traffic sign is also a part \\nof road infrastructure [ 5]. Here is an automated system that help s us to notify and identify and can help a lot. \\nThis aims to recognize and detect the traffic sign much faster and enables to build of an autonomous vehicle \\n[6].  \\n2. Background Theory  \\n \\nThe traffic signs  are placed on the road with different sizes and shape some traffic sign s are very blur and \\nsmall in size, which is covered by snow and heavy rainfall. Even though it‘s in any condition,  Our aim to \\ndetect traffic sign s [7]. The traffic sign classification is a process tha t receives the input of the image to detect \\nthe traffic sign ie.., U -Turn Ahead, Speed Limit 40, Caution Signs , etc… These signs recommend drivers to get \\nby giving input commands through progra mming the data in such to classify the traffic sign automatical ly in \\nvehicle s [8]. Traffic Sign Detection is a process that detects the traffic sign and performs the feature extraction \\nprocess, clustering , and filtering process and then the labeling process and testing process has to be taken place \\nthen detected resul t is to be displayed on the dashboard [9].  \\nThe dataset consists of various formats. With respect to size, shape structure. Images preprocessing enables \\nthe improvement of images and reduces the distortion effect [10]. Here I also used the Gaussian effect to \\neliminate the image noise [11].  The dataset processing is to be done to ensure the data is arranged suitably  to \\ndetect the traffic signs much faster.  \\n3. Challenges  of  the Study  \\n• Can build  a smart autonomous smart vehicle further to improve  the safety mechanism for drivers, \\nPedestrians , and Vehicles.  \\n• To Implement Navigating Mechanism for drivers safety.  ', metadata={'relevance_score': 0.94068086}), Document(page_content='A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \\n____________________________________________________________________________________________________________  \\n______________________________________________________________________________ ____  \\n5527  The above diagrams indicate the output of Traffic Sign Classification. First Fig  5(on left ) indicates the input \\nof the image and another one when I upload traffic this shows the desired output of an image. Traffic Sign \\nClassification is very necessary to identify traffic signs much faster even though the traffic sign is in any size or \\nshape. Even though the traffic sign is a blur and faded then also with this we can detect it very easily  as shown \\nin Fig 5 (on right ). Traffic sign detection is very necessary to take place detection automatically when we are in \\ntravelling in a vehicle. This help s to detect the traffic sign al before the tr affic sign is arrived .  This alert s the \\nwhen the person is in sleepy mode. This also guide s the way of road automatically. The following fig of traffic \\nsign detection is shown in Fig  6 as follows.  Fig 6 (on left ) indicates here the input video frame without detection \\nof any traffic sign signal. In Figure 6 (on right ) The detection of Traffic sign detection has been taken place . \\n    \\nFigure. 5: Result of Traffic Sign Detection  (on left ) Input video (on right ) Traffic Sig n detection  \\n \\nHere the output of Traffic sign Classification and Detection is fed to the input of GTTS library in python is \\nused to implement the same result with speech. The Traffic Sign is Recognition is performed from a Real-time \\nWebcam camera with voice  software i.e., My Groovy music. This involves the same training method as Traffic \\nsign detection but the procedure is followed with OpenCV python. The Traffic Sign Recognition is performed \\nas follows as shown in Fig 7.  \\n \\nFigure. 6: Traffic  Sign Recognition  \\n9. Comparative  Results   \\n     Here the Model is implemented with VGG16, VGG19, AlexNet. The performance of various algorithms is \\nimplemented as follows:  \\nThe VGG16 is implemented for the traffic sign c lassification of images. VGG16 holds 82.77% Accuracy, \\nbut this is used to handle a huge amount of datasets. Here the VGG 19 is implemented the Results are \\nsomewhat  better  compared to VGG16 . VGG19 holds  an accuracy of 88.39%. Here also input images of the \\ndataset are fed to VGG19 . To improve this model I use DCCNN (Dual Channel Convolutional Neural \\nNetwork). Here the DCCN N achieves high accuracy compared to the Normal CNN method. The DCCNN \\n', metadata={'relevance_score': 0.7516481})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type=\"stuff\", retriever=compression_retriever\n",
        ")"
      ],
      "metadata": {
        "id": "0dKqM3XbKkE4"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = hybrid_chain.invoke(query)"
      ],
      "metadata": {
        "id": "2N2k_RCmKAIL"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.get(\"result\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVJxJg-bK2pg",
        "outputId": "5870b276-43a4-4dcb-baab-216e95653ced"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \n",
            "____________________________________________________________________________________________________________  \n",
            "______________________________________________________________________________ ____  \n",
            "5525   \n",
            "Figure.1: Dataset  of traffic signs  \n",
            " \n",
            "6. Traffic Sign Classification  \n",
            "Traffic Sign Classification is a process that takes place to detect the traffic sign with the help of an image only.  \n",
            "The flow diagram of Traffic sign Classification is shown as follows in Fig 3.  \n",
            " \n",
            "Figure. 2: Flow  Diagram of Traffic Sign Classification  \n",
            "The above diagra m indicates the proposed model of the Traffic Sign Classification System.  \n",
            "The Procedure of this work is implemented as follows:  \n",
            "• Traffic signs will be in different size s and shapes. Resizing has to take place to achieve fast detection of \n",
            "traffic sign s.  \n",
            "• In this process , the image is captured through the network. Identification of image s is taken place with the \n",
            "help of Object Detection API.  \n",
            "•  The pre -process ing includes tracing the path of the traffic sign and assigning the labels for it. The traffic Sign \n",
            "classes are arranged in the form of an array with the help of its respective data associated with i t to retrieve \n",
            "the data easily. This paper also implement s a loop to trace the data until the data associated with the \n",
            "respective image is traced.  \n",
            "• Now bu ilding the model has been t aken place to train by using an Algorithm . Then training and testing \n",
            "process is carried for the training model.  \n",
            "• After the training and testing process is carried on then we can check its accuracy and confusion matrix then \n",
            "test w ith the desired image so the resultant traffic sign information is obtained.  \n",
            "\n",
            "\n",
            "Turkish Journal of Computer and Mathematics Education  \n",
            " \n",
            " \n",
            "______________________________________________________________________________ ____  \n",
            "5522    \n",
            "  \n",
            "Research Article    Vol.12 No. 6 (2021), 5522 -5529 \n",
            "A Deep Convolutional Neural Network for Traffic Sign Classification and \n",
            "Detection with Voice Recognition  \n",
            " \n",
            "K.Prakash  a. \n",
            "a M-Tech student, VNR Vignana Jyothi Institute of Engineering and Technology (Autonomous), \n",
            "Nizampet , Affiliation to JNTUH.  \n",
            " \n",
            "Article History: Received: 10 November 2020; Revised 12 January 2021 Accepted: 27 January 2021; \n",
            "Published online: 5 April 2021  \n",
            "____________________________________________________________________________________________________  \n",
            " \n",
            "Abstract: Traffic  Sign De tection and Classification play  an important major role now adays in our daily life. \n",
            "Anyway , Traffic Signs are present on roads. Even though, often the drivers do mistakes. It’s very hard to \n",
            "recognize and detect traffic signs while travelling on roads. Drivers may misinterpret traffic si gns, this leads to \n",
            "Accidents and results in damage to the vehicle. To overcome this problem this project introduces a concept \n",
            "named Traffic Sign Detection and Classification with Voice Recognition. This model is built by using CNN to \n",
            "extract the images and  classify the traffic signs. Here DCCNN model is built to improve overall accuracy and \n",
            "speed. Here Classification process is also tested with AlexNet, VGG16, VGG19. This system reveals output that \n",
            "recognizes the traffic signs automatically that helps to de tect the street condition and alerts driver soon and this \n",
            "enables to build a smart vehicle.  \n",
            " \n",
            "Keywords: DCCNN, VGG16, Deep Learning , Traffic Sign Detection , Automation.  \n",
            "___________________________________________________________________________  \n",
            " \n",
            "1. Introduct ion  \n",
            "Now a day’s various modes of Transport facilities are very important in our daily life [1].  At the same time \n",
            "safety also plays a very important role. For Safety Purpose in Roads already traffic signs are placed on roads to \n",
            "overcome accidents and to follow the traffic rules [2]. Traffic Signs guide us to be in our safety and they help \n",
            "us to travel in our vehicle in the desired path. Even though the traffic signs are present on roads, the drivers \n",
            "often do mistakes [3]. It’s very hard to detect and recognize the traffic signs while driving the vehicle. With the \n",
            "rise in a huge number of road accidents happening every year there has been need ing to develop a safety \n",
            "system that helps in contributing to the pedestrian vehicle and also driver’s safety [4].  \n",
            " Traffic signs play a very important role and crucial role while driving vehicles. A traffic sign is also a part \n",
            "of road infrastructure [ 5]. Here is an automated system that help s us to notify and identify and can help a lot. \n",
            "This aims to recognize and detect the traffic sign much faster and enables to build of an autonomous vehicle \n",
            "[6].  \n",
            "2. Background Theory  \n",
            " \n",
            "The traffic signs  are placed on the road with different sizes and shape some traffic sign s are very blur and \n",
            "small in size, which is covered by snow and heavy rainfall. Even though it‘s in any condition,  Our aim to \n",
            "detect traffic sign s [7]. The traffic sign classification is a process tha t receives the input of the image to detect \n",
            "the traffic sign ie.., U -Turn Ahead, Speed Limit 40, Caution Signs , etc… These signs recommend drivers to get \n",
            "by giving input commands through progra mming the data in such to classify the traffic sign automatical ly in \n",
            "vehicle s [8]. Traffic Sign Detection is a process that detects the traffic sign and performs the feature extraction \n",
            "process, clustering , and filtering process and then the labeling process and testing process has to be taken place \n",
            "then detected resul t is to be displayed on the dashboard [9].  \n",
            "The dataset consists of various formats. With respect to size, shape structure. Images preprocessing enables \n",
            "the improvement of images and reduces the distortion effect [10]. Here I also used the Gaussian effect to \n",
            "eliminate the image noise [11].  The dataset processing is to be done to ensure the data is arranged suitably  to \n",
            "detect the traffic signs much faster.  \n",
            "3. Challenges  of  the Study  \n",
            "• Can build  a smart autonomous smart vehicle further to improve  the safety mechanism for drivers, \n",
            "Pedestrians , and Vehicles.  \n",
            "• To Implement Navigating Mechanism for drivers safety.  \n",
            "\n",
            "A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \n",
            "____________________________________________________________________________________________________________  \n",
            "______________________________________________________________________________ ____  \n",
            "5527  The above diagrams indicate the output of Traffic Sign Classification. First Fig  5(on left ) indicates the input \n",
            "of the image and another one when I upload traffic this shows the desired output of an image. Traffic Sign \n",
            "Classification is very necessary to identify traffic signs much faster even though the traffic sign is in any size or \n",
            "shape. Even though the traffic sign is a blur and faded then also with this we can detect it very easily  as shown \n",
            "in Fig 5 (on right ). Traffic sign detection is very necessary to take place detection automatically when we are in \n",
            "travelling in a vehicle. This help s to detect the traffic sign al before the tr affic sign is arrived .  This alert s the \n",
            "when the person is in sleepy mode. This also guide s the way of road automatically. The following fig of traffic \n",
            "sign detection is shown in Fig  6 as follows.  Fig 6 (on left ) indicates here the input video frame without detection \n",
            "of any traffic sign signal. In Figure 6 (on right ) The detection of Traffic sign detection has been taken place . \n",
            "    \n",
            "Figure. 5: Result of Traffic Sign Detection  (on left ) Input video (on right ) Traffic Sig n detection  \n",
            " \n",
            "Here the output of Traffic sign Classification and Detection is fed to the input of GTTS library in python is \n",
            "used to implement the same result with speech. The Traffic Sign is Recognition is performed from a Real-time \n",
            "Webcam camera with voice  software i.e., My Groovy music. This involves the same training method as Traffic \n",
            "sign detection but the procedure is followed with OpenCV python. The Traffic Sign Recognition is performed \n",
            "as follows as shown in Fig 7.  \n",
            " \n",
            "Figure. 6: Traffic  Sign Recognition  \n",
            "9. Comparative  Results   \n",
            "     Here the Model is implemented with VGG16, VGG19, AlexNet. The performance of various algorithms is \n",
            "implemented as follows:  \n",
            "The VGG16 is implemented for the traffic sign c lassification of images. VGG16 holds 82.77% Accuracy, \n",
            "but this is used to handle a huge amount of datasets. Here the VGG 19 is implemented the Results are \n",
            "somewhat  better  compared to VGG16 . VGG19 holds  an accuracy of 88.39%. Here also input images of the \n",
            "dataset are fed to VGG19 . To improve this model I use DCCNN (Dual Channel Convolutional Neural \n",
            "Network). Here the DCCN N achieves high accuracy compared to the Normal CNN method. The DCCNN \n",
            "\n",
            "\n",
            "Question: Dataset  of traffic signs ?\n",
            "Helpful Answer: Yes, the dataset of traffic signs is shown in Figure 1. It consists of various formats, sizes, and shapes of traffic signs. The dataset processing is also carried out to ensure the data is arranged suitably to detect traffic signs faster.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.get(\"result\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wa3jBEgLwXB",
        "outputId": "235fdd6f-7838-49a0-8ac7-fd4f5472e436"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \n",
            "____________________________________________________________________________________________________________  \n",
            "______________________________________________________________________________ ____  \n",
            "5525   \n",
            "Figure.1: Dataset  of traffic signs  \n",
            " \n",
            "6. Traffic Sign Classification  \n",
            "Traffic Sign Classification is a process that takes place to detect the traffic sign with the help of an image only.  \n",
            "The flow diagram of Traffic sign Classification is shown as follows in Fig 3.  \n",
            " \n",
            "Figure. 2: Flow  Diagram of Traffic Sign Classification  \n",
            "The above diagra m indicates the proposed model of the Traffic Sign Classification System.  \n",
            "The Procedure of this work is implemented as follows:  \n",
            "• Traffic signs will be in different size s and shapes. Resizing has to take place to achieve fast detection of \n",
            "traffic sign s.  \n",
            "• In this process , the image is captured through the network. Identification of image s is taken place with the \n",
            "help of Object Detection API.  \n",
            "•  The pre -process ing includes tracing the path of the traffic sign and assigning the labels for it. The traffic Sign \n",
            "classes are arranged in the form of an array with the help of its respective data associated with i t to retrieve \n",
            "the data easily. This paper also implement s a loop to trace the data until the data associated with the \n",
            "respective image is traced.  \n",
            "• Now bu ilding the model has been t aken place to train by using an Algorithm . Then training and testing \n",
            "process is carried for the training model.  \n",
            "• After the training and testing process is carried on then we can check its accuracy and confusion matrix then \n",
            "test w ith the desired image so the resultant traffic sign information is obtained.  \n",
            "\n",
            "\n",
            "Turkish Journal of Computer and Mathematics Education  \n",
            " \n",
            " \n",
            "______________________________________________________________________________ ____  \n",
            "5522    \n",
            "  \n",
            "Research Article    Vol.12 No. 6 (2021), 5522 -5529 \n",
            "A Deep Convolutional Neural Network for Traffic Sign Classification and \n",
            "Detection with Voice Recognition  \n",
            " \n",
            "K.Prakash  a. \n",
            "a M-Tech student, VNR Vignana Jyothi Institute of Engineering and Technology (Autonomous), \n",
            "Nizampet , Affiliation to JNTUH.  \n",
            " \n",
            "Article History: Received: 10 November 2020; Revised 12 January 2021 Accepted: 27 January 2021; \n",
            "Published online: 5 April 2021  \n",
            "____________________________________________________________________________________________________  \n",
            " \n",
            "Abstract: Traffic  Sign De tection and Classification play  an important major role now adays in our daily life. \n",
            "Anyway , Traffic Signs are present on roads. Even though, often the drivers do mistakes. It’s very hard to \n",
            "recognize and detect traffic signs while travelling on roads. Drivers may misinterpret traffic si gns, this leads to \n",
            "Accidents and results in damage to the vehicle. To overcome this problem this project introduces a concept \n",
            "named Traffic Sign Detection and Classification with Voice Recognition. This model is built by using CNN to \n",
            "extract the images and  classify the traffic signs. Here DCCNN model is built to improve overall accuracy and \n",
            "speed. Here Classification process is also tested with AlexNet, VGG16, VGG19. This system reveals output that \n",
            "recognizes the traffic signs automatically that helps to de tect the street condition and alerts driver soon and this \n",
            "enables to build a smart vehicle.  \n",
            " \n",
            "Keywords: DCCNN, VGG16, Deep Learning , Traffic Sign Detection , Automation.  \n",
            "___________________________________________________________________________  \n",
            " \n",
            "1. Introduct ion  \n",
            "Now a day’s various modes of Transport facilities are very important in our daily life [1].  At the same time \n",
            "safety also plays a very important role. For Safety Purpose in Roads already traffic signs are placed on roads to \n",
            "overcome accidents and to follow the traffic rules [2]. Traffic Signs guide us to be in our safety and they help \n",
            "us to travel in our vehicle in the desired path. Even though the traffic signs are present on roads, the drivers \n",
            "often do mistakes [3]. It’s very hard to detect and recognize the traffic signs while driving the vehicle. With the \n",
            "rise in a huge number of road accidents happening every year there has been need ing to develop a safety \n",
            "system that helps in contributing to the pedestrian vehicle and also driver’s safety [4].  \n",
            " Traffic signs play a very important role and crucial role while driving vehicles. A traffic sign is also a part \n",
            "of road infrastructure [ 5]. Here is an automated system that help s us to notify and identify and can help a lot. \n",
            "This aims to recognize and detect the traffic sign much faster and enables to build of an autonomous vehicle \n",
            "[6].  \n",
            "2. Background Theory  \n",
            " \n",
            "The traffic signs  are placed on the road with different sizes and shape some traffic sign s are very blur and \n",
            "small in size, which is covered by snow and heavy rainfall. Even though it‘s in any condition,  Our aim to \n",
            "detect traffic sign s [7]. The traffic sign classification is a process tha t receives the input of the image to detect \n",
            "the traffic sign ie.., U -Turn Ahead, Speed Limit 40, Caution Signs , etc… These signs recommend drivers to get \n",
            "by giving input commands through progra mming the data in such to classify the traffic sign automatical ly in \n",
            "vehicle s [8]. Traffic Sign Detection is a process that detects the traffic sign and performs the feature extraction \n",
            "process, clustering , and filtering process and then the labeling process and testing process has to be taken place \n",
            "then detected resul t is to be displayed on the dashboard [9].  \n",
            "The dataset consists of various formats. With respect to size, shape structure. Images preprocessing enables \n",
            "the improvement of images and reduces the distortion effect [10]. Here I also used the Gaussian effect to \n",
            "eliminate the image noise [11].  The dataset processing is to be done to ensure the data is arranged suitably  to \n",
            "detect the traffic signs much faster.  \n",
            "3. Challenges  of  the Study  \n",
            "• Can build  a smart autonomous smart vehicle further to improve  the safety mechanism for drivers, \n",
            "Pedestrians , and Vehicles.  \n",
            "• To Implement Navigating Mechanism for drivers safety.  \n",
            "\n",
            "A Deep Convolutional Neural Network for Traffic Sign Classification and Detection with Voice Recognition  \n",
            "____________________________________________________________________________________________________________  \n",
            "______________________________________________________________________________ ____  \n",
            "5527  The above diagrams indicate the output of Traffic Sign Classification. First Fig  5(on left ) indicates the input \n",
            "of the image and another one when I upload traffic this shows the desired output of an image. Traffic Sign \n",
            "Classification is very necessary to identify traffic signs much faster even though the traffic sign is in any size or \n",
            "shape. Even though the traffic sign is a blur and faded then also with this we can detect it very easily  as shown \n",
            "in Fig 5 (on right ). Traffic sign detection is very necessary to take place detection automatically when we are in \n",
            "travelling in a vehicle. This help s to detect the traffic sign al before the tr affic sign is arrived .  This alert s the \n",
            "when the person is in sleepy mode. This also guide s the way of road automatically. The following fig of traffic \n",
            "sign detection is shown in Fig  6 as follows.  Fig 6 (on left ) indicates here the input video frame without detection \n",
            "of any traffic sign signal. In Figure 6 (on right ) The detection of Traffic sign detection has been taken place . \n",
            "    \n",
            "Figure. 5: Result of Traffic Sign Detection  (on left ) Input video (on right ) Traffic Sig n detection  \n",
            " \n",
            "Here the output of Traffic sign Classification and Detection is fed to the input of GTTS library in python is \n",
            "used to implement the same result with speech. The Traffic Sign is Recognition is performed from a Real-time \n",
            "Webcam camera with voice  software i.e., My Groovy music. This involves the same training method as Traffic \n",
            "sign detection but the procedure is followed with OpenCV python. The Traffic Sign Recognition is performed \n",
            "as follows as shown in Fig 7.  \n",
            " \n",
            "Figure. 6: Traffic  Sign Recognition  \n",
            "9. Comparative  Results   \n",
            "     Here the Model is implemented with VGG16, VGG19, AlexNet. The performance of various algorithms is \n",
            "implemented as follows:  \n",
            "The VGG16 is implemented for the traffic sign c lassification of images. VGG16 holds 82.77% Accuracy, \n",
            "but this is used to handle a huge amount of datasets. Here the VGG 19 is implemented the Results are \n",
            "somewhat  better  compared to VGG16 . VGG19 holds  an accuracy of 88.39%. Here also input images of the \n",
            "dataset are fed to VGG19 . To improve this model I use DCCNN (Dual Channel Convolutional Neural \n",
            "Network). Here the DCCN N achieves high accuracy compared to the Normal CNN method. The DCCNN \n",
            "\n",
            "\n",
            "Question: Dataset  of traffic signs ?\n",
            "Helpful Answer: Yes, the dataset of traffic signs is shown in Figure 1. It consists of various formats, sizes, and shapes of traffic signs. The dataset processing is also carried out to ensure the data is arranged suitably to detect traffic signs faster.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tcdaBC5gMCzh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}